[0;34m[INFO][0m Using Python: /home/b/.pyenv/versions/3.10.18/bin/python3
[0;34m[INFO][0m Python version: Python 3.10.18
[0;34m[INFO][0m Target directory: ../src
[0;34m[INFO][0m Mode: check
[0;34m[INFO][0m Found 12 Python file(s) to analyze

==========================================
[0;34m1. BLACK - Code Formatting[0m
==========================================
[0;34m[INFO][0m Running black in CHECK mode...
--- /home/b/p/cefr-classification/minimal-cefr/src/config.py	2025-10-18 02:49:32.641082+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/config.py	2025-10-18 11:24:38.501230+00:00
@@ -15,20 +15,24 @@
 
 
 @dataclass
 class TfidfConfig:
     """Configuration for TF-IDF vectorizer training."""
+
     max_features: int = 5000
     ngram_range: tuple = (1, 2)
     min_df: int = 2
     max_df: float = 0.95
     sublinear_tf: bool = True
 
     def __post_init__(self):
         if self.max_features <= 0:
             raise ValueError(f"max_features must be positive, got {self.max_features}")
-        if not isinstance(self.ngram_range, (tuple, list)) or len(self.ngram_range) != 2:
+        if (
+            not isinstance(self.ngram_range, (tuple, list))
+            or len(self.ngram_range) != 2
+        ):
             raise ValueError(f"ngram_range must be a tuple of 2 integers")
         if self.min_df < 1:
             raise ValueError(f"min_df must be >= 1, got {self.min_df}")
         if not (0 < self.max_df <= 1.0):
             raise ValueError(f"max_df must be between 0 and 1, got {self.max_df}")
@@ -51,11 +55,11 @@
             f"max_df={self.max_df},"
             f"sublinear_tf={self.sublinear_tf}"
         )
 
         # Generate SHA256 hash and take first 8 characters
-        hash_obj = hashlib.sha256(config_str.encode('utf-8'))
+        hash_obj = hashlib.sha256(config_str.encode("utf-8"))
         return hash_obj.hexdigest()[:8]
 
     def get_readable_name(self) -> str:
         """
         Generate a human-readable name for this TF-IDF configuration.
@@ -68,10 +72,11 @@
 
 
 @dataclass
 class ClassifierConfig:
     """Configuration for ML classifier training."""
+
     classifier_type: str = "multinomialnb"
 
     # Classifier-specific parameters
     logistic_max_iter: int = 1000
     logistic_class_weight: str = "balanced"
@@ -90,18 +95,27 @@
     xgb_tree_method: str = "auto"  # auto, gpu_hist, hist, exact
 
     random_state: int = 42
 
     def __post_init__(self):
-        valid_classifiers = ["multinomialnb", "logistic", "randomforest", "svm", "xgboost"]
+        valid_classifiers = [
+            "multinomialnb",
+            "logistic",
+            "randomforest",
+            "svm",
+            "xgboost",
+        ]
         if self.classifier_type not in valid_classifiers:
-            raise ValueError(f"classifier_type must be one of {valid_classifiers}, got {self.classifier_type}")
+            raise ValueError(
+                f"classifier_type must be one of {valid_classifiers}, got {self.classifier_type}"
+            )
 
 
 @dataclass
 class ExperimentConfig:
     """Configuration for experiment directory and data paths."""
+
     experiment_dir: str = "data/experiments/zero-shot"
 
     # Auto-derived paths (computed in __post_init__)
     features_training_dir: Optional[str] = None
     ml_training_dir: Optional[str] = None
@@ -146,11 +160,13 @@
             Path to feature model directory (e.g., "feature-models/252cd532_tfidf")
         """
         model_name = f"{config_hash}_{feature_type}"
         return str(Path(self.models_dir) / model_name)
 
-    def get_tfidf_model_dir(self, tfidf_config: 'TfidfConfig', feature_type: str = "tfidf") -> str:
+    def get_tfidf_model_dir(
+        self, tfidf_config: "TfidfConfig", feature_type: str = "tfidf"
+    ) -> str:
         """
         Get the TF-IDF model directory for a specific configuration.
 
         Args:
             tfidf_config: TF-IDF configuration
@@ -178,27 +194,31 @@
 
 
 @dataclass
 class DataConfig:
     """Configuration for data loading and preprocessing."""
+
     text_column: str = "text"
     label_column: str = "label"
     cefr_column: str = "cefr_label"
     min_text_length: int = 0
     max_samples: Optional[int] = None
     random_sample: bool = False
 
     def __post_init__(self):
         if self.min_text_length < 0:
-            raise ValueError(f"min_text_length must be non-negative, got {self.min_text_length}")
+            raise ValueError(
+                f"min_text_length must be non-negative, got {self.min_text_length}"
+            )
         if self.max_samples is not None and self.max_samples <= 0:
             raise ValueError(f"max_samples must be positive, got {self.max_samples}")
 
 
 @dataclass
 class OutputConfig:
     """Configuration for output and results."""
+
     save_config: bool = True
     save_models: bool = True
     save_features: bool = True
     save_results: bool = True
     verbose: bool = True
@@ -210,10 +230,11 @@
 
 
 @dataclass
 class GlobalConfig:
     """Centralized global configuration combining all sub-configs."""
+
     experiment_config: ExperimentConfig = field(default_factory=ExperimentConfig)
     tfidf_config: TfidfConfig = field(default_factory=TfidfConfig)
     classifier_config: ClassifierConfig = field(default_factory=ClassifierConfig)
     data_config: DataConfig = field(default_factory=DataConfig)
     output_config: OutputConfig = field(default_factory=OutputConfig)
@@ -237,29 +258,35 @@
         experiment_config = ExperimentConfig(**config_dict.get("experiment_config", {}))
         tfidf_config = TfidfConfig(**config_dict.get("tfidf_config", {}))
         classifier_config = ClassifierConfig(**config_dict.get("classifier_config", {}))
         data_config = DataConfig(**config_dict.get("data_config", {}))
         output_config = OutputConfig(**config_dict.get("output_config", {}))
-        return cls(experiment_config, tfidf_config, classifier_config, data_config, output_config)
+        return cls(
+            experiment_config,
+            tfidf_config,
+            classifier_config,
+            data_config,
+            output_config,
+        )
 
     @classmethod
     def from_json_string(cls, json_string: str) -> "GlobalConfig":
         """Create config from JSON string."""
         config_dict = json.loads(json_string)
         return cls.from_dict(config_dict)
 
     @classmethod
     def from_json_file(cls, json_path: str) -> "GlobalConfig":
         """Create config from JSON file."""
-        with open(json_path, 'r') as f:
+        with open(json_path, "r") as f:
             config_dict = json.load(f)
         return cls.from_dict(config_dict)
 
     @classmethod
     def from_yaml_file(cls, yaml_path: str) -> "GlobalConfig":
         """Create config from YAML file."""
-        with open(yaml_path, 'r') as f:
+        with open(yaml_path, "r") as f:
             config_dict = yaml.safe_load(f)
         return cls.from_dict(config_dict)
 
     def to_dict(self) -> Dict[str, Any]:
         """Convert config to dictionary."""
@@ -304,17 +331,17 @@
                 "save_results": self.output_config.save_results,
                 "verbose": self.output_config.verbose,
                 "overwrite": self.output_config.overwrite,
                 "save_csv": self.output_config.save_csv,
                 "save_json": self.output_config.save_json,
-            }
+            },
         }
 
     def save_json(self, path: str):
         """Save config to JSON file."""
-        with open(path, 'w') as f:
+        with open(path, "w") as f:
             json.dump(self.to_dict(), f, indent=2)
 
     def save_yaml(self, path: str):
         """Save config to YAML file."""
-        with open(path, 'w') as f:
+        with open(path, "w") as f:
             yaml.dump(self.to_dict(), f, default_flow_style=False)
would reformat /home/b/p/cefr-classification/minimal-cefr/src/config.py
--- /home/b/p/cefr-classification/minimal-cefr/src/report.py	2025-10-18 01:37:00.199200+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/report.py	2025-10-18 11:24:38.884002+00:00
@@ -15,10 +15,11 @@
 
 
 @dataclass
 class ModelMetrics:
     """Performance metrics for a single model on a single dataset."""
+
     model_name: str
     dataset_name: str
     strategy: str  # "argmax" or "rounded_avg"
 
     # Metrics
@@ -46,51 +47,67 @@
         Dict with keys "argmax" and "rounded_avg", each containing metrics dict
     """
     if not report_path.exists():
         return {}
 
-    with open(report_path, 'r') as f:
+    with open(report_path, "r") as f:
         content = f.read()
 
     results = {}
 
     # Parse both strategies
     for strategy_name, section_title in [
         ("argmax", "Strategy 1: Argmax Predictions"),
-        ("rounded_avg", "Strategy 2: Rounded Average Predictions")
+        ("rounded_avg", "Strategy 2: Rounded Average Predictions"),
     ]:
         metrics = {}
 
         # Find the section
         if section_title in content:
             # Extract accuracy from CEFR Classification Report
             # Pattern: "accuracy      0.XX      1742"
             accuracy_pattern = r"^accuracy\s+(0?\.\d+)\s+\d+$"
-            accuracy_match = re.search(accuracy_pattern, content[content.find(section_title):], re.MULTILINE)
+            accuracy_match = re.search(
+                accuracy_pattern, content[content.find(section_title) :], re.MULTILINE
+            )
             if accuracy_match:
                 metrics["accuracy"] = float(accuracy_match.group(1))
 
             # Extract adjacent accuracy
             # Pattern: "adjacent accuracy      0.XX      1742"
             adj_acc_pattern = r"^adjacent accuracy\s+(0?\.\d+)\s+\d+$"
-            adj_acc_match = re.search(adj_acc_pattern, content[content.find(section_title):], re.MULTILINE)
+            adj_acc_match = re.search(
+                adj_acc_pattern, content[content.find(section_title) :], re.MULTILINE
+            )
             if adj_acc_match:
                 metrics["adjacent_accuracy"] = float(adj_acc_match.group(1))
 
             # Extract macro avg F1 from standard classification report
             # Pattern: "macro avg       0.XX      0.XX      0.XX      1742"
             # We want the f1-score (3rd column)
             macro_f1_pattern = r"^macro avg\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)\s+\d+$"
-            macro_f1_match = re.search(macro_f1_pattern, content[content.find(section_title):], re.MULTILINE)
+            macro_f1_match = re.search(
+                macro_f1_pattern, content[content.find(section_title) :], re.MULTILINE
+            )
             if macro_f1_match:
-                metrics["macro_f1"] = float(macro_f1_match.group(3))  # 3rd group is f1-score
+                metrics["macro_f1"] = float(
+                    macro_f1_match.group(3)
+                )  # 3rd group is f1-score
 
             # Extract weighted avg F1
-            weighted_f1_pattern = r"^weighted avg\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)\s+\d+$"
-            weighted_f1_match = re.search(weighted_f1_pattern, content[content.find(section_title):], re.MULTILINE)
+            weighted_f1_pattern = (
+                r"^weighted avg\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)\s+\d+$"
+            )
+            weighted_f1_match = re.search(
+                weighted_f1_pattern,
+                content[content.find(section_title) :],
+                re.MULTILINE,
+            )
             if weighted_f1_match:
-                metrics["weighted_f1"] = float(weighted_f1_match.group(3))  # 3rd group is f1-score
+                metrics["weighted_f1"] = float(
+                    weighted_f1_match.group(3)
+                )  # 3rd group is f1-score
 
         results[strategy_name] = metrics
 
     # Extract dataset info (from header)
     samples_pattern = r"\*\*Samples\*\*:\s+(\d+)"
@@ -100,29 +117,33 @@
             strategy["n_samples"] = int(samples_match.group(1))
 
     classes_pattern = r"\*\*Classes in test set\*\*:\s+(.+)"
     classes_match = re.search(classes_pattern, content)
     if classes_match:
-        classes = [c.strip() for c in classes_match.group(1).split(',')]
+        classes = [c.strip() for c in classes_match.group(1).split(",")]
         for strategy in results.values():
             strategy["classes_in_test"] = classes
 
     return results
 
 
 def load_model_config(experiment_dir: Path, model_name: str) -> Dict:
     """Load model config.json to get TF-IDF and classifier parameters."""
-    config_path = experiment_dir / "feature-models" / "classifiers" / model_name / "config.json"
+    config_path = (
+        experiment_dir / "feature-models" / "classifiers" / model_name / "config.json"
+    )
 
     if not config_path.exists():
         return {}
 
-    with open(config_path, 'r') as f:
+    with open(config_path, "r") as f:
         return json.load(f)
 
 
-def collect_all_metrics(experiment_dir: Path, verbose: bool = False) -> List[ModelMetrics]:
+def collect_all_metrics(
+    experiment_dir: Path, verbose: bool = False
+) -> List[ModelMetrics]:
     """
     Scan results directory and collect all model metrics.
 
     Args:
         experiment_dir: Path to experiment directory
@@ -182,26 +203,28 @@
                     n_samples=metrics.get("n_samples"),
                     classes_in_test=metrics.get("classes_in_test", []),
                     tfidf_hash=model_config.get("tfidf_hash"),
                     tfidf_max_features=model_config.get("tfidf_max_features"),
                     tfidf_readable_name=model_config.get("tfidf_readable_name"),
-                    classifier_type=model_config.get("classifier_type")
+                    classifier_type=model_config.get("classifier_type"),
                 )
 
                 all_metrics.append(model_metrics)
 
     if verbose:
-        print(f"Collected {len(all_metrics)} metric records ({len(all_metrics)//2} per strategy)")
+        print(
+            f"Collected {len(all_metrics)} metric records ({len(all_metrics)//2} per strategy)"
+        )
 
     return all_metrics
 
 
 def rank_models(
     metrics_list: List[ModelMetrics],
     criterion: str = "accuracy",
     dataset_filter: Optional[str] = None,
-    strategy_filter: Optional[str] = None
+    strategy_filter: Optional[str] = None,
 ) -> List[ModelMetrics]:
     """
     Rank models according to a criterion.
 
     Args:
@@ -235,11 +258,11 @@
 
 def print_ranking_table(
     ranked_metrics: List[ModelMetrics],
     criterion: str,
     top_n: Optional[int] = None,
-    show_config: bool = True
+    show_config: bool = True,
 ):
     """Print ranked models as a formatted table."""
     if not ranked_metrics:
         print("No results to display.")
         return
@@ -251,40 +274,54 @@
     print(f"RANKING BY: {criterion.upper().replace('_', ' ')}")
     print(f"{'='*120}")
 
     # Header
     if show_config:
-        print(f"{'Rank':<6} {'Model':<45} {'Dataset':<20} {'Strategy':<12} {criterion.replace('_', ' ').title():<10} {'TF-IDF':<20} {'Classifier':<12}")
+        print(
+            f"{'Rank':<6} {'Model':<45} {'Dataset':<20} {'Strategy':<12} {criterion.replace('_', ' ').title():<10} {'TF-IDF':<20} {'Classifier':<12}"
+        )
         print(f"{'-'*6} {'-'*45} {'-'*20} {'-'*12} {'-'*10} {'-'*20} {'-'*12}")
     else:
-        print(f"{'Rank':<6} {'Model':<45} {'Dataset':<20} {'Strategy':<12} {criterion.replace('_', ' ').title():<10}")
+        print(
+            f"{'Rank':<6} {'Model':<45} {'Dataset':<20} {'Strategy':<12} {criterion.replace('_', ' ').title():<10}"
+        )
         print(f"{'-'*6} {'-'*45} {'-'*20} {'-'*12} {'-'*10}")
 
     # Rows
     for i, m in enumerate(ranked_metrics, 1):
         value = getattr(m, criterion, None)
         value_str = f"{value:.4f}" if value is not None else "N/A"
 
         # Truncate model name if too long
-        model_display = m.model_name[:43] + ".." if len(m.model_name) > 45 else m.model_name
+        model_display = (
+            m.model_name[:43] + ".." if len(m.model_name) > 45 else m.model_name
+        )
 
         if show_config:
-            tfidf_display = m.tfidf_readable_name or f"hash:{m.tfidf_hash[:8]}" if m.tfidf_hash else "N/A"
+            tfidf_display = (
+                m.tfidf_readable_name or f"hash:{m.tfidf_hash[:8]}"
+                if m.tfidf_hash
+                else "N/A"
+            )
             clf_display = m.classifier_type or "N/A"
-            print(f"{i:<6} {model_display:<45} {m.dataset_name:<20} {m.strategy:<12} {value_str:<10} {tfidf_display:<20} {clf_display:<12}")
+            print(
+                f"{i:<6} {model_display:<45} {m.dataset_name:<20} {m.strategy:<12} {value_str:<10} {tfidf_display:<20} {clf_display:<12}"
+            )
         else:
-            print(f"{i:<6} {model_display:<45} {m.dataset_name:<20} {m.strategy:<12} {value_str:<10}")
+            print(
+                f"{i:<6} {model_display:<45} {m.dataset_name:<20} {m.strategy:<12} {value_str:<10}"
+            )
 
     print(f"{'='*120}\n")
 
 
 def print_ranking_grouped_by_dataset(
     metrics_list: List[ModelMetrics],
     criterion: str,
     strategy_filter: Optional[str] = None,
     top_n: Optional[int] = None,
-    show_config: bool = True
+    show_config: bool = True,
 ):
     """Print ranked models grouped by dataset."""
     if not metrics_list:
         print("No results to display.")
         return
@@ -304,13 +341,11 @@
     for dataset_name in sorted(datasets.keys()):
         dataset_metrics = datasets[dataset_name]
 
         # Rank within this dataset
         ranked = rank_models(
-            dataset_metrics,
-            criterion=criterion,
-            strategy_filter=strategy_filter
+            dataset_metrics, criterion=criterion, strategy_filter=strategy_filter
         )
 
         if not ranked:
             continue
 
@@ -322,28 +357,40 @@
         print(f"📊 Dataset: {dataset_name}")
         print(f"{'-'*120}")
 
         # Header
         if show_config:
-            print(f"{'Rank':<6} {'Model':<45} {'Strategy':<12} {criterion.replace('_', ' ').title():<10} {'TF-IDF':<20} {'Classifier':<12}")
+            print(
+                f"{'Rank':<6} {'Model':<45} {'Strategy':<12} {criterion.replace('_', ' ').title():<10} {'TF-IDF':<20} {'Classifier':<12}"
+            )
             print(f"{'-'*6} {'-'*45} {'-'*12} {'-'*10} {'-'*20} {'-'*12}")
         else:
-            print(f"{'Rank':<6} {'Model':<45} {'Strategy':<12} {criterion.replace('_', ' ').title():<10}")
+            print(
+                f"{'Rank':<6} {'Model':<45} {'Strategy':<12} {criterion.replace('_', ' ').title():<10}"
+            )
             print(f"{'-'*6} {'-'*45} {'-'*12} {'-'*10}")
 
         # Rows
         for i, m in enumerate(ranked, 1):
             value = getattr(m, criterion, None)
             value_str = f"{value:.4f}" if value is not None else "N/A"
 
             # Truncate model name if too long
-            model_display = m.model_name[:43] + ".." if len(m.model_name) > 45 else m.model_name
+            model_display = (
+                m.model_name[:43] + ".." if len(m.model_name) > 45 else m.model_name
+            )
 
             if show_config:
-                tfidf_display = m.tfidf_readable_name or f"hash:{m.tfidf_hash[:8]}" if m.tfidf_hash else "N/A"
+                tfidf_display = (
+                    m.tfidf_readable_name or f"hash:{m.tfidf_hash[:8]}"
+                    if m.tfidf_hash
+                    else "N/A"
+                )
                 clf_display = m.classifier_type or "N/A"
-                print(f"{i:<6} {model_display:<45} {m.strategy:<12} {value_str:<10} {tfidf_display:<20} {clf_display:<12}")
+                print(
+                    f"{i:<6} {model_display:<45} {m.strategy:<12} {value_str:<10} {tfidf_display:<20} {clf_display:<12}"
+                )
             else:
                 print(f"{i:<6} {model_display:<45} {m.strategy:<12} {value_str:<10}")
 
         print()  # Blank line between datasets
 
@@ -351,11 +398,11 @@
 
 
 def generate_summary_report(
     experiment_dir: Path,
     metrics_list: List[ModelMetrics],
-    output_path: Optional[Path] = None
+    output_path: Optional[Path] = None,
 ):
     """Generate comprehensive markdown summary report."""
     if not metrics_list:
         print("No metrics to summarize.")
         return
@@ -382,33 +429,47 @@
     # Top models by different criteria
     criteria = [
         ("accuracy", "Accuracy"),
         ("adjacent_accuracy", "Adjacent Accuracy"),
         ("macro_f1", "Macro F1-Score"),
-        ("weighted_f1", "Weighted F1-Score")
+        ("weighted_f1", "Weighted F1-Score"),
     ]
 
     for criterion, criterion_name in criteria:
         report_lines.append(f"## Top 10 Models by {criterion_name}")
         report_lines.append("")
 
         for strategy in ["argmax", "rounded_avg"]:
-            ranked = rank_models(metrics_list, criterion=criterion, strategy_filter=strategy)[:10]
+            ranked = rank_models(
+                metrics_list, criterion=criterion, strategy_filter=strategy
+            )[:10]
 
             if ranked:
-                report_lines.append(f"### Strategy: {strategy.replace('_', ' ').title()}")
+                report_lines.append(
+                    f"### Strategy: {strategy.replace('_', ' ').title()}"
+                )
                 report_lines.append("")
-                report_lines.append(f"| Rank | Model | Dataset | {criterion_name} | TF-IDF Config | Classifier |")
-                report_lines.append("|------|-------|---------|----------|---------------|------------|")
+                report_lines.append(
+                    f"| Rank | Model | Dataset | {criterion_name} | TF-IDF Config | Classifier |"
+                )
+                report_lines.append(
+                    "|------|-------|---------|----------|---------------|------------|"
+                )
 
                 for i, m in enumerate(ranked, 1):
                     value = getattr(m, criterion, None)
                     value_str = f"{value:.4f}" if value is not None else "N/A"
-                    tfidf_display = m.tfidf_readable_name or f"`{m.tfidf_hash[:8]}`" if m.tfidf_hash else "N/A"
+                    tfidf_display = (
+                        m.tfidf_readable_name or f"`{m.tfidf_hash[:8]}`"
+                        if m.tfidf_hash
+                        else "N/A"
+                    )
                     clf_display = m.classifier_type or "N/A"
 
-                    report_lines.append(f"| {i} | `{m.model_name}` | {m.dataset_name} | {value_str} | {tfidf_display} | {clf_display} |")
+                    report_lines.append(
+                        f"| {i} | `{m.model_name}` | {m.dataset_name} | {value_str} | {tfidf_display} | {clf_display} |"
+                    )
 
                 report_lines.append("")
 
     # Performance by dataset
     report_lines.append("## Performance by Dataset")
@@ -416,19 +477,27 @@
 
     for dataset in sorted(unique_datasets):
         report_lines.append(f"### {dataset}")
         report_lines.append("")
 
-        dataset_metrics = [m for m in metrics_list if m.dataset_name == dataset and m.strategy == "argmax"]
+        dataset_metrics = [
+            m
+            for m in metrics_list
+            if m.dataset_name == dataset and m.strategy == "argmax"
+        ]
 
         if dataset_metrics:
             # Find best model for this dataset
             best_by_acc = max(dataset_metrics, key=lambda m: m.accuracy or 0)
             best_by_adj = max(dataset_metrics, key=lambda m: m.adjacent_accuracy or 0)
 
-            report_lines.append(f"- **Best Accuracy:** {best_by_acc.accuracy:.4f} (`{best_by_acc.model_name}`)")
-            report_lines.append(f"- **Best Adjacent Accuracy:** {best_by_adj.adjacent_accuracy:.4f} (`{best_by_adj.model_name}`)")
+            report_lines.append(
+                f"- **Best Accuracy:** {best_by_acc.accuracy:.4f} (`{best_by_acc.model_name}`)"
+            )
+            report_lines.append(
+                f"- **Best Adjacent Accuracy:** {best_by_adj.adjacent_accuracy:.4f} (`{best_by_adj.model_name}`)"
+            )
             report_lines.append(f"- **Models Evaluated:** {len(dataset_metrics)}")
             report_lines.append("")
 
     # TF-IDF configuration comparison
     report_lines.append("## TF-IDF Configuration Analysis")
@@ -439,38 +508,52 @@
         if m.tfidf_hash and m.strategy == "argmax":
             if m.tfidf_hash not in tfidf_configs:
                 tfidf_configs[m.tfidf_hash] = {
                     "readable_name": m.tfidf_readable_name,
                     "max_features": m.tfidf_max_features,
-                    "accuracies": []
+                    "accuracies": [],
                 }
             if m.accuracy is not None:
                 tfidf_configs[m.tfidf_hash]["accuracies"].append(m.accuracy)
 
     if tfidf_configs:
-        report_lines.append("| TF-IDF Config | Max Features | Avg Accuracy | Min Accuracy | Max Accuracy | Evaluations |")
-        report_lines.append("|---------------|--------------|--------------|--------------|--------------|-------------|")
-
-        for hash_id, config in sorted(tfidf_configs.items(), key=lambda x: sum(x[1]["accuracies"])/len(x[1]["accuracies"]) if x[1]["accuracies"] else 0, reverse=True):
+        report_lines.append(
+            "| TF-IDF Config | Max Features | Avg Accuracy | Min Accuracy | Max Accuracy | Evaluations |"
+        )
+        report_lines.append(
+            "|---------------|--------------|--------------|--------------|--------------|-------------|"
+        )
+
+        for hash_id, config in sorted(
+            tfidf_configs.items(),
+            key=lambda x: (
+                sum(x[1]["accuracies"]) / len(x[1]["accuracies"])
+                if x[1]["accuracies"]
+                else 0
+            ),
+            reverse=True,
+        ):
             accs = config["accuracies"]
             if accs:
                 avg_acc = sum(accs) / len(accs)
                 min_acc = min(accs)
                 max_acc = max(accs)
 
                 name_display = config["readable_name"] or f"`{hash_id[:8]}`"
                 features_display = config["max_features"] or "N/A"
 
-                report_lines.append(f"| {name_display} | {features_display} | {avg_acc:.4f} | {min_acc:.4f} | {max_acc:.4f} | {len(accs)} |")
+                report_lines.append(
+                    f"| {name_display} | {features_display} | {avg_acc:.4f} | {min_acc:.4f} | {max_acc:.4f} | {len(accs)} |"
+                )
 
         report_lines.append("")
 
     # Write or print report
     report_content = "\n".join(report_lines)
 
     if output_path:
-        with open(output_path, 'w') as f:
+        with open(output_path, "w") as f:
             f.write(report_content)
         print(f"Summary report saved to: {output_path}")
     else:
         print(report_content)
 
@@ -505,64 +588,53 @@
   6. Rank by macro F1 for rounded_avg strategy:
      python -m src.report -e data/experiments/zero-shot \\
          --rank macro_f1 \\
          --strategy rounded_avg \\
          --top 10
-"""
+""",
     )
 
     parser.add_argument(
-        "-e", "--experiment-dir",
-        required=True,
-        help="Path to experiment directory"
+        "-e", "--experiment-dir", required=True, help="Path to experiment directory"
     )
 
     parser.add_argument(
         "--rank",
         choices=["accuracy", "adjacent_accuracy", "macro_f1", "weighted_f1"],
-        help="Rank models by criterion"
+        help="Rank models by criterion",
     )
 
     parser.add_argument(
-        "--dataset",
-        help="Filter by specific dataset (e.g., norm-CELVA-SP)"
+        "--dataset", help="Filter by specific dataset (e.g., norm-CELVA-SP)"
     )
 
     parser.add_argument(
         "--strategy",
         choices=["argmax", "rounded_avg"],
-        help="Filter by prediction strategy"
-    )
-
-    parser.add_argument(
-        "--top",
-        type=int,
-        help="Show only top N results"
-    )
+        help="Filter by prediction strategy",
+    )
+
+    parser.add_argument("--top", type=int, help="Show only top N results")
 
     parser.add_argument(
         "--summary-report",
-        help="Generate comprehensive summary report (markdown file path)"
+        help="Generate comprehensive summary report (markdown file path)",
     )
 
     parser.add_argument(
         "--no-config",
         action="store_true",
-        help="Don't show TF-IDF/classifier config in ranking table"
+        help="Don't show TF-IDF/classifier config in ranking table",
     )
 
     parser.add_argument(
         "--no-group",
         action="store_true",
-        help="Don't group rankings by dataset (show flat ranking instead)"
-    )
-
-    parser.add_argument(
-        "-v", "--verbose",
-        action="store_true",
-        help="Verbose output"
-    )
+        help="Don't group rankings by dataset (show flat ranking instead)",
+    )
+
+    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")
 
     args = parser.parse_args()
 
     experiment_dir = Path(args.experiment_dir)
 
@@ -593,26 +665,26 @@
             print_ranking_grouped_by_dataset(
                 metrics_list,
                 criterion=args.rank,
                 strategy_filter=args.strategy,
                 top_n=args.top,
-                show_config=not args.no_config
+                show_config=not args.no_config,
             )
         else:
             # Flat ranking (when filtering by specific dataset or --no-group)
             ranked = rank_models(
                 metrics_list,
                 criterion=args.rank,
                 dataset_filter=args.dataset,
-                strategy_filter=args.strategy
+                strategy_filter=args.strategy,
             )
 
             print_ranking_table(
                 ranked,
                 criterion=args.rank,
                 top_n=args.top,
-                show_config=not args.no_config
+                show_config=not args.no_config,
             )
 
     # If neither ranking nor summary requested, show brief overview
     if not args.rank and not args.summary_report:
         print("\nQuick Overview:")
would reformat /home/b/p/cefr-classification/minimal-cefr/src/report.py
--- /home/b/p/cefr-classification/minimal-cefr/src/extract_perplexity_features.py	2025-10-18 11:23:10.571010+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/extract_perplexity_features.py	2025-10-18 11:24:38.936625+00:00
@@ -145,30 +145,34 @@
 from tqdm import tqdm
 
 try:
     import torch
     import torch.nn.functional as F
+
     TORCH_AVAILABLE = True
 except ImportError:
     TORCH_AVAILABLE = False
 
 try:
     from transformers import AutoModelForCausalLM, AutoTokenizer
+
     HF_AVAILABLE = True
 except ImportError:
     HF_AVAILABLE = False
 
 try:
     from datasets import load_dataset
+
     DATASETS_AVAILABLE = True
 except ImportError:
     DATASETS_AVAILABLE = False
 
 
 @dataclass
 class TokenPerplexity:
     """Detailed per-token perplexity information."""
+
     token: str
     token_id: int
     position: int
     logit: float
     prob: float
@@ -185,34 +189,32 @@
 
 
 @dataclass
 class TextPerplexityResult:
     """Complete perplexity analysis for a text."""
+
     text: str
     model: str
     tokens: List[TokenPerplexity]
     aggregate: Dict[str, float]
 
     def to_dict(self) -> Dict[str, Any]:
         """Convert to dictionary."""
         return {
-            'text': self.text,
-            'model': self.model,
-            'tokens': [t.to_dict() for t in self.tokens],
-            'aggregate': self.aggregate
+            "text": self.text,
+            "model": self.model,
+            "tokens": [t.to_dict() for t in self.tokens],
+            "aggregate": self.aggregate,
         }
 
 
 class LanguageModel(ABC):
     """Abstract interface for language models."""
 
     @abstractmethod
     def compute_token_perplexities(
-        self,
-        text: str,
-        top_k: int = 5,
-        context_window: int = 3
+        self, text: str, top_k: int = 5, context_window: int = 3
     ) -> TextPerplexityResult:
         """
         Compute per-token perplexity for input text.
 
         Args:
@@ -233,53 +235,47 @@
 
 class HuggingFaceLanguageModel(LanguageModel):
     """Wrapper for HuggingFace causal language models."""
 
     def __init__(
-        self,
-        model_name: str,
-        device: str = "cpu",
-        trust_remote_code: bool = False
+        self, model_name: str, device: str = "cpu", trust_remote_code: bool = False
     ):
         """
         Initialize HuggingFace model.
 
         Args:
             model_name: HuggingFace model identifier
             device: Device to use ('cpu', 'cuda', 'mps')
             trust_remote_code: Trust remote code for custom models
         """
         if not HF_AVAILABLE:
-            raise ImportError("transformers not installed. Install with: pip install transformers")
+            raise ImportError(
+                "transformers not installed. Install with: pip install transformers"
+            )
         if not TORCH_AVAILABLE:
             raise ImportError("torch not installed. Install with: pip install torch")
 
         self.model_name = model_name
         self.device = device
 
         print(f"Loading model: {model_name}...")
         self.tokenizer = AutoTokenizer.from_pretrained(
-            model_name,
-            trust_remote_code=trust_remote_code
+            model_name, trust_remote_code=trust_remote_code
         )
         self.model = AutoModelForCausalLM.from_pretrained(
-            model_name,
-            trust_remote_code=trust_remote_code
+            model_name, trust_remote_code=trust_remote_code
         ).to(device)
         self.model.eval()
 
         # Set pad token if not available
         if self.tokenizer.pad_token is None:
             self.tokenizer.pad_token = self.tokenizer.eos_token
 
         print(f"✓ Model loaded on {device}")
 
     def compute_token_perplexities(
-        self,
-        text: str,
-        top_k: int = 5,
-        context_window: int = 3
+        self, text: str, top_k: int = 5, context_window: int = 3
     ) -> TextPerplexityResult:
         """Compute per-token perplexity using HuggingFace model."""
 
         # Tokenize
         inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
@@ -293,11 +289,13 @@
             logits = outputs.logits[0]  # [seq_len, vocab_size]
 
             # Process each token (skip first token as it has no previous context for prediction)
             for i in range(1, len(input_ids)):
                 # Get logits for predicting current token based on previous tokens
-                token_logits = logits[i - 1]  # Logits at position i-1 predict token at position i
+                token_logits = logits[
+                    i - 1
+                ]  # Logits at position i-1 predict token at position i
                 token_id = input_ids[i].item()
 
                 # Compute probabilities
                 probs = F.softmax(token_logits, dim=-1)
                 token_prob = probs[token_id].item()
@@ -317,11 +315,11 @@
                 top_k_probs, top_k_indices = torch.topk(probs, min(top_k, len(probs)))
                 top_k_tokens = [
                     {
                         "token": self.tokenizer.decode([idx.item()]),
                         "token_id": idx.item(),
-                        "prob": prob.item()
+                        "prob": prob.item(),
                     }
                     for prob, idx in zip(top_k_probs, top_k_indices)
                 ]
 
                 # Get context
@@ -338,23 +336,25 @@
                 ]
 
                 # Decode token
                 token_str = self.tokenizer.decode([token_id])
 
-                token_perplexities.append(TokenPerplexity(
-                    token=token_str,
-                    token_id=token_id,
-                    position=i,
-                    logit=logit,
-                    prob=token_prob,
-                    perplexity=perplexity,
-                    entropy=entropy,
-                    rank=rank,
-                    top_k_tokens=top_k_tokens,
-                    context_before=context_before,
-                    context_after=context_after
-                ))
+                token_perplexities.append(
+                    TokenPerplexity(
+                        token=token_str,
+                        token_id=token_id,
+                        position=i,
+                        logit=logit,
+                        prob=token_prob,
+                        perplexity=perplexity,
+                        entropy=entropy,
+                        rank=rank,
+                        top_k_tokens=top_k_tokens,
+                        context_before=context_before,
+                        context_after=context_after,
+                    )
+                )
 
         # Compute aggregate statistics
         perplexities = [tp.perplexity for tp in token_perplexities]
         entropies = [tp.entropy for tp in token_perplexities]
 
@@ -364,34 +364,29 @@
             "std_perplexity": float(np.std(perplexities)),
             "min_perplexity": float(np.min(perplexities)),
             "max_perplexity": float(np.max(perplexities)),
             "mean_entropy": float(np.mean(entropies)),
             "std_entropy": float(np.std(entropies)),
-            "total_tokens": len(token_perplexities)
+            "total_tokens": len(token_perplexities),
         }
 
         return TextPerplexityResult(
             text=text,
             model=self.model_name,
             tokens=token_perplexities,
-            aggregate=aggregate
+            aggregate=aggregate,
         )
 
     def get_model_name(self) -> str:
         """Get model name."""
         return self.model_name
 
 
 class PyTorchLanguageModel(LanguageModel):
     """Wrapper for custom PyTorch language models."""
 
-    def __init__(
-        self,
-        model_path: str,
-        tokenizer_path: str,
-        device: str = "cpu"
-    ):
+    def __init__(self, model_path: str, tokenizer_path: str, device: str = "cpu"):
         """
         Initialize custom PyTorch model.
 
         Args:
             model_path: Path to saved PyTorch model
@@ -408,26 +403,23 @@
         print(f"Loading custom PyTorch model from: {model_path}...")
         self.model = torch.load(model_path, map_location=device)
         self.model.eval()
 
         print(f"Loading tokenizer from: {tokenizer_path}...")
-        if tokenizer_path.endswith('.pkl'):
-            with open(tokenizer_path, 'rb') as f:
+        if tokenizer_path.endswith(".pkl"):
+            with open(tokenizer_path, "rb") as f:
                 self.tokenizer = pickle.load(f)
-        elif tokenizer_path.endswith('.json'):
-            with open(tokenizer_path, 'r') as f:
+        elif tokenizer_path.endswith(".json"):
+            with open(tokenizer_path, "r") as f:
                 self.tokenizer = json.load(f)
         else:
             raise ValueError(f"Unsupported tokenizer format: {tokenizer_path}")
 
         print(f"✓ Model loaded on {device}")
 
     def compute_token_perplexities(
-        self,
-        text: str,
-        top_k: int = 5,
-        context_window: int = 3
+        self, text: str, top_k: int = 5, context_window: int = 3
     ) -> TextPerplexityResult:
         """Compute per-token perplexity using custom PyTorch model."""
         # This is a placeholder - actual implementation depends on your model architecture
         raise NotImplementedError(
             "PyTorch model implementation depends on your specific model architecture. "
@@ -438,14 +430,11 @@
         """Get model name."""
         return Path(self.model_path).stem
 
 
 def load_model(
-    model_type: str,
-    model_name_or_path: str,
-    device: str = "cpu",
-    **kwargs
+    model_type: str, model_name_or_path: str, device: str = "cpu", **kwargs
 ) -> LanguageModel:
     """
     Load language model based on type.
 
     Args:
@@ -459,27 +448,24 @@
     """
     if model_type == "huggingface":
         return HuggingFaceLanguageModel(
             model_name=model_name_or_path,
             device=device,
-            trust_remote_code=kwargs.get('trust_remote_code', False)
+            trust_remote_code=kwargs.get("trust_remote_code", False),
         )
     elif model_type == "pytorch":
         return PyTorchLanguageModel(
             model_path=model_name_or_path,
-            tokenizer_path=kwargs.get('tokenizer_path'),
-            device=device
+            tokenizer_path=kwargs.get("tokenizer_path"),
+            device=device,
         )
     else:
         raise ValueError(f"Unknown model type: {model_type}")
 
 
 def process_single_text(
-    text: str,
-    model: LanguageModel,
-    top_k: int = 5,
-    context_window: int = 3
+    text: str, model: LanguageModel, top_k: int = 5, context_window: int = 3
 ) -> TextPerplexityResult:
     """Process a single text."""
     return model.compute_token_perplexities(text, top_k, context_window)
 
 
@@ -488,19 +474,19 @@
     text_column: str,
     model: LanguageModel,
     top_k: int = 5,
     context_window: int = 3,
     limit: Optional[int] = None,
-    verbose: bool = True
+    verbose: bool = True,
 ) -> List[TextPerplexityResult]:
     """Process CSV file with text column."""
     df = pd.read_csv(csv_path)
 
     if text_column not in df.columns:
         raise ValueError(f"Column '{text_column}' not found in {csv_path}")
 
-    texts = df[text_column].fillna('').astype(str).tolist()
+    texts = df[text_column].fillna("").astype(str).tolist()
 
     if limit:
         texts = texts[:limit]
 
     results = []
@@ -513,17 +499,14 @@
 
     return results
 
 
 def process_text_file(
-    txt_path: str,
-    model: LanguageModel,
-    top_k: int = 5,
-    context_window: int = 3
+    txt_path: str, model: LanguageModel, top_k: int = 5, context_window: int = 3
 ) -> TextPerplexityResult:
     """Process plain text file."""
-    with open(txt_path, 'r', encoding='utf-8') as f:
+    with open(txt_path, "r", encoding="utf-8") as f:
         text = f.read()
 
     return model.compute_token_perplexities(text, top_k, context_window)
 
 
@@ -533,11 +516,11 @@
     text_column: str,
     model: LanguageModel,
     top_k: int = 5,
     context_window: int = 3,
     limit: Optional[int] = None,
-    verbose: bool = True
+    verbose: bool = True,
 ) -> List[TextPerplexityResult]:
     """Process HuggingFace dataset."""
     if not DATASETS_AVAILABLE:
         raise ImportError("datasets not installed. Install with: pip install datasets")
 
@@ -565,11 +548,11 @@
 
 def save_results(
     results: Union[TextPerplexityResult, List[TextPerplexityResult]],
     output_path: str,
     save_format: str = "json",
-    aggregate_only: bool = False
+    aggregate_only: bool = False,
 ):
     """
     Save perplexity results to file.
 
     Args:
@@ -585,47 +568,36 @@
         results = [results]
 
     if save_format == "json":
         # Save as single JSON file
         if aggregate_only:
-            data = [
-                {
-                    'text': r.text,
-                    'model': r.model,
-                    **r.aggregate
-                }
-                for r in results
-            ]
+            data = [{"text": r.text, "model": r.model, **r.aggregate} for r in results]
         else:
             data = [r.to_dict() for r in results]
 
-        with open(output_path, 'w', encoding='utf-8') as f:
+        with open(output_path, "w", encoding="utf-8") as f:
             json.dump(data, f, indent=2, ensure_ascii=False)
 
         print(f"✓ Saved to: {output_path}")
 
     elif save_format == "jsonl":
         # Save as JSON lines
-        with open(output_path, 'w', encoding='utf-8') as f:
+        with open(output_path, "w", encoding="utf-8") as f:
             for r in results:
                 if aggregate_only:
-                    data = {'text': r.text, 'model': r.model, **r.aggregate}
+                    data = {"text": r.text, "model": r.model, **r.aggregate}
                 else:
                     data = r.to_dict()
-                f.write(json.dumps(data, ensure_ascii=False) + '\n')
+                f.write(json.dumps(data, ensure_ascii=False) + "\n")
 
         print(f"✓ Saved to: {output_path}")
 
     elif save_format == "csv":
         # Save aggregates as CSV
         data = []
         for r in results:
-            row = {
-                'text': r.text,
-                'model': r.model,
-                **r.aggregate
-            }
+            row = {"text": r.text, "model": r.model, **r.aggregate}
             data.append(row)
 
         df = pd.DataFrame(data)
         df.to_csv(output_path, index=False)
 
@@ -637,129 +609,104 @@
 
 def main():
     """Main entry point."""
     parser = argparse.ArgumentParser(
         description="Extract per-token perplexity features from language models",
-        formatter_class=argparse.RawDescriptionHelpFormatter
+        formatter_class=argparse.RawDescriptionHelpFormatter,
     )
 
     # Input configuration
     input_group = parser.add_mutually_exclusive_group(required=True)
-    input_group.add_argument(
-        '--text',
+    input_group.add_argument("--text", type=str, help="Single text string to process")
+    input_group.add_argument("-i", "--input", type=str, help="Input file (CSV or TXT)")
+    input_group.add_argument("--dataset", type=str, help="HuggingFace dataset name")
+
+    # CSV/Dataset configuration
+    parser.add_argument(
+        "--text-column",
         type=str,
-        help='Single text string to process'
-    )
-    input_group.add_argument(
-        '-i', '--input',
+        default="text",
+        help="Column name containing text (default: text)",
+    )
+    parser.add_argument(
+        "--dataset-split",
         type=str,
-        help='Input file (CSV or TXT)'
-    )
-    input_group.add_argument(
-        '--dataset',
+        default="test",
+        help="Dataset split to use (default: test)",
+    )
+    parser.add_argument("--limit", type=int, help="Limit number of texts to process")
+
+    # Model configuration
+    parser.add_argument(
+        "--model-type",
         type=str,
-        help='HuggingFace dataset name'
-    )
-
-    # CSV/Dataset configuration
-    parser.add_argument(
-        '--text-column',
+        choices=["huggingface", "pytorch"],
+        default="huggingface",
+        help="Model type (default: huggingface)",
+    )
+    parser.add_argument(
+        "--model", type=str, help="Model name (HuggingFace) or path (PyTorch)"
+    )
+    parser.add_argument(
+        "--tokenizer-path", type=str, help="Path to tokenizer (for PyTorch models)"
+    )
+    parser.add_argument(
+        "--device",
         type=str,
-        default='text',
-        help='Column name containing text (default: text)'
-    )
-    parser.add_argument(
-        '--dataset-split',
-        type=str,
-        default='test',
-        help='Dataset split to use (default: test)'
-    )
-    parser.add_argument(
-        '--limit',
-        type=int,
-        help='Limit number of texts to process'
-    )
-
-    # Model configuration
-    parser.add_argument(
-        '--model-type',
-        type=str,
-        choices=['huggingface', 'pytorch'],
-        default='huggingface',
-        help='Model type (default: huggingface)'
-    )
-    parser.add_argument(
-        '--model',
-        type=str,
-        help='Model name (HuggingFace) or path (PyTorch)'
-    )
-    parser.add_argument(
-        '--tokenizer-path',
-        type=str,
-        help='Path to tokenizer (for PyTorch models)'
-    )
-    parser.add_argument(
-        '--device',
-        type=str,
-        default='cpu',
-        choices=['cpu', 'cuda', 'mps'],
-        help='Device to use (default: cpu)'
-    )
-    parser.add_argument(
-        '--trust-remote-code',
-        action='store_true',
-        help='Trust remote code for custom HuggingFace models'
+        default="cpu",
+        choices=["cpu", "cuda", "mps"],
+        help="Device to use (default: cpu)",
+    )
+    parser.add_argument(
+        "--trust-remote-code",
+        action="store_true",
+        help="Trust remote code for custom HuggingFace models",
     )
 
     # Feature configuration
     parser.add_argument(
-        '--top-k',
+        "--top-k",
         type=int,
         default=5,
-        help='Number of top alternative tokens to save (default: 5)'
-    )
-    parser.add_argument(
-        '--context-window',
+        help="Number of top alternative tokens to save (default: 5)",
+    )
+    parser.add_argument(
+        "--context-window",
         type=int,
         default=3,
-        help='Number of context tokens before/after (default: 3)'
+        help="Number of context tokens before/after (default: 3)",
     )
 
     # Output configuration
-    parser.add_argument(
-        '--output',
+    parser.add_argument("--output", type=str, required=True, help="Output file path")
+    parser.add_argument(
+        "--save-format",
         type=str,
-        required=True,
-        help='Output file path'
-    )
-    parser.add_argument(
-        '--save-format',
-        type=str,
-        choices=['json', 'jsonl', 'csv'],
-        default='json',
-        help='Output format (default: json)'
-    )
-    parser.add_argument(
-        '--aggregate-only',
-        action='store_true',
-        help='Save only aggregate statistics, not per-token details'
+        choices=["json", "jsonl", "csv"],
+        default="json",
+        help="Output format (default: json)",
+    )
+    parser.add_argument(
+        "--aggregate-only",
+        action="store_true",
+        help="Save only aggregate statistics, not per-token details",
     )
 
     # Other options
     parser.add_argument(
-        '-q', '--quiet',
-        action='store_true',
-        help='Suppress verbose output'
+        "-q", "--quiet", action="store_true", help="Suppress verbose output"
     )
 
     args = parser.parse_args()
 
     # Check dependencies
-    if args.model_type == 'huggingface' and not HF_AVAILABLE:
-        parser.error("transformers not installed. Install with: pip install transformers")
-
-    if args.model_type == 'pytorch' and not args.tokenizer_path:
+    if args.model_type == "huggingface" and not HF_AVAILABLE:
+        parser.error(
+            "transformers not installed. Install with: pip install transformers"
+        )
+
+    if args.model_type == "pytorch" and not args.tokenizer_path:
         parser.error("--tokenizer-path required for PyTorch models")
 
     if not TORCH_AVAILABLE:
         parser.error("torch not installed. Install with: pip install torch")
 
@@ -771,11 +718,11 @@
         model = load_model(
             model_type=args.model_type,
             model_name_or_path=args.model,
             device=args.device,
             tokenizer_path=args.tokenizer_path,
-            trust_remote_code=args.trust_remote_code
+            trust_remote_code=args.trust_remote_code,
         )
     except Exception as e:
         parser.error(f"Failed to load model: {e}")
 
     # Process input
@@ -784,33 +731,33 @@
             # Single text
             result = process_single_text(
                 text=args.text,
                 model=model,
                 top_k=args.top_k,
-                context_window=args.context_window
+                context_window=args.context_window,
             )
             results = [result]
 
         elif args.input:
             # File input
             input_path = Path(args.input)
-            if input_path.suffix == '.csv':
+            if input_path.suffix == ".csv":
                 results = process_csv_file(
                     csv_path=str(input_path),
                     text_column=args.text_column,
                     model=model,
                     top_k=args.top_k,
                     context_window=args.context_window,
                     limit=args.limit,
-                    verbose=not args.quiet
+                    verbose=not args.quiet,
                 )
-            elif input_path.suffix == '.txt':
+            elif input_path.suffix == ".txt":
                 result = process_text_file(
                     txt_path=str(input_path),
                     model=model,
                     top_k=args.top_k,
-                    context_window=args.context_window
+                    context_window=args.context_window,
                 )
                 results = [result]
             else:
                 parser.error(f"Unsupported file type: {input_path.suffix}")
 
@@ -822,26 +769,26 @@
                 text_column=args.text_column,
                 model=model,
                 top_k=args.top_k,
                 context_window=args.context_window,
                 limit=args.limit,
-                verbose=not args.quiet
+                verbose=not args.quiet,
             )
 
         else:
             parser.error("Must specify --text, -i/--input, or --dataset")
 
         # Save results
         save_results(
             results=results,
             output_path=args.output,
             save_format=args.save_format,
-            aggregate_only=args.aggregate_only
+            aggregate_only=args.aggregate_only,
         )
 
     except Exception as e:
         parser.error(f"Error during processing: {e}")
         raise
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     main()
would reformat /home/b/p/cefr-classification/minimal-cefr/src/extract_perplexity_features.py
--- /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf_groupby.py	2025-10-18 02:50:29.093729+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf_groupby.py	2025-10-18 11:24:39.011856+00:00
@@ -15,11 +15,17 @@
 import pandas as pd
 import numpy as np
 from sklearn.feature_extraction.text import TfidfVectorizer
 from sklearn.base import BaseEstimator, TransformerMixin
 
-from src.config import GlobalConfig, ExperimentConfig, TfidfConfig, DataConfig, OutputConfig
+from src.config import (
+    GlobalConfig,
+    ExperimentConfig,
+    TfidfConfig,
+    DataConfig,
+    OutputConfig,
+)
 
 
 class GroupedTfidfVectorizer(BaseEstimator, TransformerMixin):
     """
     Composite TF-IDF vectorizer that trains separate models per group
@@ -31,11 +37,11 @@
         group_column: str,
         max_features: int = 5000,
         ngram_range: tuple = (1, 2),
         min_df: int = 2,
         max_df: float = 0.95,
-        sublinear_tf: bool = True
+        sublinear_tf: bool = True,
     ):
         self.group_column = group_column
         self.max_features = max_features
         self.ngram_range = ngram_range
         self.min_df = min_df
@@ -57,34 +63,36 @@
 
         Returns:
             self
         """
         if self.group_column not in X.columns:
-            raise ValueError(f"Group column '{self.group_column}' not found in DataFrame")
-
-        if 'text' not in X.columns:
+            raise ValueError(
+                f"Group column '{self.group_column}' not found in DataFrame"
+            )
+
+        if "text" not in X.columns:
             raise ValueError("'text' column required in DataFrame")
 
         # Get unique groups (sorted for consistency)
         self.groups_ = sorted(X[self.group_column].unique().astype(str))
 
         # Train one vectorizer per group
         for group in self.groups_:
             # Get texts for this group
             group_mask = X[self.group_column].astype(str) == group
-            group_texts = X.loc[group_mask, 'text'].fillna('').astype(str)
+            group_texts = X.loc[group_mask, "text"].fillna("").astype(str)
 
             if len(group_texts) == 0:
                 continue
 
             # Train TF-IDF for this group
             vectorizer = TfidfVectorizer(
                 max_features=self.max_features,
                 ngram_range=self.ngram_range,
                 min_df=self.min_df,
                 max_df=self.max_df,
-                sublinear_tf=self.sublinear_tf
+                sublinear_tf=self.sublinear_tf,
             )
 
             vectorizer.fit(group_texts)
             self.group_vectorizers_[group] = vectorizer
 
@@ -108,18 +116,18 @@
         Returns:
             Concatenated feature matrix
         """
         # Handle different input types
         if isinstance(X, pd.DataFrame):
-            if 'text' not in X.columns:
+            if "text" not in X.columns:
                 raise ValueError("'text' column required in DataFrame")
-            texts = X['text'].fillna('').astype(str)
+            texts = X["text"].fillna("").astype(str)
         elif isinstance(X, pd.Series):
-            texts = X.fillna('').astype(str)
+            texts = X.fillna("").astype(str)
         else:
             # Assume array-like
-            texts = pd.Series(X).fillna('').astype(str)
+            texts = pd.Series(X).fillna("").astype(str)
 
         # Transform with each group's vectorizer
         feature_matrices = []
 
         for group in self.groups_:
@@ -141,16 +149,16 @@
         return np.array(self.feature_names_)
 
     def get_params(self, deep=True):
         """Get parameters for this estimator."""
         return {
-            'group_column': self.group_column,
-            'max_features': self.max_features,
-            'ngram_range': self.ngram_range,
-            'min_df': self.min_df,
-            'max_df': self.max_df,
-            'sublinear_tf': self.sublinear_tf
+            "group_column": self.group_column,
+            "max_features": self.max_features,
+            "ngram_range": self.ngram_range,
+            "min_df": self.min_df,
+            "max_df": self.max_df,
+            "sublinear_tf": self.sublinear_tf,
         }
 
     def set_params(self, **params):
         """Set parameters for this estimator."""
         for key, value in params.items():
@@ -186,22 +194,26 @@
         print(f"Loading features training data: {training_file}")
 
     # Load data
     df = pd.read_csv(training_file)
     if data_config.text_column not in df.columns:
-        raise ValueError(f"'{data_config.text_column}' column required in {training_file}")
+        raise ValueError(
+            f"'{data_config.text_column}' column required in {training_file}"
+        )
 
     if group_column not in df.columns:
         raise ValueError(f"Group column '{group_column}' not found in {training_file}")
 
     # Prepare DataFrame with text and group columns
     df_filtered = df[[data_config.text_column, group_column]].copy()
-    df_filtered.columns = ['text', group_column]
+    df_filtered.columns = ["text", group_column]
 
     # Apply filtering
     if data_config.min_text_length > 0:
-        df_filtered = df_filtered[df_filtered['text'].fillna('').str.len() >= data_config.min_text_length]
+        df_filtered = df_filtered[
+            df_filtered["text"].fillna("").str.len() >= data_config.min_text_length
+        ]
 
     if verbose:
         print(f"Training samples: {len(df_filtered)}")
         print(f"Grouping by: {group_column}")
         group_counts = df_filtered[group_column].value_counts().sort_index()
@@ -217,11 +229,11 @@
         group_column=group_column,
         max_features=tfidf_config.max_features,
         ngram_range=tfidf_config.ngram_range,
         min_df=tfidf_config.min_df,
         max_df=tfidf_config.max_df,
-        sublinear_tf=tfidf_config.sublinear_tf
+        sublinear_tf=tfidf_config.sublinear_tf,
     )
 
     grouped_tfidf.fit(df_filtered)
 
     # Save model using new naming scheme: {hash}_tfidf_grouped
@@ -232,21 +244,23 @@
     output_dir = Path(exp_config.get_feature_model_dir(tfidf_hash, feature_type))
     output_dir.mkdir(parents=True, exist_ok=True)
 
     if config.output_config.save_models:
         model_path = output_dir / "tfidf_model.pkl"
-        with open(model_path, 'wb') as f:
+        with open(model_path, "wb") as f:
             pickle.dump(grouped_tfidf, f)
 
         if verbose:
             print(f"\n✓ Grouped TF-IDF model saved to: {model_path}")
             print(f"✓ Number of groups: {len(grouped_tfidf.groups_)}")
             print(f"✓ Total vocabulary size: {len(grouped_tfidf.feature_names_)}")
             print(f"✓ Feature dimensions per group:")
             for group in grouped_tfidf.groups_:
                 if group in grouped_tfidf.group_vectorizers_:
-                    vocab_size = len(grouped_tfidf.group_vectorizers_[group].vocabulary_)
+                    vocab_size = len(
+                        grouped_tfidf.group_vectorizers_[group].vocabulary_
+                    )
                     print(f"    - {group}: {vocab_size} features")
             print(f"✓ Config hash: {tfidf_hash}")
             print(f"✓ Feature type: {feature_type}")
             print(f"✓ Readable name: {tfidf_config.get_readable_name()}_{feature_type}")
 
@@ -269,16 +283,16 @@
                 group: len(grouped_tfidf.group_vectorizers_[group].vocabulary_)
                 for group in grouped_tfidf.groups_
                 if group in grouped_tfidf.group_vectorizers_
             },
             "training_file": training_file.name,
-            "training_samples": len(df_filtered)
+            "training_samples": len(df_filtered),
         }
 
         if config.output_config.save_json:
             config_path = output_dir / "config.json"
-            with open(config_path, 'w') as f:
+            with open(config_path, "w") as f:
                 json.dump(model_config, f, indent=2)
 
             if verbose:
                 print(f"✓ Config saved to: {config_path}")
 
@@ -310,135 +324,125 @@
          -c config.yaml \\
          --group-by cefr_level
 
 Note: This creates one TF-IDF model per group value and concatenates
 their features. For CEFR levels, this captures level-specific vocabulary.
-"""
+""",
     )
 
     # Config loading
     config_group = parser.add_argument_group("Configuration Loading")
     config_method = config_group.add_mutually_exclusive_group()
     config_method.add_argument(
-        "-c", "--config-file",
-        help="Path to JSON or YAML config file"
+        "-c", "--config-file", help="Path to JSON or YAML config file"
     )
     config_method.add_argument(
-        "--config-json",
-        help="JSON string containing full configuration"
+        "--config-json", help="JSON string containing full configuration"
     )
 
     # Experiment configuration
     exp_group = parser.add_argument_group("Experiment Configuration")
     exp_group.add_argument(
-        "-e", "--experiment-dir",
-        help="Path to experiment directory (e.g., data/experiments/zero-shot)"
+        "-e",
+        "--experiment-dir",
+        help="Path to experiment directory (e.g., data/experiments/zero-shot)",
     )
     exp_group.add_argument(
-        "-o", "--output-dir",
-        help="Custom output directory for models (default: <experiment-dir>/feature-models)"
+        "-o",
+        "--output-dir",
+        help="Custom output directory for models (default: <experiment-dir>/feature-models)",
     )
 
     # Grouping configuration (REQUIRED)
     group_group = parser.add_argument_group("Grouping Configuration")
     group_group.add_argument(
         "--group-by",
         required=True,
-        help="Column name to group by (e.g., 'cefr_level') - REQUIRED"
+        help="Column name to group by (e.g., 'cefr_level') - REQUIRED",
     )
 
     # TF-IDF configuration
     tfidf_group = parser.add_argument_group("TF-IDF Configuration")
     tfidf_group.add_argument(
         "--max-features",
         type=int,
-        help="Maximum number of TF-IDF features per group (default: 5000)"
+        help="Maximum number of TF-IDF features per group (default: 5000)",
     )
     tfidf_group.add_argument(
-        "--ngram-min",
-        type=int,
-        default=1,
-        help="Minimum n-gram size (default: 1)"
+        "--ngram-min", type=int, default=1, help="Minimum n-gram size (default: 1)"
     )
     tfidf_group.add_argument(
-        "--ngram-max",
-        type=int,
-        default=2,
-        help="Maximum n-gram size (default: 2)"
+        "--ngram-max", type=int, default=2, help="Maximum n-gram size (default: 2)"
     )
     tfidf_group.add_argument(
-        "--min-df",
-        type=int,
-        help="Minimum document frequency (default: 2)"
+        "--min-df", type=int, help="Minimum document frequency (default: 2)"
     )
     tfidf_group.add_argument(
-        "--max-df",
-        type=float,
-        help="Maximum document frequency (default: 0.95)"
+        "--max-df", type=float, help="Maximum document frequency (default: 0.95)"
     )
     tfidf_group.add_argument(
-        "--no-sublinear-tf",
-        action="store_true",
-        help="Disable sublinear TF scaling"
+        "--no-sublinear-tf", action="store_true", help="Disable sublinear TF scaling"
     )
 
     # Data configuration
     data_group = parser.add_argument_group("Data Configuration")
     data_group.add_argument(
         "--text-column",
         default="text",
-        help="Column name containing text (default: text)"
+        help="Column name containing text (default: text)",
     )
     data_group.add_argument(
-        "--min-length",
-        type=int,
-        help="Minimum text length filter (default: 0)"
+        "--min-length", type=int, help="Minimum text length filter (default: 0)"
     )
 
     # Output configuration
     output_group = parser.add_argument_group("Output Configuration")
     output_group.add_argument(
-        "--no-save-config",
-        action="store_true",
-        help="Skip saving configuration files"
+        "--no-save-config", action="store_true", help="Skip saving configuration files"
     )
     output_group.add_argument(
-        "-q", "--quiet",
-        action="store_true",
-        help="Suppress verbose output"
+        "-q", "--quiet", action="store_true", help="Suppress verbose output"
     )
 
     return parser
 
 
 def args_to_config(args: argparse.Namespace) -> GlobalConfig:
     """Convert argparse namespace to GlobalConfig."""
     # Check if config file or json string provided
     if args.config_file:
         config_path = Path(args.config_file)
-        if config_path.suffix in ['.yaml', '.yml']:
+        if config_path.suffix in [".yaml", ".yml"]:
             config = GlobalConfig.from_yaml_file(str(config_path))
-        elif config_path.suffix == '.json':
+        elif config_path.suffix == ".json":
             config = GlobalConfig.from_json_file(str(config_path))
         else:
             raise ValueError(f"Unsupported config file format: {config_path.suffix}")
 
         # CLI args override config file
         if args.experiment_dir:
             config.experiment_config = ExperimentConfig(
                 experiment_dir=args.experiment_dir,
-                models_dir=args.output_dir if args.output_dir else None
+                models_dir=args.output_dir if args.output_dir else None,
             )
         elif args.output_dir:
             config.experiment_config.models_dir = args.output_dir
 
         if args.max_features:
             config.tfidf_config.max_features = args.max_features
         if args.ngram_min or args.ngram_max:
             config.tfidf_config.ngram_range = (
-                args.ngram_min if args.ngram_min else config.tfidf_config.ngram_range[0],
-                args.ngram_max if args.ngram_max else config.tfidf_config.ngram_range[1]
+                (
+                    args.ngram_min
+                    if args.ngram_min
+                    else config.tfidf_config.ngram_range[0]
+                ),
+                (
+                    args.ngram_max
+                    if args.ngram_max
+                    else config.tfidf_config.ngram_range[1]
+                ),
             )
         if args.min_df:
             config.tfidf_config.min_df = args.min_df
         if args.max_df:
             config.tfidf_config.max_df = args.max_df
@@ -466,32 +470,35 @@
 
     else:
         # Build config from CLI args
         experiment_config = ExperimentConfig(
             experiment_dir=args.experiment_dir or "data/experiments/zero-shot",
-            models_dir=args.output_dir if args.output_dir else None
+            models_dir=args.output_dir if args.output_dir else None,
         )
 
         tfidf_config = TfidfConfig(
             max_features=args.max_features or 5000,
             ngram_range=(args.ngram_min, args.ngram_max),
             min_df=args.min_df or 2,
             max_df=args.max_df or 0.95,
-            sublinear_tf=not args.no_sublinear_tf
+            sublinear_tf=not args.no_sublinear_tf,
         )
 
         data_config = DataConfig(
-            text_column=args.text_column,
-            min_text_length=args.min_length or 0
+            text_column=args.text_column, min_text_length=args.min_length or 0
         )
 
         output_config = OutputConfig(
-            save_config=not args.no_save_config,
-            verbose=not args.quiet
+            save_config=not args.no_save_config, verbose=not args.quiet
         )
 
-        return GlobalConfig(experiment_config, tfidf_config, data_config=data_config, output_config=output_config)
+        return GlobalConfig(
+            experiment_config,
+            tfidf_config,
+            data_config=data_config,
+            output_config=output_config,
+        )
 
 
 def main():
     """Main entry point for Grouped TF-IDF training."""
     parser = create_parser()
would reformat /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf_groupby.py
--- /home/b/p/cefr-classification/minimal-cefr/src/extract_features.py	2025-10-18 03:04:40.976764+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/extract_features.py	2025-10-18 11:24:39.078390+00:00
@@ -321,18 +321,17 @@
 from typing import List, Optional
 import pandas as pd
 import numpy as np
 
 from src.config import GlobalConfig, ExperimentConfig, DataConfig, OutputConfig
+
 # Import GroupedTfidfVectorizer so pickle can load it
 from src.train_tfidf_groupby import GroupedTfidfVectorizer
 
 
 def extract_features_for_file(
-    config: GlobalConfig,
-    file_name: str,
-    data_source: str = "test"
+    config: GlobalConfig, file_name: str, data_source: str = "test"
 ) -> Path:
     """
     Extract TF-IDF features for a specific data file.
 
     Args:
@@ -364,11 +363,11 @@
     config_path = tfidf_model_dir / "config.json"
     feature_type = "tfidf"  # Default
     config_hash = tfidf_config.get_hash()  # Default
 
     if config_path.exists():
-        with open(config_path, 'r') as f:
+        with open(config_path, "r") as f:
             model_config = json.load(f)
             # Get feature_type, with backward compatibility
             feature_type = model_config.get("feature_type")
             if feature_type is None:
                 # Old config format - infer from model_type
@@ -376,26 +375,30 @@
                     feature_type = "tfidf_grouped"
                 else:
                     feature_type = "tfidf"
 
             # Get config_hash (old configs use "tfidf_hash")
-            config_hash = model_config.get("config_hash") or model_config.get("tfidf_hash", config_hash)
+            config_hash = model_config.get("config_hash") or model_config.get(
+                "tfidf_hash", config_hash
+            )
 
     if verbose:
         print(f"Loading TF-IDF model: {tfidf_model_path}")
         print(f"Feature type: {feature_type}")
 
-    with open(tfidf_model_path, 'rb') as f:
+    with open(tfidf_model_path, "rb") as f:
         tfidf = pickle.load(f)
 
     # Load data from appropriate source
     if data_source == "test":
         data_dir = exp_config.ml_test_dir
     elif data_source == "training":
         data_dir = exp_config.ml_training_dir
     else:
-        raise ValueError(f"Invalid data_source: {data_source}. Must be 'test' or 'training'")
+        raise ValueError(
+            f"Invalid data_source: {data_source}. Must be 'test' or 'training'"
+        )
 
     data_file_path = Path(data_dir) / file_name
     if not data_file_path.exists():
         raise FileNotFoundError(f"Data file not found: {data_file_path}")
 
@@ -403,13 +406,15 @@
         print(f"Loading data from: {data_file_path}")
 
     df_data = pd.read_csv(data_file_path)
 
     if data_config.text_column not in df_data.columns:
-        raise ValueError(f"'{data_config.text_column}' column required in {data_file_path}")
-
-    X_data = df_data[data_config.text_column].fillna('').astype(str)
+        raise ValueError(
+            f"'{data_config.text_column}' column required in {data_file_path}"
+        )
+
+    X_data = df_data[data_config.text_column].fillna("").astype(str)
 
     if verbose:
         print(f"Samples: {len(X_data)}")
 
     # Extract features
@@ -417,11 +422,11 @@
         print("Extracting TF-IDF features...")
 
     X_data_tfidf = tfidf.transform(X_data)
 
     # Handle both sparse matrices (standard TF-IDF) and dense arrays (grouped TF-IDF)
-    if hasattr(X_data_tfidf, 'toarray'):
+    if hasattr(X_data_tfidf, "toarray"):
         X_data_dense = X_data_tfidf.toarray()
     else:
         X_data_dense = X_data_tfidf
 
     # Create output folder using feature_type (e.g., features/252cd532_tfidf/dataset)
@@ -436,11 +441,13 @@
         pd.DataFrame(X_data_dense).to_csv(features_dense_path, index=False)
 
         # Save feature_names.csv
         feature_names = tfidf.get_feature_names_out()
         feature_names_path = features_dir / "feature_names.csv"
-        pd.DataFrame({'feature_name': feature_names}).to_csv(feature_names_path, index=False)
+        pd.DataFrame({"feature_name": feature_names}).to_csv(
+            feature_names_path, index=False
+        )
 
         if verbose:
             print(f"\n✓ Features saved to: {features_dir}")
             print(f"  - features_dense.csv: {X_data_dense.shape}")
             print(f"  - feature_names.csv: {len(feature_names)} features")
@@ -452,27 +459,26 @@
             "data_source": data_source,
             "feature_type": feature_type,
             "config_hash": config_hash,
             "n_samples": int(len(X_data)),
             "n_features": int(X_data_dense.shape[1]),
-            "model_path": str(tfidf_model_path.relative_to(exp_config.experiment_dir))
+            "model_path": str(tfidf_model_path.relative_to(exp_config.experiment_dir)),
         }
 
         if config.output_config.save_json:
             config_path = features_dir / "config.json"
-            with open(config_path, 'w') as f:
+            with open(config_path, "w") as f:
                 json.dump(feature_config, f, indent=2)
 
             if verbose:
                 print(f"  - config.json")
 
     return features_dir
 
 
 def extract_all_from_source(
-    config: GlobalConfig,
-    data_source: str = "test"
+    config: GlobalConfig, data_source: str = "test"
 ) -> List[Path]:
     """
     Extract features for all files in specified data source.
 
     Args:
@@ -551,100 +557,101 @@
 
 def create_parser() -> argparse.ArgumentParser:
     """Create argument parser for extract_features."""
     parser = argparse.ArgumentParser(
         description="Extract TF-IDF features for CEFR test sets",
-        formatter_class=argparse.RawDescriptionHelpFormatter
+        formatter_class=argparse.RawDescriptionHelpFormatter,
     )
 
     # Config loading
     config_group = parser.add_argument_group("Configuration Loading")
     config_method = config_group.add_mutually_exclusive_group()
     config_method.add_argument(
-        "-c", "--config-file",
-        help="Path to JSON or YAML config file"
+        "-c", "--config-file", help="Path to JSON or YAML config file"
     )
     config_method.add_argument(
-        "--config-json",
-        help="JSON string containing full configuration"
+        "--config-json", help="JSON string containing full configuration"
     )
 
     # Experiment configuration
     exp_group = parser.add_argument_group("Experiment Configuration")
     exp_group.add_argument(
-        "-e", "--experiment-dir",
-        help="Path to experiment directory (e.g., data/experiments/zero-shot)"
+        "-e",
+        "--experiment-dir",
+        help="Path to experiment directory (e.g., data/experiments/zero-shot)",
     )
     exp_group.add_argument(
-        "-o", "--output-dir",
-        help="Custom output directory for models (default: <experiment-dir>/feature-models)"
+        "-o",
+        "--output-dir",
+        help="Custom output directory for models (default: <experiment-dir>/feature-models)",
     )
     exp_group.add_argument(
-        "-p", "--pretrained-dir",
-        help="Directory containing pre-trained TF-IDF model (default: <output-dir>/tfidf)"
+        "-p",
+        "--pretrained-dir",
+        help="Directory containing pre-trained TF-IDF model (default: <output-dir>/tfidf)",
     )
 
     # Data file selection
     data_group = parser.add_argument_group("Data File Selection")
     data_group.add_argument(
-        "-t", "--test-file",
-        help="Specific file name to process (e.g., norm-celva-sp.csv). If not provided, processes all files from data source."
+        "-t",
+        "--test-file",
+        help="Specific file name to process (e.g., norm-celva-sp.csv). If not provided, processes all files from data source.",
     )
     data_group.add_argument(
-        "-s", "--data-source",
+        "-s",
+        "--data-source",
         choices=["test", "training", "both"],
         default="test",
-        help="Data source to process: 'test' (ml-test-data), 'training' (ml-training-data), or 'both' (default: test)"
+        help="Data source to process: 'test' (ml-test-data), 'training' (ml-training-data), or 'both' (default: test)",
     )
 
     # Data configuration
     data_group = parser.add_argument_group("Data Configuration")
     data_group.add_argument(
         "--text-column",
         default="text",
-        help="Column name containing text (default: text)"
+        help="Column name containing text (default: text)",
     )
 
     # Output configuration
     output_group = parser.add_argument_group("Output Configuration")
     output_group.add_argument(
-        "--no-save-config",
-        action="store_true",
-        help="Skip saving configuration files"
+        "--no-save-config", action="store_true", help="Skip saving configuration files"
     )
     output_group.add_argument(
         "--no-save-features",
         action="store_true",
-        help="Skip saving feature files (for testing)"
+        help="Skip saving feature files (for testing)",
     )
     output_group.add_argument(
-        "-q", "--quiet",
-        action="store_true",
-        help="Suppress verbose output"
+        "-q", "--quiet", action="store_true", help="Suppress verbose output"
     )
 
     return parser
 
 
 def args_to_config(args: argparse.Namespace) -> GlobalConfig:
     """Convert argparse namespace to GlobalConfig."""
     # Check if config file or json string provided
     if args.config_file:
         config_path = Path(args.config_file)
-        if config_path.suffix in ['.yaml', '.yml']:
+        if config_path.suffix in [".yaml", ".yml"]:
             config = GlobalConfig.from_yaml_file(str(config_path))
-        elif config_path.suffix == '.json':
+        elif config_path.suffix == ".json":
             config = GlobalConfig.from_json_file(str(config_path))
         else:
             raise ValueError(f"Unsupported config file format: {config_path.suffix}")
 
         # CLI args override config file
         if args.experiment_dir:
             config.experiment_config = ExperimentConfig(
                 experiment_dir=args.experiment_dir,
                 models_dir=args.output_dir if args.output_dir else None,
-                pretrained_tfidf_dir=args.pretrained_dir if args.pretrained_dir else None
+                pretrained_tfidf_dir=(
+                    args.pretrained_dir if args.pretrained_dir else None
+                ),
             )
         elif args.output_dir or args.pretrained_dir:
             if args.output_dir:
                 config.experiment_config.models_dir = args.output_dir
             if args.pretrained_dir:
@@ -674,27 +681,23 @@
     else:
         # Build config from CLI args
         experiment_config = ExperimentConfig(
             experiment_dir=args.experiment_dir or "data/experiments/zero-shot",
             models_dir=args.output_dir if args.output_dir else None,
-            pretrained_tfidf_dir=args.pretrained_dir if args.pretrained_dir else None
+            pretrained_tfidf_dir=args.pretrained_dir if args.pretrained_dir else None,
         )
 
-        data_config = DataConfig(
-            text_column=args.text_column
-        )
+        data_config = DataConfig(text_column=args.text_column)
 
         output_config = OutputConfig(
             save_config=not args.no_save_config,
             save_features=not args.no_save_features,
-            verbose=not args.quiet
+            verbose=not args.quiet,
         )
 
         return GlobalConfig(
-            experiment_config,
-            data_config=data_config,
-            output_config=output_config
+            experiment_config, data_config=data_config, output_config=output_config
         )
 
 
 def main():
     """Main entry point for feature extraction."""
@@ -713,19 +716,21 @@
 
         # Show TF-IDF model location
         if config.experiment_config.pretrained_tfidf_dir:
             tfidf_dir = Path(config.experiment_config.pretrained_tfidf_dir)
         else:
-            tfidf_dir = Path(config.experiment_config.get_tfidf_model_dir(config.tfidf_config))
+            tfidf_dir = Path(
+                config.experiment_config.get_tfidf_model_dir(config.tfidf_config)
+            )
 
         # Load model config to get feature_type
         model_config_path = tfidf_dir / "config.json"
         feature_type = "tfidf"  # Default
         config_hash = config.tfidf_config.get_hash()  # Default
 
         if model_config_path.exists():
-            with open(model_config_path, 'r') as f:
+            with open(model_config_path, "r") as f:
                 model_cfg = json.load(f)
                 # Get feature_type, with backward compatibility
                 feature_type = model_cfg.get("feature_type")
                 if feature_type is None:
                     # Old config format - infer from model_type
@@ -733,15 +738,19 @@
                         feature_type = "tfidf_grouped"
                     else:
                         feature_type = "tfidf"
 
                 # Get config_hash (old configs use "tfidf_hash")
-                config_hash = model_cfg.get("config_hash") or model_cfg.get("tfidf_hash", config_hash)
+                config_hash = model_cfg.get("config_hash") or model_cfg.get(
+                    "tfidf_hash", config_hash
+                )
 
         print(f"\nTF-IDF model directory: {tfidf_dir}")
         print(f"Config hash: {config_hash}")
-        print(f"Features output directory: {config.experiment_config.get_features_dir(config_hash, feature_type)}")
+        print(
+            f"Features output directory: {config.experiment_config.get_features_dir(config_hash, feature_type)}"
+        )
         print()
 
     # Extract features
     try:
         if args.test_file:
would reformat /home/b/p/cefr-classification/minimal-cefr/src/extract_features.py
--- /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf.py	2025-10-18 02:49:56.192116+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf.py	2025-10-18 11:24:39.278507+00:00
@@ -11,11 +11,17 @@
 from pathlib import Path
 from typing import Optional
 import pandas as pd
 from sklearn.feature_extraction.text import TfidfVectorizer
 
-from src.config import GlobalConfig, ExperimentConfig, TfidfConfig, DataConfig, OutputConfig
+from src.config import (
+    GlobalConfig,
+    ExperimentConfig,
+    TfidfConfig,
+    DataConfig,
+    OutputConfig,
+)
 
 
 def train_tfidf(config: GlobalConfig) -> Path:
     """
     Train TF-IDF vectorizer on features-training-data.
@@ -43,13 +49,15 @@
         print(f"Loading features training data: {training_file}")
 
     # Load data
     df = pd.read_csv(training_file)
     if data_config.text_column not in df.columns:
-        raise ValueError(f"'{data_config.text_column}' column required in {training_file}")
-
-    X_train = df[data_config.text_column].fillna('').astype(str)
+        raise ValueError(
+            f"'{data_config.text_column}' column required in {training_file}"
+        )
+
+    X_train = df[data_config.text_column].fillna("").astype(str)
 
     # Apply filtering
     if data_config.min_text_length > 0:
         X_train = X_train[X_train.str.len() >= data_config.min_text_length]
 
@@ -63,21 +71,21 @@
     tfidf = TfidfVectorizer(
         max_features=tfidf_config.max_features,
         ngram_range=tfidf_config.ngram_range,
         min_df=tfidf_config.min_df,
         max_df=tfidf_config.max_df,
-        sublinear_tf=tfidf_config.sublinear_tf
+        sublinear_tf=tfidf_config.sublinear_tf,
     )
     tfidf.fit(X_train)
 
     # Save model to hashed directory (prevents overwrites with different TF-IDF configs)
     output_dir = Path(exp_config.get_tfidf_model_dir(tfidf_config))
     output_dir.mkdir(parents=True, exist_ok=True)
 
     if config.output_config.save_models:
         model_path = output_dir / "tfidf_model.pkl"
-        with open(model_path, 'wb') as f:
+        with open(model_path, "wb") as f:
             pickle.dump(tfidf, f)
 
         if verbose:
             print(f"\n✓ TF-IDF model saved to: {model_path}")
             print(f"✓ Vocabulary size: {len(tfidf.vocabulary_)}")
@@ -96,16 +104,16 @@
             "min_df": tfidf_config.min_df,
             "max_df": tfidf_config.max_df,
             "sublinear_tf": tfidf_config.sublinear_tf,
             "vocabulary_size": len(tfidf.vocabulary_),
             "training_file": training_file.name,
-            "training_samples": len(X_train)
+            "training_samples": len(X_train),
         }
 
         if config.output_config.save_json:
             config_path = output_dir / "config.json"
-            with open(config_path, 'w') as f:
+            with open(config_path, "w") as f:
                 json.dump(model_config, f, indent=2)
 
             if verbose:
                 print(f"✓ Config saved to: {config_path}")
 
@@ -114,127 +122,117 @@
 
 def create_parser() -> argparse.ArgumentParser:
     """Create argument parser for train_tfidf."""
     parser = argparse.ArgumentParser(
         description="Train TF-IDF Vectorizer for CEFR classification",
-        formatter_class=argparse.RawDescriptionHelpFormatter
+        formatter_class=argparse.RawDescriptionHelpFormatter,
     )
 
     # Config loading
     config_group = parser.add_argument_group("Configuration Loading")
     config_method = config_group.add_mutually_exclusive_group()
     config_method.add_argument(
-        "-c", "--config-file",
-        help="Path to JSON or YAML config file"
+        "-c", "--config-file", help="Path to JSON or YAML config file"
     )
     config_method.add_argument(
-        "--config-json",
-        help="JSON string containing full configuration"
+        "--config-json", help="JSON string containing full configuration"
     )
 
     # Experiment configuration
     exp_group = parser.add_argument_group("Experiment Configuration")
     exp_group.add_argument(
-        "-e", "--experiment-dir",
-        help="Path to experiment directory (e.g., data/experiments/zero-shot)"
+        "-e",
+        "--experiment-dir",
+        help="Path to experiment directory (e.g., data/experiments/zero-shot)",
     )
     exp_group.add_argument(
-        "-o", "--output-dir",
-        help="Custom output directory for models (default: <experiment-dir>/feature-models)"
+        "-o",
+        "--output-dir",
+        help="Custom output directory for models (default: <experiment-dir>/feature-models)",
     )
 
     # TF-IDF configuration
     tfidf_group = parser.add_argument_group("TF-IDF Configuration")
     tfidf_group.add_argument(
         "--max-features",
         type=int,
-        help="Maximum number of TF-IDF features (default: 5000)"
-    )
-    tfidf_group.add_argument(
-        "--ngram-min",
-        type=int,
-        default=1,
-        help="Minimum n-gram size (default: 1)"
-    )
-    tfidf_group.add_argument(
-        "--ngram-max",
-        type=int,
-        default=2,
-        help="Maximum n-gram size (default: 2)"
-    )
-    tfidf_group.add_argument(
-        "--min-df",
-        type=int,
-        help="Minimum document frequency (default: 2)"
-    )
-    tfidf_group.add_argument(
-        "--max-df",
-        type=float,
-        help="Maximum document frequency (default: 0.95)"
-    )
-    tfidf_group.add_argument(
-        "--no-sublinear-tf",
-        action="store_true",
-        help="Disable sublinear TF scaling"
+        help="Maximum number of TF-IDF features (default: 5000)",
+    )
+    tfidf_group.add_argument(
+        "--ngram-min", type=int, default=1, help="Minimum n-gram size (default: 1)"
+    )
+    tfidf_group.add_argument(
+        "--ngram-max", type=int, default=2, help="Maximum n-gram size (default: 2)"
+    )
+    tfidf_group.add_argument(
+        "--min-df", type=int, help="Minimum document frequency (default: 2)"
+    )
+    tfidf_group.add_argument(
+        "--max-df", type=float, help="Maximum document frequency (default: 0.95)"
+    )
+    tfidf_group.add_argument(
+        "--no-sublinear-tf", action="store_true", help="Disable sublinear TF scaling"
     )
 
     # Data configuration
     data_group = parser.add_argument_group("Data Configuration")
     data_group.add_argument(
         "--text-column",
         default="text",
-        help="Column name containing text (default: text)"
+        help="Column name containing text (default: text)",
     )
     data_group.add_argument(
-        "--min-length",
-        type=int,
-        help="Minimum text length filter (default: 0)"
+        "--min-length", type=int, help="Minimum text length filter (default: 0)"
     )
 
     # Output configuration
     output_group = parser.add_argument_group("Output Configuration")
     output_group.add_argument(
-        "--no-save-config",
-        action="store_true",
-        help="Skip saving configuration files"
+        "--no-save-config", action="store_true", help="Skip saving configuration files"
     )
     output_group.add_argument(
-        "-q", "--quiet",
-        action="store_true",
-        help="Suppress verbose output"
+        "-q", "--quiet", action="store_true", help="Suppress verbose output"
     )
 
     return parser
 
 
 def args_to_config(args: argparse.Namespace) -> GlobalConfig:
     """Convert argparse namespace to GlobalConfig."""
     # Check if config file or json string provided
     if args.config_file:
         config_path = Path(args.config_file)
-        if config_path.suffix in ['.yaml', '.yml']:
+        if config_path.suffix in [".yaml", ".yml"]:
             config = GlobalConfig.from_yaml_file(str(config_path))
-        elif config_path.suffix == '.json':
+        elif config_path.suffix == ".json":
             config = GlobalConfig.from_json_file(str(config_path))
         else:
             raise ValueError(f"Unsupported config file format: {config_path.suffix}")
 
         # CLI args override config file
         if args.experiment_dir:
             config.experiment_config = ExperimentConfig(
                 experiment_dir=args.experiment_dir,
-                models_dir=args.output_dir if args.output_dir else None
+                models_dir=args.output_dir if args.output_dir else None,
             )
         elif args.output_dir:
             config.experiment_config.models_dir = args.output_dir
 
         if args.max_features:
             config.tfidf_config.max_features = args.max_features
         if args.ngram_min or args.ngram_max:
             config.tfidf_config.ngram_range = (
-                args.ngram_min if args.ngram_min else config.tfidf_config.ngram_range[0],
-                args.ngram_max if args.ngram_max else config.tfidf_config.ngram_range[1]
+                (
+                    args.ngram_min
+                    if args.ngram_min
+                    else config.tfidf_config.ngram_range[0]
+                ),
+                (
+                    args.ngram_max
+                    if args.ngram_max
+                    else config.tfidf_config.ngram_range[1]
+                ),
             )
         if args.min_df:
             config.tfidf_config.min_df = args.min_df
         if args.max_df:
             config.tfidf_config.max_df = args.max_df
@@ -262,32 +260,35 @@
 
     else:
         # Build config from CLI args
         experiment_config = ExperimentConfig(
             experiment_dir=args.experiment_dir or "data/experiments/zero-shot",
-            models_dir=args.output_dir if args.output_dir else None
+            models_dir=args.output_dir if args.output_dir else None,
         )
 
         tfidf_config = TfidfConfig(
             max_features=args.max_features or 5000,
             ngram_range=(args.ngram_min, args.ngram_max),
             min_df=args.min_df or 2,
             max_df=args.max_df or 0.95,
-            sublinear_tf=not args.no_sublinear_tf
+            sublinear_tf=not args.no_sublinear_tf,
         )
 
         data_config = DataConfig(
-            text_column=args.text_column,
-            min_text_length=args.min_length or 0
+            text_column=args.text_column, min_text_length=args.min_length or 0
         )
 
         output_config = OutputConfig(
-            save_config=not args.no_save_config,
-            verbose=not args.quiet
-        )
-
-        return GlobalConfig(experiment_config, tfidf_config, data_config=data_config, output_config=output_config)
+            save_config=not args.no_save_config, verbose=not args.quiet
+        )
+
+        return GlobalConfig(
+            experiment_config,
+            tfidf_config,
+            data_config=data_config,
+            output_config=output_config,
+        )
 
 
 def main():
     """Main entry point for TF-IDF training."""
     parser = create_parser()
@@ -300,11 +301,13 @@
         parser.error(f"Configuration error: {e}")
 
     if config.output_config.verbose:
         print("Configuration:")
         print(json.dumps(config.to_dict(), indent=2))
-        output_dir = Path(config.experiment_config.get_tfidf_model_dir(config.tfidf_config))
+        output_dir = Path(
+            config.experiment_config.get_tfidf_model_dir(config.tfidf_config)
+        )
         print(f"\nOutput directory: {output_dir}")
         print(f"Config hash: {config.tfidf_config.get_hash()}")
         print(f"Readable name: {config.tfidf_config.get_readable_name()}")
         print()
 
would reformat /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf.py
--- /home/b/p/cefr-classification/minimal-cefr/src/pipeline.py	2025-10-18 01:27:09.624897+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/pipeline.py	2025-10-18 11:24:39.640196+00:00
@@ -12,11 +12,18 @@
 import json
 import sys
 from pathlib import Path
 from typing import Optional, List
 
-from src.config import GlobalConfig, ExperimentConfig, TfidfConfig, ClassifierConfig, DataConfig, OutputConfig
+from src.config import (
+    GlobalConfig,
+    ExperimentConfig,
+    TfidfConfig,
+    ClassifierConfig,
+    DataConfig,
+    OutputConfig,
+)
 from src.train_tfidf import train_tfidf
 from src.extract_features import extract_all_from_source
 from src.train_classifiers import train_all_classifiers
 from src.predict import predict_all_feature_sets
 
@@ -24,11 +31,11 @@
 def run_pipeline(
     config: GlobalConfig,
     steps: Optional[List[int]] = None,
     classifiers: Optional[List[str]] = None,
     tfidf_configs: Optional[List[dict]] = None,
-    summarize: bool = False
+    summarize: bool = False,
 ) -> bool:
     """
     Run the complete CEFR classification pipeline or specific steps.
 
     Args:
@@ -66,29 +73,39 @@
         # Step 1: Train TF-IDF Vectorizer(s) and Extract Training Features
         if 1 in steps_to_run:
             for i, tfidf_conf_dict in enumerate(tfidf_configs, 1):
                 if verbose:
                     print("\n" + "=" * 70)
-                    print(f"STEP 1.{i}: Train TF-IDF Vectorizer (config {i}/{len(tfidf_configs)})")
-                    print(f"  max_features: {tfidf_conf_dict.get('max_features', config.tfidf_config.max_features)}")
+                    print(
+                        f"STEP 1.{i}: Train TF-IDF Vectorizer (config {i}/{len(tfidf_configs)})"
+                    )
+                    print(
+                        f"  max_features: {tfidf_conf_dict.get('max_features', config.tfidf_config.max_features)}"
+                    )
                     print("=" * 70)
 
                 # Create config with this TF-IDF configuration
                 tfidf_config = TfidfConfig(
-                    max_features=tfidf_conf_dict.get('max_features', config.tfidf_config.max_features),
-                    ngram_range=tfidf_conf_dict.get('ngram_range', config.tfidf_config.ngram_range),
-                    min_df=tfidf_conf_dict.get('min_df', config.tfidf_config.min_df),
-                    max_df=tfidf_conf_dict.get('max_df', config.tfidf_config.max_df),
-                    sublinear_tf=tfidf_conf_dict.get('sublinear_tf', config.tfidf_config.sublinear_tf)
+                    max_features=tfidf_conf_dict.get(
+                        "max_features", config.tfidf_config.max_features
+                    ),
+                    ngram_range=tfidf_conf_dict.get(
+                        "ngram_range", config.tfidf_config.ngram_range
+                    ),
+                    min_df=tfidf_conf_dict.get("min_df", config.tfidf_config.min_df),
+                    max_df=tfidf_conf_dict.get("max_df", config.tfidf_config.max_df),
+                    sublinear_tf=tfidf_conf_dict.get(
+                        "sublinear_tf", config.tfidf_config.sublinear_tf
+                    ),
                 )
 
                 step_config = GlobalConfig(
                     config.experiment_config,
                     tfidf_config,
                     config.classifier_config,
                     config.data_config,
-                    config.output_config
+                    config.output_config,
                 )
 
                 train_tfidf(step_config)
 
                 if verbose:
@@ -97,29 +114,39 @@
         # Step 2: Extract Test Features for all TF-IDF configurations
         if 2 in steps_to_run:
             for i, tfidf_conf_dict in enumerate(tfidf_configs, 1):
                 if verbose:
                     print("\n" + "=" * 70)
-                    print(f"STEP 2.{i}: Extract All Features (config {i}/{len(tfidf_configs)})")
-                    print(f"  max_features: {tfidf_conf_dict.get('max_features', config.tfidf_config.max_features)}")
+                    print(
+                        f"STEP 2.{i}: Extract All Features (config {i}/{len(tfidf_configs)})"
+                    )
+                    print(
+                        f"  max_features: {tfidf_conf_dict.get('max_features', config.tfidf_config.max_features)}"
+                    )
                     print("=" * 70)
 
                 # Create config with this TF-IDF configuration
                 tfidf_config = TfidfConfig(
-                    max_features=tfidf_conf_dict.get('max_features', config.tfidf_config.max_features),
-                    ngram_range=tfidf_conf_dict.get('ngram_range', config.tfidf_config.ngram_range),
-                    min_df=tfidf_conf_dict.get('min_df', config.tfidf_config.min_df),
-                    max_df=tfidf_conf_dict.get('max_df', config.tfidf_config.max_df),
-                    sublinear_tf=tfidf_conf_dict.get('sublinear_tf', config.tfidf_config.sublinear_tf)
+                    max_features=tfidf_conf_dict.get(
+                        "max_features", config.tfidf_config.max_features
+                    ),
+                    ngram_range=tfidf_conf_dict.get(
+                        "ngram_range", config.tfidf_config.ngram_range
+                    ),
+                    min_df=tfidf_conf_dict.get("min_df", config.tfidf_config.min_df),
+                    max_df=tfidf_conf_dict.get("max_df", config.tfidf_config.max_df),
+                    sublinear_tf=tfidf_conf_dict.get(
+                        "sublinear_tf", config.tfidf_config.sublinear_tf
+                    ),
                 )
 
                 step_config = GlobalConfig(
                     config.experiment_config,
                     tfidf_config,
                     config.classifier_config,
                     config.data_config,
-                    config.output_config
+                    config.output_config,
                 )
 
                 # Extract features from both training and test data
                 extract_all_from_source(step_config, data_source="both")
 
@@ -127,11 +154,13 @@
                     print(f"\n✓ Step 2.{i} completed successfully")
 
         # Step 3: Train ML Classifiers (all combinations)
         if 3 in steps_to_run:
             for tfidf_idx, tfidf_conf_dict in enumerate(tfidf_configs, 1):
-                tfidf_max_feat = tfidf_conf_dict.get('max_features', config.tfidf_config.max_features)
+                tfidf_max_feat = tfidf_conf_dict.get(
+                    "max_features", config.tfidf_config.max_features
+                )
 
                 for clf_idx, classifier_type in enumerate(classifiers, 1):
                     if verbose:
                         print("\n" + "=" * 70)
                         print(f"STEP 3.{tfidf_idx}.{clf_idx}: Train Classifiers")
@@ -140,14 +169,22 @@
                         print("=" * 70)
 
                     # Create config for this combination
                     tfidf_config = TfidfConfig(
                         max_features=tfidf_max_feat,
-                        ngram_range=tfidf_conf_dict.get('ngram_range', config.tfidf_config.ngram_range),
-                        min_df=tfidf_conf_dict.get('min_df', config.tfidf_config.min_df),
-                        max_df=tfidf_conf_dict.get('max_df', config.tfidf_config.max_df),
-                        sublinear_tf=tfidf_conf_dict.get('sublinear_tf', config.tfidf_config.sublinear_tf)
+                        ngram_range=tfidf_conf_dict.get(
+                            "ngram_range", config.tfidf_config.ngram_range
+                        ),
+                        min_df=tfidf_conf_dict.get(
+                            "min_df", config.tfidf_config.min_df
+                        ),
+                        max_df=tfidf_conf_dict.get(
+                            "max_df", config.tfidf_config.max_df
+                        ),
+                        sublinear_tf=tfidf_conf_dict.get(
+                            "sublinear_tf", config.tfidf_config.sublinear_tf
+                        ),
                     )
 
                     classifier_config = ClassifierConfig(
                         classifier_type=classifier_type,
                         logistic_max_iter=config.classifier_config.logistic_max_iter,
@@ -159,40 +196,44 @@
                         xgb_n_estimators=config.classifier_config.xgb_n_estimators,
                         xgb_max_depth=config.classifier_config.xgb_max_depth,
                         xgb_learning_rate=config.classifier_config.xgb_learning_rate,
                         xgb_use_gpu=config.classifier_config.xgb_use_gpu,
                         xgb_tree_method=config.classifier_config.xgb_tree_method,
-                        random_state=config.classifier_config.random_state
+                        random_state=config.classifier_config.random_state,
                     )
 
                     step_config = GlobalConfig(
                         config.experiment_config,
                         tfidf_config,
                         classifier_config,
                         config.data_config,
-                        config.output_config
+                        config.output_config,
                     )
 
                     # Train on all feature directories in batch
                     train_all_classifiers(
                         step_config,
                         features_dir=None,  # Use default from config
-                        labels_csv_dir=None  # Use default from config
+                        labels_csv_dir=None,  # Use default from config
                     )
 
                     if verbose:
-                        print(f"\n✓ Step 3.{tfidf_idx}.{clf_idx} completed successfully")
+                        print(
+                            f"\n✓ Step 3.{tfidf_idx}.{clf_idx} completed successfully"
+                        )
 
         # Step 4: Make Predictions (all combinations)
         if 4 in steps_to_run:
             # Get all trained classifiers
             classifiers_dir = Path(config.experiment_config.models_dir) / "classifiers"
             if not classifiers_dir.exists():
                 print(f"✗ No classifiers found in {classifiers_dir}")
                 return False
 
-            trained_models = sorted([d.name for d in classifiers_dir.iterdir() if d.is_dir()])
+            trained_models = sorted(
+                [d.name for d in classifiers_dir.iterdir() if d.is_dir()]
+            )
 
             if not trained_models:
                 print(f"✗ No trained models found in {classifiers_dir}")
                 return False
 
@@ -203,49 +244,61 @@
                 print("=" * 70)
 
             # Predict with each model on all test sets
             for model_idx, model_name in enumerate(trained_models, 1):
                 if verbose:
-                    print(f"\n--- Model {model_idx}/{len(trained_models)}: {model_name} ---")
+                    print(
+                        f"\n--- Model {model_idx}/{len(trained_models)}: {model_name} ---"
+                    )
 
                 # Load model config to get TF-IDF hash and determine features directory
                 model_dir = classifiers_dir / model_name
                 model_config_path = model_dir / "config.json"
 
                 if model_config_path.exists():
-                    with open(model_config_path, 'r') as f:
+                    with open(model_config_path, "r") as f:
                         model_config_data = json.load(f)
-                    tfidf_hash = model_config_data.get('tfidf_hash')
+                    tfidf_hash = model_config_data.get("tfidf_hash")
 
                     if tfidf_hash:
                         # Use hashed features directory
-                        features_dir = Path(config.experiment_config.features_output_dir) / tfidf_hash
+                        features_dir = (
+                            Path(config.experiment_config.features_output_dir)
+                            / tfidf_hash
+                        )
                         if verbose:
                             print(f"  Using features from TF-IDF config: {tfidf_hash}")
                             print(f"  Features directory: {features_dir}")
                     else:
                         # Backward compatibility: old models without hash
-                        features_dir = Path(config.experiment_config.features_output_dir)
+                        features_dir = Path(
+                            config.experiment_config.features_output_dir
+                        )
                         if verbose:
-                            print(f"  Warning: Model config missing tfidf_hash, using default features directory")
+                            print(
+                                f"  Warning: Model config missing tfidf_hash, using default features directory"
+                            )
                 else:
                     # Fallback: use default features directory
                     features_dir = Path(config.experiment_config.features_output_dir)
                     if verbose:
-                        print(f"  Warning: Model config not found, using default features directory")
+                        print(
+                            f"  Warning: Model config not found, using default features directory"
+                        )
 
                 try:
                     predict_all_feature_sets(
                         config,
                         classifier_model_name=model_name,
                         features_dir=str(features_dir),
-                        labels_csv_dir=str(Path(config.experiment_config.ml_test_dir))
+                        labels_csv_dir=str(Path(config.experiment_config.ml_test_dir)),
                     )
                 except Exception as e:
                     print(f"✗ Error predicting with {model_name}: {e}")
                     if verbose:
                         import traceback
+
                         traceback.print_exc()
 
             if verbose:
                 print("\n✓ Step 4 completed successfully")
 
@@ -256,19 +309,23 @@
             print("=" * 70)
 
             # Summary of trained models
             classifiers_dir = Path(config.experiment_config.models_dir) / "classifiers"
             if classifiers_dir.exists():
-                trained_models = sorted([d.name for d in classifiers_dir.iterdir() if d.is_dir()])
+                trained_models = sorted(
+                    [d.name for d in classifiers_dir.iterdir() if d.is_dir()]
+                )
                 print(f"\nTrained models ({len(trained_models)}):")
                 for model in trained_models:
                     print(f"  - {model}")
 
             # Summary of results
             results_dir = Path(config.experiment_config.results_dir)
             if results_dir.exists():
-                result_dirs = sorted([d.name for d in results_dir.iterdir() if d.is_dir()])
+                result_dirs = sorted(
+                    [d.name for d in results_dir.iterdir() if d.is_dir()]
+                )
                 print(f"\nResults saved ({len(result_dirs)} model directories):")
                 for dataset in result_dirs:
                     print(f"  - {dataset}/")
                 print(f"\nResults directory: {results_dir}")
 
@@ -297,18 +354,20 @@
 
             except Exception as e:
                 print(f"\n✗ Error generating summary report: {e}")
                 if verbose:
                     import traceback
+
                     traceback.print_exc()
 
         return True
 
     except Exception as e:
         print(f"\n✗ Pipeline execution failed: {e}")
         if verbose:
             import traceback
+
             traceback.print_exc()
         return False
 
 
 def create_parser() -> argparse.ArgumentParser:
@@ -1306,180 +1365,164 @@
   • Results organized by dataset for easy comparison
 
 ═══════════════════════════════════════════════════════════════════════════════
 
 For more information, see documentation in the project repository.
-        """
+        """,
     )
 
     # Config loading
     config_group = parser.add_argument_group("Configuration Loading")
     config_method = config_group.add_mutually_exclusive_group()
     config_method.add_argument(
-        "-c", "--config-file",
-        help="Path to JSON or YAML config file"
+        "-c", "--config-file", help="Path to JSON or YAML config file"
     )
     config_method.add_argument(
-        "--config-json",
-        help="JSON string containing full configuration"
+        "--config-json", help="JSON string containing full configuration"
     )
 
     # Experiment configuration
     exp_group = parser.add_argument_group("Experiment Configuration")
     exp_group.add_argument(
-        "-e", "--experiment-dir",
+        "-e",
+        "--experiment-dir",
         required=True,
-        help="Path to experiment directory (e.g., data/experiments/zero-shot)"
+        help="Path to experiment directory (e.g., data/experiments/zero-shot)",
     )
     exp_group.add_argument(
-        "-o", "--output-dir",
-        help="Custom output directory for models (default: <experiment-dir>/feature-models)"
+        "-o",
+        "--output-dir",
+        help="Custom output directory for models (default: <experiment-dir>/feature-models)",
     )
 
     # Pipeline control
     pipeline_group = parser.add_argument_group("Pipeline Control")
     pipeline_group.add_argument(
         "--steps",
         nargs="+",
         type=int,
         choices=[1, 2, 3, 4],
-        help="Specific steps to run (1=TF-IDF, 2=Features, 3=Classifiers, 4=Predict). Default: all steps"
+        help="Specific steps to run (1=TF-IDF, 2=Features, 3=Classifiers, 4=Predict). Default: all steps",
     )
 
     # TF-IDF configuration
     tfidf_group = parser.add_argument_group("TF-IDF Configuration")
     tfidf_group.add_argument(
         "--max-features",
         type=int,
-        help="Maximum number of TF-IDF features (default: 5000)"
+        help="Maximum number of TF-IDF features (default: 5000)",
     )
     tfidf_group.add_argument(
         "--max-features-list",
         nargs="+",
         type=int,
-        help="Train multiple TF-IDF models with different max_features (e.g., 1000 5000 10000)"
+        help="Train multiple TF-IDF models with different max_features (e.g., 1000 5000 10000)",
     )
     tfidf_group.add_argument(
-        "--ngram-min",
-        type=int,
-        help="Minimum n-gram size (default: 1)"
+        "--ngram-min", type=int, help="Minimum n-gram size (default: 1)"
     )
     tfidf_group.add_argument(
-        "--ngram-max",
-        type=int,
-        help="Maximum n-gram size (default: 2)"
+        "--ngram-max", type=int, help="Maximum n-gram size (default: 2)"
     )
     tfidf_group.add_argument(
-        "--min-df",
-        type=int,
-        help="Minimum document frequency (default: 2)"
+        "--min-df", type=int, help="Minimum document frequency (default: 2)"
     )
     tfidf_group.add_argument(
-        "--max-df",
-        type=float,
-        help="Maximum document frequency (default: 0.95)"
+        "--max-df", type=float, help="Maximum document frequency (default: 0.95)"
     )
 
     # Classifier configuration
     clf_group = parser.add_argument_group("Classifier Configuration")
     clf_group.add_argument(
         "--classifier",
         choices=["multinomialnb", "logistic", "randomforest", "svm", "xgboost"],
-        help="Single classifier type (default: xgboost)"
+        help="Single classifier type (default: xgboost)",
     )
     clf_group.add_argument(
         "--classifiers",
         nargs="+",
         choices=["multinomialnb", "logistic", "randomforest", "svm", "xgboost"],
-        help="Train multiple classifiers (e.g., xgboost logistic randomforest)"
-    )
+        help="Train multiple classifiers (e.g., xgboost logistic randomforest)",
+    )
+    clf_group.add_argument("--random-state", type=int, help="Random seed (default: 42)")
     clf_group.add_argument(
-        "--random-state",
-        type=int,
-        help="Random seed (default: 42)"
-    )
-    clf_group.add_argument(
-        "--xgb-use-gpu",
-        action="store_true",
-        help="Use GPU for XGBoost training"
+        "--xgb-use-gpu", action="store_true", help="Use GPU for XGBoost training"
     )
     clf_group.add_argument(
         "--xgb-n-estimators",
         type=int,
-        help="Number of boosting rounds for XGBoost (default: 100)"
+        help="Number of boosting rounds for XGBoost (default: 100)",
     )
     clf_group.add_argument(
-        "--xgb-max-depth",
-        type=int,
-        help="Max tree depth for XGBoost (default: 6)"
+        "--xgb-max-depth", type=int, help="Max tree depth for XGBoost (default: 6)"
     )
 
     # Data configuration
     data_group = parser.add_argument_group("Data Configuration")
     data_group.add_argument(
         "--text-column",
         default="text",
-        help="Column name containing text (default: text)"
+        help="Column name containing text (default: text)",
     )
     data_group.add_argument(
         "--label-column",
         default="label",
-        help="Column name containing labels (default: label)"
+        help="Column name containing labels (default: label)",
     )
     data_group.add_argument(
         "--cefr-column",
         default="cefr_label",
-        help="Column name containing CEFR labels (default: cefr_label)"
+        help="Column name containing CEFR labels (default: cefr_label)",
     )
 
     # Output configuration
     output_group = parser.add_argument_group("Output Configuration")
     output_group.add_argument(
-        "--no-save-config",
-        action="store_true",
-        help="Skip saving configuration files"
+        "--no-save-config", action="store_true", help="Skip saving configuration files"
     )
     output_group.add_argument(
-        "-q", "--quiet",
-        action="store_true",
-        help="Suppress verbose output"
+        "-q", "--quiet", action="store_true", help="Suppress verbose output"
     )
     output_group.add_argument(
         "--summarize",
         action="store_true",
-        help="Generate summary report after pipeline completion (saves to results_summary.md)"
+        help="Generate summary report after pipeline completion (saves to results_summary.md)",
     )
 
     return parser
 
 
 def args_to_config(args: argparse.Namespace) -> GlobalConfig:
     """Convert argparse namespace to GlobalConfig."""
     # Check if config file or json string provided
     if args.config_file:
         config_path = Path(args.config_file)
-        if config_path.suffix in ['.yaml', '.yml']:
+        if config_path.suffix in [".yaml", ".yml"]:
             config = GlobalConfig.from_yaml_file(str(config_path))
-        elif config_path.suffix == '.json':
+        elif config_path.suffix == ".json":
             config = GlobalConfig.from_json_file(str(config_path))
         else:
             raise ValueError(f"Unsupported config file format: {config_path.suffix}")
 
         # CLI args override config file
         if args.experiment_dir:
             config.experiment_config = ExperimentConfig(
                 experiment_dir=args.experiment_dir,
-                models_dir=args.output_dir if args.output_dir else None
+                models_dir=args.output_dir if args.output_dir else None,
             )
         elif args.output_dir:
             config.experiment_config.models_dir = args.output_dir
 
         if args.max_features:
             config.tfidf_config.max_features = args.max_features
         if args.ngram_min or args.ngram_max:
-            ngram_min = args.ngram_min if args.ngram_min else config.tfidf_config.ngram_range[0]
-            ngram_max = args.ngram_max if args.ngram_max else config.tfidf_config.ngram_range[1]
+            ngram_min = (
+                args.ngram_min if args.ngram_min else config.tfidf_config.ngram_range[0]
+            )
+            ngram_max = (
+                args.ngram_max if args.ngram_max else config.tfidf_config.ngram_range[1]
+            )
             config.tfidf_config.ngram_range = (ngram_min, ngram_max)
         if args.min_df:
             config.tfidf_config.min_df = args.min_df
         if args.max_df:
             config.tfidf_config.max_df = args.max_df
@@ -1518,45 +1561,44 @@
 
     else:
         # Build config from CLI args
         experiment_config = ExperimentConfig(
             experiment_dir=args.experiment_dir,
-            models_dir=args.output_dir if args.output_dir else None
+            models_dir=args.output_dir if args.output_dir else None,
         )
 
         tfidf_config = TfidfConfig(
             max_features=args.max_features or 5000,
             ngram_range=(args.ngram_min or 1, args.ngram_max or 2),
             min_df=args.min_df or 2,
-            max_df=args.max_df or 0.95
+            max_df=args.max_df or 0.95,
         )
 
         classifier_config = ClassifierConfig(
             classifier_type=args.classifier or "xgboost",
             random_state=args.random_state or 42,
             xgb_use_gpu=args.xgb_use_gpu if args.xgb_use_gpu else False,
             xgb_n_estimators=args.xgb_n_estimators or 100,
-            xgb_max_depth=args.xgb_max_depth or 6
+            xgb_max_depth=args.xgb_max_depth or 6,
         )
 
         data_config = DataConfig(
             text_column=args.text_column,
             label_column=args.label_column,
-            cefr_column=args.cefr_column
+            cefr_column=args.cefr_column,
         )
 
         output_config = OutputConfig(
-            save_config=not args.no_save_config,
-            verbose=not args.quiet
+            save_config=not args.no_save_config, verbose=not args.quiet
         )
 
         return GlobalConfig(
             experiment_config,
             tfidf_config,
             classifier_config,
             data_config,
-            output_config
+            output_config,
         )
 
 
 def main():
     """Main entry point for the CEFR classification pipeline."""
@@ -1590,15 +1632,11 @@
             print(f"TF-IDF configurations: {tfidf_configs}")
         print()
 
     # Run pipeline
     success = run_pipeline(
-        config,
-        args.steps,
-        classifiers,
-        tfidf_configs,
-        summarize=args.summarize
+        config, args.steps, classifiers, tfidf_configs, summarize=args.summarize
     )
     sys.exit(0 if success else 1)
 
 
 if __name__ == "__main__":
would reformat /home/b/p/cefr-classification/minimal-cefr/src/pipeline.py
--- /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho.py	2025-10-18 09:00:08.917583+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho.py	2025-10-18 11:24:39.765173+00:00
@@ -17,25 +17,33 @@
 from sklearn.preprocessing import LabelEncoder
 from sklearn.model_selection import train_test_split
 
 try:
     import xgboost as xgb
+
     XGBOOST_AVAILABLE = True
 except ImportError:
     XGBOOST_AVAILABLE = False
 
 try:
     import optuna
+
     OPTUNA_AVAILABLE = True
 except ImportError:
     OPTUNA_AVAILABLE = False
 
-from src.config import GlobalConfig, ExperimentConfig, ClassifierConfig, DataConfig, OutputConfig
+from src.config import (
+    GlobalConfig,
+    ExperimentConfig,
+    ClassifierConfig,
+    DataConfig,
+    OutputConfig,
+)
 
 
 # Fixed CEFR classes (all 6 levels)
-CEFR_CLASSES = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
+CEFR_CLASSES = ["A1", "A2", "B1", "B2", "C1", "C2"]
 
 
 def get_cefr_label_encoder() -> LabelEncoder:
     """
     Create a fixed label encoder for all 6 CEFR classes.
@@ -52,11 +60,11 @@
     features_file: str,
     feature_names_file: Optional[str] = None,
     labels_file: Optional[str] = None,
     labels_csv: Optional[str] = None,
     cefr_column: str = "cefr_label",
-    verbose: bool = True
+    verbose: bool = True,
 ) -> Tuple[np.ndarray, np.ndarray, List[str], pd.Series]:
     """
     Load pre-extracted features and labels.
 
     Args:
@@ -82,12 +90,12 @@
     if feature_names_file:
         if verbose:
             print(f"Loading feature names from: {feature_names_file}")
 
         feature_names_path = Path(feature_names_file)
-        if feature_names_path.suffix == '.txt':
-            with open(feature_names_file, 'r') as f:
+        if feature_names_path.suffix == ".txt":
+            with open(feature_names_file, "r") as f:
                 feature_names = [line.strip() for line in f if line.strip()]
         else:  # CSV
             fn_df = pd.read_csv(feature_names_file)
             feature_names = fn_df.iloc[:, 0].tolist()
     else:
@@ -97,11 +105,11 @@
     # Load labels
     if labels_file:
         if verbose:
             print(f"Loading labels from: {labels_file}")
 
-        with open(labels_file, 'r') as f:
+        with open(labels_file, "r") as f:
             y_train = np.array([line.strip() for line in f if line.strip()])
         y_train_series = pd.Series(y_train)
 
     elif labels_csv:
         if verbose:
@@ -137,11 +145,11 @@
     X_val: np.ndarray,
     y_val: np.ndarray,
     n_trials: int = 50,
     use_gpu: bool = False,
     random_state: int = 42,
-    verbose: bool = True
+    verbose: bool = True,
 ) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
     """
     Optimize XGBoost hyperparameters using Optuna.
 
     Args:
@@ -156,83 +164,88 @@
 
     Returns:
         Tuple of (best_params, tuning_history)
     """
     if not OPTUNA_AVAILABLE:
-        raise ImportError("Optuna is not installed. Install it with: pip install optuna")
+        raise ImportError(
+            "Optuna is not installed. Install it with: pip install optuna"
+        )
 
     if not XGBOOST_AVAILABLE:
-        raise ImportError("XGBoost is not installed. Install it with: pip install xgboost")
+        raise ImportError(
+            "XGBoost is not installed. Install it with: pip install xgboost"
+        )
 
     # Create DMatrix for XGBoost
     dtrain = xgb.DMatrix(X_train, label=y_train)
     dval = xgb.DMatrix(X_val, label=y_val)
 
     tuning_history = []
 
     def objective(trial):
         """Optuna objective function for XGBoost."""
         params = {
-            'objective': 'multi:softmax',
-            'num_class': len(CEFR_CLASSES),
-            'eval_metric': 'mlogloss',
-            'tree_method': 'gpu_hist' if use_gpu else 'hist',
-            'device': 'cuda' if use_gpu else 'cpu',
-            'random_state': random_state,
-
+            "objective": "multi:softmax",
+            "num_class": len(CEFR_CLASSES),
+            "eval_metric": "mlogloss",
+            "tree_method": "gpu_hist" if use_gpu else "hist",
+            "device": "cuda" if use_gpu else "cpu",
+            "random_state": random_state,
             # Hyperparameters to tune
-            'max_depth': trial.suggest_int('max_depth', 3, 10),
-            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
-            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
-            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
-            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
-            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
-            'gamma': trial.suggest_float('gamma', 0.0, 5.0),
-            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),
-            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),
+            "max_depth": trial.suggest_int("max_depth", 3, 10),
+            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
+            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
+            "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
+            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
+            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
+            "gamma": trial.suggest_float("gamma", 0.0, 5.0),
+            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 5.0),
+            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 5.0),
         }
 
         # Train model
         booster = xgb.train(
             params,
             dtrain,
-            num_boost_round=params['n_estimators'],
-            evals=[(dval, 'validation')],
-            verbose_eval=False
+            num_boost_round=params["n_estimators"],
+            evals=[(dval, "validation")],
+            verbose_eval=False,
         )
 
         # Predict on validation set
         y_pred = booster.predict(dval)
         accuracy = accuracy_score(y_val, y_pred)
 
         # Record trial
         trial_info = {
-            'trial_number': trial.number,
-            'params': params.copy(),
-            'accuracy': accuracy
+            "trial_number": trial.number,
+            "params": params.copy(),
+            "accuracy": accuracy,
         }
         tuning_history.append(trial_info)
 
         return accuracy  # Optuna maximizes by default
 
     # Create study and optimize
-    study = optuna.create_study(direction='maximize', study_name='xgboost_cefr')
+    study = optuna.create_study(direction="maximize", study_name="xgboost_cefr")
 
     if verbose:
         print(f"\nStarting Optuna hyperparameter optimization ({n_trials} trials)...")
 
     study.optimize(objective, n_trials=n_trials, show_progress_bar=verbose)
 
     best_params = study.best_params
-    best_params.update({
-        'objective': 'multi:softmax',
-        'num_class': len(CEFR_CLASSES),
-        'eval_metric': 'mlogloss',
-        'tree_method': 'gpu_hist' if use_gpu else 'hist',
-        'device': 'cuda' if use_gpu else 'cpu',
-        'random_state': random_state
-    })
+    best_params.update(
+        {
+            "objective": "multi:softmax",
+            "num_class": len(CEFR_CLASSES),
+            "eval_metric": "mlogloss",
+            "tree_method": "gpu_hist" if use_gpu else "hist",
+            "device": "cuda" if use_gpu else "cpu",
+            "random_state": random_state,
+        }
+    )
 
     if verbose:
         print(f"\nBest trial accuracy: {study.best_value:.4f}")
         print(f"Best parameters: {best_params}")
 
@@ -244,11 +257,11 @@
     y_train: np.ndarray,
     X_val: np.ndarray,
     y_val: np.ndarray,
     n_trials: int = 30,
     random_state: int = 42,
-    verbose: bool = True
+    verbose: bool = True,
 ) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
     """
     Optimize Logistic Regression hyperparameters using Optuna.
 
     Args:
@@ -262,28 +275,34 @@
 
     Returns:
         Tuple of (best_params, tuning_history)
     """
     if not OPTUNA_AVAILABLE:
-        raise ImportError("Optuna is not installed. Install it with: pip install optuna")
+        raise ImportError(
+            "Optuna is not installed. Install it with: pip install optuna"
+        )
 
     tuning_history = []
 
     def objective(trial):
         """Optuna objective function for Logistic Regression."""
         params = {
-            'C': trial.suggest_float('C', 0.001, 100.0, log=True),
-            'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet']),
-            'solver': trial.suggest_categorical('solver', ['saga']),  # saga supports all penalties
-            'max_iter': trial.suggest_int('max_iter', 500, 3000),
-            'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),
-            'random_state': random_state
+            "C": trial.suggest_float("C", 0.001, 100.0, log=True),
+            "penalty": trial.suggest_categorical("penalty", ["l1", "l2", "elasticnet"]),
+            "solver": trial.suggest_categorical(
+                "solver", ["saga"]
+            ),  # saga supports all penalties
+            "max_iter": trial.suggest_int("max_iter", 500, 3000),
+            "class_weight": trial.suggest_categorical(
+                "class_weight", ["balanced", None]
+            ),
+            "random_state": random_state,
         }
 
         # Add l1_ratio for elasticnet
-        if params['penalty'] == 'elasticnet':
-            params['l1_ratio'] = trial.suggest_float('l1_ratio', 0.0, 1.0)
+        if params["penalty"] == "elasticnet":
+            params["l1_ratio"] = trial.suggest_float("l1_ratio", 0.0, 1.0)
 
         # Train model
         try:
             clf = LogisticRegression(**params)
             clf.fit(X_train, y_train)
@@ -292,23 +311,23 @@
             y_pred = clf.predict(X_val)
             accuracy = accuracy_score(y_val, y_pred)
 
             # Record trial
             trial_info = {
-                'trial_number': trial.number,
-                'params': params.copy(),
-                'accuracy': accuracy
+                "trial_number": trial.number,
+                "params": params.copy(),
+                "accuracy": accuracy,
             }
             tuning_history.append(trial_info)
 
             return accuracy
         except Exception as e:
             # Return low score if training fails
             return 0.0
 
     # Create study and optimize
-    study = optuna.create_study(direction='maximize', study_name='logistic_cefr')
+    study = optuna.create_study(direction="maximize", study_name="logistic_cefr")
 
     if verbose:
         print(f"\nStarting Optuna hyperparameter optimization ({n_trials} trials)...")
 
     study.optimize(objective, n_trials=n_trials, show_progress_bar=verbose)
@@ -328,11 +347,11 @@
     feature_names_file: Optional[str] = None,
     labels_file: Optional[str] = None,
     labels_csv: Optional[str] = None,
     model_name: Optional[str] = None,
     n_trials: int = 50,
-    val_split: float = 0.2
+    val_split: float = 0.2,
 ) -> Path:
     """
     Train a classifier with hyperparameter optimization on pre-extracted features.
 
     Args:
@@ -365,11 +384,11 @@
         features_file=features_file,
         feature_names_file=feature_names_file,
         labels_file=labels_file,
         labels_csv=labels_csv,
         cefr_column=data_config.cefr_column,
-        verbose=verbose
+        verbose=verbose,
     )
 
     # Create fixed CEFR label encoder
     label_encoder = get_cefr_label_encoder()
     y_encoded = label_encoder.transform(y)
@@ -379,48 +398,57 @@
         for label, encoded in zip(CEFR_CLASSES, range(len(CEFR_CLASSES))):
             print(f"  {label} → {encoded}")
 
     # Split into train and validation
     X_train, X_val, y_train, y_val = train_test_split(
-        X, y_encoded, test_size=val_split, random_state=classifier_config.random_state,
-        stratify=y_encoded
+        X,
+        y_encoded,
+        test_size=val_split,
+        random_state=classifier_config.random_state,
+        stratify=y_encoded,
     )
 
     if verbose:
         print(f"\nSplit data:")
         print(f"  Train: {len(X_train)} samples")
         print(f"  Validation: {len(X_val)} samples")
 
     # Optimize hyperparameters based on classifier type
     if classifier_config.classifier_type == "xgboost":
         best_params, tuning_history = optimize_xgboost(
-            X_train, y_train, X_val, y_val,
+            X_train,
+            y_train,
+            X_val,
+            y_val,
             n_trials=n_trials,
             use_gpu=classifier_config.xgb_use_gpu,
             random_state=classifier_config.random_state,
-            verbose=verbose
+            verbose=verbose,
         )
 
         # Train final model on full dataset with best params
         if verbose:
             print(f"\nTraining final XGBoost model on full dataset...")
 
         dtrain = xgb.DMatrix(X, label=y_encoded)
-        num_boost_round = best_params.pop('n_estimators')
+        num_boost_round = best_params.pop("n_estimators")
         booster = xgb.train(best_params, dtrain, num_boost_round=num_boost_round)
 
         # Restore n_estimators to best_params for saving
-        best_params['n_estimators'] = num_boost_round
+        best_params["n_estimators"] = num_boost_round
 
         model = booster  # For XGBoost, we save the booster directly
 
     elif classifier_config.classifier_type == "logistic":
         best_params, tuning_history = optimize_logistic(
-            X_train, y_train, X_val, y_val,
+            X_train,
+            y_train,
+            X_val,
+            y_val,
             n_trials=n_trials,
             random_state=classifier_config.random_state,
-            verbose=verbose
+            verbose=verbose,
         )
 
         # Train final model on full dataset with best params
         if verbose:
             print(f"\nTraining final Logistic Regression model on full dataset...")
@@ -447,11 +475,11 @@
         # Check if features_file is in a feature directory with config
         feature_dir = features_path.parent.parent
         feature_config_path = feature_dir / "config.json"
 
         if feature_config_path.exists():
-            with open(feature_config_path, 'r') as f:
+            with open(feature_config_path, "r") as f:
                 feature_cfg = json.load(f)
                 feature_type = feature_cfg.get("feature_type", "tfidf")
                 config_hash = feature_cfg.get("config_hash")
         else:
             feature_type = "tfidf"
@@ -473,21 +501,21 @@
     if verbose:
         print(f"\nSaving model to: {output_dir}")
 
     # Save model
     model_file = output_dir / "model.pkl"
-    with open(model_file, 'wb') as f:
+    with open(model_file, "wb") as f:
         pickle.dump(model, f)
 
     # Save label encoder
     encoder_file = output_dir / "label_encoder.pkl"
-    with open(encoder_file, 'wb') as f:
+    with open(encoder_file, "wb") as f:
         pickle.dump(label_encoder, f)
 
     # Save feature names
     feature_names_file_out = output_dir / "feature_names.txt"
-    with open(feature_names_file_out, 'w') as f:
+    with open(feature_names_file_out, "w") as f:
         for name in feature_names:
             f.write(f"{name}\n")
 
     # Save configuration
     config_dict = {
@@ -503,16 +531,16 @@
         "random_state": classifier_config.random_state,
     }
 
     # Save config as JSON
     config_file = output_dir / "config.json"
-    with open(config_file, 'w') as f:
+    with open(config_file, "w") as f:
         json.dump(config_dict, f, indent=2)
 
     # Save tuning history
     tuning_file = output_dir / "tuning_history.json"
-    with open(tuning_file, 'w') as f:
+    with open(tuning_file, "w") as f:
         json.dump(tuning_history, f, indent=2)
 
     if verbose:
         print(f"✓ Model saved: {model_file}")
         print(f"✓ Label encoder saved: {encoder_file}")
@@ -529,11 +557,11 @@
 def batch_train_classifiers_with_ho(
     config: GlobalConfig,
     batch_features_dir: str,
     labels_csv_dir: Optional[str] = None,
     n_trials: int = 50,
-    val_split: float = 0.2
+    val_split: float = 0.2,
 ) -> List[Path]:
     """
     Train classifiers with hyperparameter optimization on multiple datasets in a batch directory.
 
     Args:
@@ -557,11 +585,13 @@
     for subdir in batch_path.iterdir():
         if subdir.is_dir() and (subdir / "features_dense.csv").exists():
             dataset_dirs.append(subdir)
 
     if not dataset_dirs:
-        raise ValueError(f"No dataset directories with features_dense.csv found in {batch_features_dir}")
+        raise ValueError(
+            f"No dataset directories with features_dense.csv found in {batch_features_dir}"
+        )
 
     if verbose:
         print(f"Found {len(dataset_dirs)} datasets to process")
 
     trained_models = []
@@ -580,93 +610,156 @@
         # Determine labels CSV path
         if labels_csv_dir:
             labels_csv = Path(labels_csv_dir) / dataset_name / "data.csv"
         else:
             # Try to find in standard experiment structure
-            labels_csv = Path(config.experiment_config.ml_training_dir) / dataset_name / "data.csv"
+            labels_csv = (
+                Path(config.experiment_config.ml_training_dir)
+                / dataset_name
+                / "data.csv"
+            )
             if not labels_csv.exists():
                 # Try test data
-                labels_csv = Path(config.experiment_config.ml_test_dir) / dataset_name / "data.csv"
+                labels_csv = (
+                    Path(config.experiment_config.ml_test_dir)
+                    / dataset_name
+                    / "data.csv"
+                )
 
         if not labels_csv.exists():
             print(f"⚠ Warning: Labels CSV not found for {dataset_name}, skipping")
             continue
 
         # Train classifier with hyperparameter optimization
         try:
             model_dir = train_classifier_with_ho(
                 config=config,
                 features_file=str(features_file),
-                feature_names_file=str(feature_names_file) if feature_names_file.exists() else None,
+                feature_names_file=(
+                    str(feature_names_file) if feature_names_file.exists() else None
+                ),
                 labels_csv=str(labels_csv),
                 n_trials=n_trials,
-                val_split=val_split
+                val_split=val_split,
             )
             trained_models.append(model_dir)
 
         except Exception as e:
             print(f"✗ Error training classifier for {dataset_name}: {e}")
             continue
 
     if verbose:
         print(f"\n{'='*70}")
-        print(f"Batch training complete: {len(trained_models)}/{len(dataset_dirs)} models trained")
+        print(
+            f"Batch training complete: {len(trained_models)}/{len(dataset_dirs)} models trained"
+        )
         print(f"{'='*70}")
 
     return trained_models
 
 
 def main():
     """Main entry point for CLI."""
     parser = argparse.ArgumentParser(
         description="Train ML classifiers with Optuna hyperparameter optimization for CEFR classification",
-        formatter_class=argparse.RawDescriptionHelpFormatter
+        formatter_class=argparse.RawDescriptionHelpFormatter,
     )
 
     # Configuration options
     config_group = parser.add_mutually_exclusive_group()
-    config_group.add_argument('-c', '--config-file', type=str, help='Path to config file (YAML or JSON)')
-    config_group.add_argument('--config-json', type=str, help='JSON string with configuration')
+    config_group.add_argument(
+        "-c", "--config-file", type=str, help="Path to config file (YAML or JSON)"
+    )
+    config_group.add_argument(
+        "--config-json", type=str, help="JSON string with configuration"
+    )
 
     # Experiment configuration
-    parser.add_argument('-e', '--experiment-dir', type=str, help='Experiment directory path')
-    parser.add_argument('-o', '--output-dir', type=str, help='Custom output directory for models')
+    parser.add_argument(
+        "-e", "--experiment-dir", type=str, help="Experiment directory path"
+    )
+    parser.add_argument(
+        "-o", "--output-dir", type=str, help="Custom output directory for models"
+    )
 
     # Input files (single dataset mode)
-    parser.add_argument('-d', '--feature-dir', type=str, help='Feature directory (contains features_dense.csv)')
-    parser.add_argument('-f', '--features-file', type=str, help='Features CSV file')
-    parser.add_argument('--feature-names-file', type=str, help='Feature names file (TXT or CSV)')
-    parser.add_argument('--labels-file', type=str, help='Labels file (one per line)')
-    parser.add_argument('--labels-csv', type=str, help='CSV file containing labels')
+    parser.add_argument(
+        "-d",
+        "--feature-dir",
+        type=str,
+        help="Feature directory (contains features_dense.csv)",
+    )
+    parser.add_argument("-f", "--features-file", type=str, help="Features CSV file")
+    parser.add_argument(
+        "--feature-names-file", type=str, help="Feature names file (TXT or CSV)"
+    )
+    parser.add_argument("--labels-file", type=str, help="Labels file (one per line)")
+    parser.add_argument("--labels-csv", type=str, help="CSV file containing labels")
 
     # Batch mode
-    parser.add_argument('--batch-features-dir', type=str, help='Directory with multiple feature subdirectories')
-    parser.add_argument('--labels-csv-dir', type=str, help='Directory with label CSV files for batch mode')
+    parser.add_argument(
+        "--batch-features-dir",
+        type=str,
+        help="Directory with multiple feature subdirectories",
+    )
+    parser.add_argument(
+        "--labels-csv-dir",
+        type=str,
+        help="Directory with label CSV files for batch mode",
+    )
 
     # Model configuration
-    parser.add_argument('--model-name', type=str, help='Custom model name')
-    parser.add_argument('--classifier', type=str, choices=['xgboost', 'logistic'],
-                        default='xgboost', help='Classifier type (only xgboost and logistic supported for HO)')
+    parser.add_argument("--model-name", type=str, help="Custom model name")
+    parser.add_argument(
+        "--classifier",
+        type=str,
+        choices=["xgboost", "logistic"],
+        default="xgboost",
+        help="Classifier type (only xgboost and logistic supported for HO)",
+    )
 
     # Hyperparameter optimization settings
-    parser.add_argument('--n-trials', type=int, default=50, help='Number of Optuna trials (default: 50)')
-    parser.add_argument('--val-split', type=float, default=0.2, help='Validation split ratio (default: 0.2)')
+    parser.add_argument(
+        "--n-trials", type=int, default=50, help="Number of Optuna trials (default: 50)"
+    )
+    parser.add_argument(
+        "--val-split",
+        type=float,
+        default=0.2,
+        help="Validation split ratio (default: 0.2)",
+    )
 
     # XGBoost specific
-    parser.add_argument('--xgb-use-gpu', action='store_true', help='Use GPU for XGBoost')
+    parser.add_argument(
+        "--xgb-use-gpu", action="store_true", help="Use GPU for XGBoost"
+    )
 
     # Random state
-    parser.add_argument('--random-state', type=int, default=42, help='Random state for reproducibility')
+    parser.add_argument(
+        "--random-state", type=int, default=42, help="Random state for reproducibility"
+    )
 
     # Data configuration
-    parser.add_argument('--text-column', type=str, default='text', help='Text column name')
-    parser.add_argument('--label-column', type=str, default='label', help='Label column name')
-    parser.add_argument('--cefr-column', type=str, default='cefr_label', help='CEFR column name')
+    parser.add_argument(
+        "--text-column", type=str, default="text", help="Text column name"
+    )
+    parser.add_argument(
+        "--label-column", type=str, default="label", help="Label column name"
+    )
+    parser.add_argument(
+        "--cefr-column", type=str, default="cefr_label", help="CEFR column name"
+    )
 
     # Output options
-    parser.add_argument('--no-save-config', action='store_true', help='Do not save config to output directory')
-    parser.add_argument('-q', '--quiet', action='store_true', help='Suppress verbose output')
+    parser.add_argument(
+        "--no-save-config",
+        action="store_true",
+        help="Do not save config to output directory",
+    )
+    parser.add_argument(
+        "-q", "--quiet", action="store_true", help="Suppress verbose output"
+    )
 
     args = parser.parse_args()
 
     # Load or create configuration
     if args.config_file:
@@ -707,11 +800,11 @@
         batch_train_classifiers_with_ho(
             config=config,
             batch_features_dir=args.batch_features_dir,
             labels_csv_dir=args.labels_csv_dir,
             n_trials=args.n_trials,
-            val_split=args.val_split
+            val_split=args.val_split,
         )
         return 0
 
     # Single dataset mode
     if args.feature_dir:
@@ -728,26 +821,36 @@
         if args.labels_csv:
             labels_csv = args.labels_csv
         else:
             # Try to infer from experiment structure
             dataset_name = feature_dir.name
-            labels_csv = Path(config.experiment_config.ml_training_dir) / dataset_name / "data.csv"
+            labels_csv = (
+                Path(config.experiment_config.ml_training_dir)
+                / dataset_name
+                / "data.csv"
+            )
             if not labels_csv.exists():
-                labels_csv = Path(config.experiment_config.ml_test_dir) / dataset_name / "data.csv"
+                labels_csv = (
+                    Path(config.experiment_config.ml_test_dir)
+                    / dataset_name
+                    / "data.csv"
+                )
             if not labels_csv.exists():
                 print(f"ERROR: Could not find labels CSV for {dataset_name}")
                 print(f"Please specify --labels-csv explicitly")
                 return 1
 
         train_classifier_with_ho(
             config=config,
             features_file=str(features_file),
-            feature_names_file=str(feature_names_file) if feature_names_file.exists() else None,
+            feature_names_file=(
+                str(feature_names_file) if feature_names_file.exists() else None
+            ),
             labels_csv=str(labels_csv),
             model_name=args.model_name,
             n_trials=args.n_trials,
-            val_split=args.val_split
+            val_split=args.val_split,
         )
 
     elif args.features_file:
         # Direct file mode
         train_classifier_with_ho(
@@ -756,18 +859,20 @@
             feature_names_file=args.feature_names_file,
             labels_file=args.labels_file,
             labels_csv=args.labels_csv,
             model_name=args.model_name,
             n_trials=args.n_trials,
-            val_split=args.val_split
+            val_split=args.val_split,
         )
 
     else:
-        print("ERROR: Must specify either --batch-features-dir, --feature-dir, or --features-file")
+        print(
+            "ERROR: Must specify either --batch-features-dir, --feature-dir, or --features-file"
+        )
         parser.print_help()
         return 1
 
     return 0
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     exit(main())
would reformat /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho.py
--- /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers.py	2025-10-18 03:17:20.183916+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers.py	2025-10-18 11:24:40.614394+00:00
@@ -21,19 +21,26 @@
 from sklearn.metrics import classification_report
 from sklearn.preprocessing import LabelEncoder
 
 try:
     import xgboost as xgb
+
     XGBOOST_AVAILABLE = True
 except ImportError:
     XGBOOST_AVAILABLE = False
 
-from src.config import GlobalConfig, ExperimentConfig, ClassifierConfig, DataConfig, OutputConfig
+from src.config import (
+    GlobalConfig,
+    ExperimentConfig,
+    ClassifierConfig,
+    DataConfig,
+    OutputConfig,
+)
 
 
 # Fixed CEFR classes (all 6 levels)
-CEFR_CLASSES = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
+CEFR_CLASSES = ["A1", "A2", "B1", "B2", "C1", "C2"]
 
 
 def get_cefr_label_encoder() -> LabelEncoder:
     """
     Create a fixed label encoder for all 6 CEFR classes.
@@ -64,25 +71,25 @@
 
     elif config.classifier_type == "logistic":
         return LogisticRegression(
             max_iter=config.logistic_max_iter,
             random_state=config.random_state,
-            class_weight=config.logistic_class_weight
+            class_weight=config.logistic_class_weight,
         )
 
     elif config.classifier_type == "randomforest":
         return RandomForestClassifier(
             n_estimators=config.rf_n_estimators,
             random_state=config.random_state,
-            class_weight=config.rf_class_weight
+            class_weight=config.rf_class_weight,
         )
 
     elif config.classifier_type == "svm":
         return LinearSVC(
             max_iter=config.svm_max_iter,
             random_state=config.random_state,
-            class_weight=config.svm_class_weight
+            class_weight=config.svm_class_weight,
         )
 
     elif config.classifier_type == "xgboost":
         if not XGBOOST_AVAILABLE:
             raise ImportError(
@@ -105,12 +112,12 @@
             max_depth=config.xgb_max_depth,
             learning_rate=config.xgb_learning_rate,
             tree_method=tree_method,
             device=device,
             random_state=config.random_state,
-            eval_metric='mlogloss',
-            num_class=len(CEFR_CLASSES)  # Always expect 6 CEFR classes
+            eval_metric="mlogloss",
+            num_class=len(CEFR_CLASSES),  # Always expect 6 CEFR classes
         )
 
     else:
         raise ValueError(f"Unknown classifier type: {config.classifier_type}")
 
@@ -119,11 +126,11 @@
     features_file: str,
     feature_names_file: Optional[str] = None,
     labels_file: Optional[str] = None,
     labels_csv: Optional[str] = None,
     cefr_column: str = "cefr_label",
-    verbose: bool = True
+    verbose: bool = True,
 ) -> Tuple[np.ndarray, np.ndarray, List[str], pd.Series]:
     """
     Load pre-extracted features and labels.
 
     Args:
@@ -149,12 +156,12 @@
     if feature_names_file:
         if verbose:
             print(f"Loading feature names from: {feature_names_file}")
 
         feature_names_path = Path(feature_names_file)
-        if feature_names_path.suffix == '.txt':
-            with open(feature_names_file, 'r') as f:
+        if feature_names_path.suffix == ".txt":
+            with open(feature_names_file, "r") as f:
                 feature_names = [line.strip() for line in f if line.strip()]
         else:  # CSV
             fn_df = pd.read_csv(feature_names_file)
             feature_names = fn_df.iloc[:, 0].tolist()
     else:
@@ -164,11 +171,11 @@
     # Load labels
     if labels_file:
         if verbose:
             print(f"Loading labels from: {labels_file}")
 
-        with open(labels_file, 'r') as f:
+        with open(labels_file, "r") as f:
             y_train = np.array([line.strip() for line in f if line.strip()])
         y_train_series = pd.Series(y_train)
 
     elif labels_csv:
         if verbose:
@@ -202,11 +209,11 @@
     config: GlobalConfig,
     features_file: str,
     feature_names_file: Optional[str] = None,
     labels_file: Optional[str] = None,
     labels_csv: Optional[str] = None,
-    model_name: Optional[str] = None
+    model_name: Optional[str] = None,
 ) -> Path:
     """
     Train a classifier on pre-extracted features.
 
     Args:
@@ -230,11 +237,11 @@
         features_file=features_file,
         feature_names_file=feature_names_file,
         labels_file=labels_file,
         labels_csv=labels_csv,
         cefr_column=data_config.cefr_column,
-        verbose=verbose
+        verbose=verbose,
     )
 
     # Create fixed CEFR label encoder (ensures all 6 classes even if training data lacks some)
     label_encoder = get_cefr_label_encoder()
 
@@ -271,11 +278,11 @@
         feature_type = "tfidf"  # Default
         config_hash = None
 
         if feature_config_path.exists():
             try:
-                with open(feature_config_path, 'r') as f:
+                with open(feature_config_path, "r") as f:
                     feature_cfg = json.load(f)
                     feature_type = feature_cfg.get("feature_type", "tfidf")
                     config_hash = feature_cfg.get("config_hash")
             except:
                 pass
@@ -296,16 +303,16 @@
     model_dir.mkdir(parents=True, exist_ok=True)
 
     # Save model and label encoder
     if config.output_config.save_models:
         model_path = model_dir / "classifier.pkl"
-        with open(model_path, 'wb') as f:
+        with open(model_path, "wb") as f:
             pickle.dump(clf, f)
 
         # Save label encoder
         encoder_path = model_dir / "label_encoder.pkl"
-        with open(encoder_path, 'wb') as f:
+        with open(encoder_path, "wb") as f:
             pickle.dump(label_encoder, f)
 
         if verbose:
             print(f"\n✓ Classifier saved to: {model_path}")
             print(f"✓ Label encoder saved to: {encoder_path}")
@@ -314,36 +321,40 @@
     if config.output_config.save_config:
         tfidf_config = config.tfidf_config
 
         # Extract feature_type and config_hash from model_name if available
         # model_name format: {dataset}_{classifier}_{hash}_{feature_type}
-        parts = model_name.split('_')
+        parts = model_name.split("_")
         feature_type_from_name = parts[-1] if len(parts) >= 4 else "tfidf"
-        config_hash_from_name = parts[-2] if len(parts) >= 4 else tfidf_config.get_hash()
+        config_hash_from_name = (
+            parts[-2] if len(parts) >= 4 else tfidf_config.get_hash()
+        )
 
         model_config = {
             "model_name": model_name,
             "classifier_type": classifier_config.classifier_type,
             "feature_type": feature_type_from_name,
             "config_hash": config_hash_from_name,
             "tfidf_readable_name": tfidf_config.get_readable_name(),
             "tfidf_max_features": tfidf_config.max_features,
             "features_file": str(features_file),
-            "feature_names_file": str(feature_names_file) if feature_names_file else None,
+            "feature_names_file": (
+                str(feature_names_file) if feature_names_file else None
+            ),
             "labels_source": str(labels_file or labels_csv),
             "n_samples": int(len(X_train)),
             "n_features": int(X_train.shape[1]),
             "classes_in_training": sorted([str(c) for c in y_train_series.unique()]),
             "n_classes_in_training": int(len(y_train_series.unique())),
             "all_cefr_classes": CEFR_CLASSES,
             "n_cefr_classes": len(CEFR_CLASSES),
-            "label_encoder": "label_encoder.pkl"
+            "label_encoder": "label_encoder.pkl",
         }
 
         if config.output_config.save_json:
             config_path = model_dir / "config.json"
-            with open(config_path, 'w') as f:
+            with open(config_path, "w") as f:
                 json.dump(model_config, f, indent=2)
 
             if verbose:
                 print(f"✓ Config saved to: {config_path}")
 
@@ -351,11 +362,11 @@
 
 
 def train_all_classifiers(
     config: GlobalConfig,
     features_dir: Optional[str] = None,
-    labels_csv_dir: Optional[str] = None
+    labels_csv_dir: Optional[str] = None,
 ) -> List[Path]:
     """
     Train classifiers for all feature sets in features directory.
 
     Args:
@@ -422,30 +433,35 @@
             feature_config_path = feature_subdir / "config.json"
             feature_config_hash = tfidf_config.get_hash()
             feature_type_str = "tfidf"
 
             if feature_config_path.exists():
-                with open(feature_config_path, 'r') as f:
+                with open(feature_config_path, "r") as f:
                     feature_cfg = json.load(f)
-                    feature_config_hash = feature_cfg.get("config_hash", feature_config_hash)
+                    feature_config_hash = feature_cfg.get(
+                        "config_hash", feature_config_hash
+                    )
                     feature_type_str = feature_cfg.get("feature_type", feature_type_str)
 
             # Include config hash and feature type in model name
             model_name = f"{feature_subdir.name}_{config.classifier_config.classifier_type}_{feature_config_hash}_{feature_type_str}"
 
             model_dir = train_classifier(
                 config,
                 features_file=str(features_file),
-                feature_names_file=str(feature_names_file) if feature_names_file.exists() else None,
+                feature_names_file=(
+                    str(feature_names_file) if feature_names_file.exists() else None
+                ),
                 labels_csv=str(labels_csv),
-                model_name=model_name
+                model_name=model_name,
             )
             model_dirs.append(model_dir)
         except Exception as e:
             print(f"✗ Error: {e}")
             if verbose:
                 import traceback
+
                 traceback.print_exc()
 
     if verbose:
         print("\n" + "=" * 70)
         print("Classifier training complete!")
@@ -455,183 +471,171 @@
 
 def create_parser() -> argparse.ArgumentParser:
     """Create argument parser for train_classifiers."""
     parser = argparse.ArgumentParser(
         description="Train ML classifiers for CEFR classification",
-        formatter_class=argparse.RawDescriptionHelpFormatter
+        formatter_class=argparse.RawDescriptionHelpFormatter,
     )
 
     # Config loading
     config_group = parser.add_argument_group("Configuration Loading")
     config_method = config_group.add_mutually_exclusive_group()
     config_method.add_argument(
-        "-c", "--config-file",
-        help="Path to JSON or YAML config file"
+        "-c", "--config-file", help="Path to JSON or YAML config file"
     )
     config_method.add_argument(
-        "--config-json",
-        help="JSON string containing full configuration"
+        "--config-json", help="JSON string containing full configuration"
     )
 
     # Experiment configuration
     exp_group = parser.add_argument_group("Experiment Configuration")
     exp_group.add_argument(
-        "-e", "--experiment-dir",
-        help="Path to experiment directory (e.g., data/experiments/zero-shot)"
+        "-e",
+        "--experiment-dir",
+        help="Path to experiment directory (e.g., data/experiments/zero-shot)",
     )
     exp_group.add_argument(
-        "-o", "--output-dir",
-        help="Custom output directory for models (default: <experiment-dir>/feature-models)"
+        "-o",
+        "--output-dir",
+        help="Custom output directory for models (default: <experiment-dir>/feature-models)",
     )
 
     # Input data selection
     input_group = parser.add_argument_group("Input Data Selection")
 
     # Simple use case: single feature directory
     input_group.add_argument(
-        "-d", "--feature-dir",
+        "-d",
+        "--feature-dir",
         help="Path to TF-IDF feature directory (e.g., features/norm-EFCAMDAT-train/). "
-             "Must contain features_dense.csv. Optional: feature_names.csv"
+        "Must contain features_dense.csv. Optional: feature_names.csv",
     )
 
     # Advanced: explicit file paths
     input_group.add_argument(
-        "-f", "--features-file",
-        help="Path to pre-extracted features CSV file (flat features, one row per sample)"
+        "-f",
+        "--features-file",
+        help="Path to pre-extracted features CSV file (flat features, one row per sample)",
     )
     input_group.add_argument(
         "--feature-names-file",
-        help="Path to feature names file (CSV or TXT). Optional, uses column names if not provided."
+        help="Path to feature names file (CSV or TXT). Optional, uses column names if not provided.",
     )
     input_group.add_argument(
         "--labels-file",
-        help="Path to labels file (one label per line, matching features row order)"
+        help="Path to labels file (one label per line, matching features row order)",
     )
     input_group.add_argument(
         "--labels-csv",
-        help="Path to CSV file containing labels in a column (use with --cefr-column)"
+        help="Path to CSV file containing labels in a column (use with --cefr-column)",
     )
 
     # Batch processing: multiple feature directories
     input_group.add_argument(
         "--batch-features-dir",
-        help="Directory containing multiple feature subdirectories (for batch processing all)"
+        help="Directory containing multiple feature subdirectories (for batch processing all)",
     )
     input_group.add_argument(
         "--labels-csv-dir",
-        help="Directory containing label CSV files (for batch processing, default: ml-training-data)"
+        help="Directory containing label CSV files (for batch processing, default: ml-training-data)",
     )
 
     input_group.add_argument(
         "--model-name",
-        help="Custom model name (optional, derived from feature directory name if not provided)"
+        help="Custom model name (optional, derived from feature directory name if not provided)",
     )
 
     # Classifier configuration
     clf_group = parser.add_argument_group("Classifier Configuration")
     clf_group.add_argument(
         "--classifier",
         choices=["multinomialnb", "logistic", "randomforest", "svm", "xgboost"],
-        help="Classifier type (default: multinomialnb)"
+        help="Classifier type (default: multinomialnb)",
     )
     clf_group.add_argument(
         "--logistic-max-iter",
         type=int,
-        help="Max iterations for logistic regression (default: 1000)"
+        help="Max iterations for logistic regression (default: 1000)",
     )
     clf_group.add_argument(
         "--rf-n-estimators",
         type=int,
-        help="Number of trees for random forest (default: 100)"
+        help="Number of trees for random forest (default: 100)",
     )
     clf_group.add_argument(
-        "--svm-max-iter",
-        type=int,
-        help="Max iterations for SVM (default: 2000)"
+        "--svm-max-iter", type=int, help="Max iterations for SVM (default: 2000)"
     )
     clf_group.add_argument(
         "--xgb-n-estimators",
         type=int,
-        help="Number of boosting rounds for XGBoost (default: 100)"
+        help="Number of boosting rounds for XGBoost (default: 100)",
     )
     clf_group.add_argument(
-        "--xgb-max-depth",
-        type=int,
-        help="Max tree depth for XGBoost (default: 6)"
+        "--xgb-max-depth", type=int, help="Max tree depth for XGBoost (default: 6)"
     )
     clf_group.add_argument(
         "--xgb-learning-rate",
         type=float,
-        help="Learning rate for XGBoost (default: 0.3)"
+        help="Learning rate for XGBoost (default: 0.3)",
     )
     clf_group.add_argument(
-        "--xgb-use-gpu",
-        action="store_true",
-        help="Use GPU for XGBoost training"
+        "--xgb-use-gpu", action="store_true", help="Use GPU for XGBoost training"
     )
     clf_group.add_argument(
         "--xgb-tree-method",
         choices=["auto", "gpu_hist", "hist", "exact"],
-        help="Tree method for XGBoost (default: auto)"
-    )
-    clf_group.add_argument(
-        "--random-state",
-        type=int,
-        help="Random seed (default: 42)"
-    )
+        help="Tree method for XGBoost (default: auto)",
+    )
+    clf_group.add_argument("--random-state", type=int, help="Random seed (default: 42)")
 
     # Data configuration
     data_group = parser.add_argument_group("Data Configuration")
     data_group.add_argument(
         "--text-column",
         default="text",
-        help="Column name containing text (default: text)"
+        help="Column name containing text (default: text)",
     )
     data_group.add_argument(
         "--label-column",
         default="label",
-        help="Column name containing labels (default: label)"
+        help="Column name containing labels (default: label)",
     )
     data_group.add_argument(
         "--cefr-column",
         default="cefr_label",
-        help="Column name containing CEFR labels (default: cefr_label)"
+        help="Column name containing CEFR labels (default: cefr_label)",
     )
 
     # Output configuration
     output_group = parser.add_argument_group("Output Configuration")
     output_group.add_argument(
-        "--no-save-config",
-        action="store_true",
-        help="Skip saving configuration files"
+        "--no-save-config", action="store_true", help="Skip saving configuration files"
     )
     output_group.add_argument(
-        "-q", "--quiet",
-        action="store_true",
-        help="Suppress verbose output"
+        "-q", "--quiet", action="store_true", help="Suppress verbose output"
     )
 
     return parser
 
 
 def args_to_config(args: argparse.Namespace) -> GlobalConfig:
     """Convert argparse namespace to GlobalConfig."""
     # Check if config file or json string provided
     if args.config_file:
         config_path = Path(args.config_file)
-        if config_path.suffix in ['.yaml', '.yml']:
+        if config_path.suffix in [".yaml", ".yml"]:
             config = GlobalConfig.from_yaml_file(str(config_path))
-        elif config_path.suffix == '.json':
+        elif config_path.suffix == ".json":
             config = GlobalConfig.from_json_file(str(config_path))
         else:
             raise ValueError(f"Unsupported config file format: {config_path.suffix}")
 
         # CLI args override config file
         if args.experiment_dir:
             config.experiment_config = ExperimentConfig(
                 experiment_dir=args.experiment_dir,
-                models_dir=args.output_dir if args.output_dir else None
+                models_dir=args.output_dir if args.output_dir else None,
             )
         elif args.output_dir:
             config.experiment_config.models_dir = args.output_dir
 
         if args.classifier:
@@ -678,11 +682,11 @@
 
     else:
         # Build config from CLI args
         experiment_config = ExperimentConfig(
             experiment_dir=args.experiment_dir or "data/experiments/zero-shot",
-            models_dir=args.output_dir if args.output_dir else None
+            models_dir=args.output_dir if args.output_dir else None,
         )
 
         classifier_config = ClassifierConfig(
             classifier_type=args.classifier or "multinomialnb",
             logistic_max_iter=args.logistic_max_iter or 1000,
@@ -691,29 +695,28 @@
             xgb_n_estimators=args.xgb_n_estimators or 100,
             xgb_max_depth=args.xgb_max_depth or 6,
             xgb_learning_rate=args.xgb_learning_rate or 0.3,
             xgb_use_gpu=args.xgb_use_gpu,
             xgb_tree_method=args.xgb_tree_method or "auto",
-            random_state=args.random_state or 42
+            random_state=args.random_state or 42,
         )
 
         data_config = DataConfig(
             text_column=args.text_column,
             label_column=args.label_column,
-            cefr_column=args.cefr_column
+            cefr_column=args.cefr_column,
         )
 
         output_config = OutputConfig(
-            save_config=not args.no_save_config,
-            verbose=not args.quiet
+            save_config=not args.no_save_config, verbose=not args.quiet
         )
 
         return GlobalConfig(
             experiment_config,
             classifier_config=classifier_config,
             data_config=data_config,
-            output_config=output_config
+            output_config=output_config,
         )
 
 
 def main():
     """Main entry point for classifier training."""
@@ -748,11 +751,13 @@
 
             # Derive dataset name from directory name
             dataset_name = feature_dir_path.name
 
             # Look for corresponding label CSV in ml-training-data
-            labels_csv_path = Path(config.experiment_config.ml_training_dir) / f"{dataset_name}.csv"
+            labels_csv_path = (
+                Path(config.experiment_config.ml_training_dir) / f"{dataset_name}.csv"
+            )
 
             if not labels_csv_path.exists():
                 parser.error(
                     f"Label CSV not found: {labels_csv_path}\n"
                     f"Expected: ml-training-data/{dataset_name}.csv to match {args.feature_dir}"
@@ -764,43 +769,47 @@
                 print(f"CEFR column: {config.data_config.cefr_column}")
 
             train_classifier(
                 config,
                 features_file=str(features_file),
-                feature_names_file=str(feature_names_file) if feature_names_file.exists() else None,
+                feature_names_file=(
+                    str(feature_names_file) if feature_names_file.exists() else None
+                ),
                 labels_csv=str(labels_csv_path),
-                model_name=args.model_name
+                model_name=args.model_name,
             )
 
         elif args.features_file:
             # Advanced: train on specific features file
             if not (args.labels_file or args.labels_csv):
-                parser.error("Must provide either --labels-file or --labels-csv with --features-file")
+                parser.error(
+                    "Must provide either --labels-file or --labels-csv with --features-file"
+                )
 
             train_classifier(
                 config,
                 features_file=args.features_file,
                 feature_names_file=args.feature_names_file,
                 labels_file=args.labels_file,
                 labels_csv=args.labels_csv,
-                model_name=args.model_name
+                model_name=args.model_name,
             )
 
         elif args.batch_features_dir:
             # Batch: train all feature sets in directory
             train_all_classifiers(
                 config,
                 features_dir=args.batch_features_dir,
-                labels_csv_dir=args.labels_csv_dir
+                labels_csv_dir=args.labels_csv_dir,
             )
 
         else:
             # Default: batch process features from experiment directory
             train_all_classifiers(
                 config,
                 features_dir=None,  # Will use default from config
-                labels_csv_dir=args.labels_csv_dir
+                labels_csv_dir=args.labels_csv_dir,
             )
 
     except Exception as e:
         print(f"Error during classifier training: {e}")
         raise
would reformat /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers.py
--- /home/b/p/cefr-classification/minimal-cefr/src/predict.py	2025-10-18 03:33:13.267817+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/predict.py	2025-10-18 11:24:41.728764+00:00
@@ -18,21 +18,23 @@
 from sklearn.pipeline import make_pipeline
 from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
 from sklearn.metrics import precision_recall_fscore_support
 
 from src.config import GlobalConfig, ExperimentConfig, DataConfig, OutputConfig
+
 # Import GroupedTfidfVectorizer so pickle can load it
 from src.train_tfidf_groupby import GroupedTfidfVectorizer
 
 
 class PretrainedTfidfWrapper:
     """
     Wrapper for pretrained TfidfVectorizer to work with sklearn make_pipeline API.
     Used with --preprocess-text option.
     """
+
     def __init__(self, model_path: str):
-        with open(model_path, 'rb') as f:
+        with open(model_path, "rb") as f:
             self.vectorizer = pickle.load(f)
 
     def fit(self, X, y=None):
         # Already fitted, return self for pipeline compatibility
         return self
@@ -48,19 +50,20 @@
     """
     Wrapper for pretrained classifier to work with sklearn make_pipeline API.
     Handles label encoding/decoding for classifiers that use encoded labels.
     Used with --preprocess-text option.
     """
+
     def __init__(self, classifier_dir: Path):
         classifier_path = classifier_dir / "classifier.pkl"
-        with open(classifier_path, 'rb') as f:
+        with open(classifier_path, "rb") as f:
             self.classifier = pickle.load(f)
 
         # Try to load label encoder if it exists
         encoder_path = classifier_dir / "label_encoder.pkl"
         if encoder_path.exists():
-            with open(encoder_path, 'rb') as f:
+            with open(encoder_path, "rb") as f:
                 self.label_encoder = pickle.load(f)
         else:
             # Backward compatibility: old models without label encoder
             self.label_encoder = None
 
@@ -76,11 +79,11 @@
             return self.label_encoder.inverse_transform(y_pred_encoded)
         else:
             return y_pred_encoded
 
     def predict_proba(self, X):
-        if hasattr(self.classifier, 'predict_proba'):
+        if hasattr(self.classifier, "predict_proba"):
             return self.classifier.predict_proba(X)
         raise AttributeError("Classifier does not support predict_proba")
 
 
 def load_classifier_and_encoder(classifier_dir: Path) -> Tuple:
@@ -95,29 +98,29 @@
     """
     classifier_path = classifier_dir / "classifier.pkl"
     if not classifier_path.exists():
         raise FileNotFoundError(f"Classifier not found: {classifier_path}")
 
-    with open(classifier_path, 'rb') as f:
+    with open(classifier_path, "rb") as f:
         classifier = pickle.load(f)
 
     # Load label encoder if it exists
     encoder_path = classifier_dir / "label_encoder.pkl"
     label_encoder = None
     if encoder_path.exists():
-        with open(encoder_path, 'rb') as f:
+        with open(encoder_path, "rb") as f:
             label_encoder = pickle.load(f)
 
     return classifier, label_encoder
 
 
 def load_features_and_labels(
     features_file: str,
     labels_file: Optional[str] = None,
     labels_csv: Optional[str] = None,
     cefr_column: str = "cefr_label",
-    verbose: bool = True
+    verbose: bool = True,
 ) -> Tuple[np.ndarray, Optional[np.ndarray], Optional[pd.Series]]:
     """
     Load pre-extracted features and optional labels for evaluation.
 
     Args:
@@ -143,11 +146,11 @@
 
     if labels_file:
         if verbose:
             print(f"Loading labels from: {labels_file}")
 
-        with open(labels_file, 'r') as f:
+        with open(labels_file, "r") as f:
             y_test = np.array([line.strip() for line in f if line.strip()])
         y_test_series = pd.Series(y_test)
 
     elif labels_csv:
         if verbose:
@@ -177,11 +180,11 @@
 def cefr_classification_report(
     y_true: np.ndarray,
     y_pred: np.ndarray,
     labels: Optional[List] = None,
     target_names: Optional[List[str]] = None,
-    digits: int = 2
+    digits: int = 2,
 ) -> str:
     """
     Custom CEFR multiclass classification report.
 
     Similar interface to sklearn and imblearn classification reports but with
@@ -208,14 +211,16 @@
         y_true, y_pred, labels=labels, average=None, zero_division=0
     )
 
     # Calculate macro and weighted averages
     macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(
-        y_true, y_pred, labels=labels, average='macro', zero_division=0
-    )
-    weighted_precision, weighted_recall, weighted_f1, _ = precision_recall_fscore_support(
-        y_true, y_pred, labels=labels, average='weighted', zero_division=0
+        y_true, y_pred, labels=labels, average="macro", zero_division=0
+    )
+    weighted_precision, weighted_recall, weighted_f1, _ = (
+        precision_recall_fscore_support(
+            y_true, y_pred, labels=labels, average="weighted", zero_division=0
+        )
     )
 
     # Calculate accuracy
     accuracy = accuracy_score(y_true, y_pred)
 
@@ -226,55 +231,57 @@
     y_pred_idx = np.array([cefr_order.get(y, -1) for y in y_pred])
     adjacent_correct = np.abs(y_true_idx - y_pred_idx) <= 1
     adjacent_accuracy = np.mean(adjacent_correct[y_true_idx >= 0])
 
     # Build report string
-    headers = ['precision', 'recall', 'f1-score', 'support']
-    head_fmt = '{:>{width}s} ' + ' {:>9}' * len(headers)
-    report = head_fmt.format('', *headers, width=max(len(name) for name in target_names))
-    report += '\n\n'
-
-    row_fmt = '{:>{width}s} ' + ' {:>9.{digits}f}' * 3 + ' {:>9}\n'
+    headers = ["precision", "recall", "f1-score", "support"]
+    head_fmt = "{:>{width}s} " + " {:>9}" * len(headers)
+    report = head_fmt.format(
+        "", *headers, width=max(len(name) for name in target_names)
+    )
+    report += "\n\n"
+
+    row_fmt = "{:>{width}s} " + " {:>9.{digits}f}" * 3 + " {:>9}\n"
 
     # Per-class rows
     for i, label in enumerate(labels):
         report += row_fmt.format(
             target_names[i],
             precision[i],
             recall[i],
             f1[i],
             int(support[i]),
             width=max(len(name) for name in target_names),
-            digits=digits
+            digits=digits,
         )
 
-    report += '\n'
+    report += "\n"
 
     # Macro average
     report += row_fmt.format(
-        'macro avg',
+        "macro avg",
         macro_precision,
         macro_recall,
         macro_f1,
         int(np.sum(support)),
         width=max(len(name) for name in target_names),
-        digits=digits
+        digits=digits,
     )
 
     # Weighted average
     report += row_fmt.format(
-        'weighted avg',
+        "weighted avg",
         weighted_precision,
         weighted_recall,
         weighted_f1,
         int(np.sum(support)),
         width=max(len(name) for name in target_names),
-        digits=digits
+        digits=digits,
     )
 
     # Add CEFR-specific metrics
-    report += '\n'
+    report += "\n"
     report += f"{'accuracy':{max(len(name) for name in target_names)}s} "
     report += f"{accuracy:>9.{digits}f}"
     report += f" {int(np.sum(support)):>9}\n"
 
     report += f"{'adjacent accuracy':{max(len(name) for name in target_names)}s} "
@@ -288,11 +295,11 @@
     y_true: np.ndarray,
     y_pred_proba: np.ndarray,
     labels: Optional[List] = None,
     target_names: Optional[List[str]] = None,
     n_bins: int = 10,
-    digits: int = 2
+    digits: int = 2,
 ) -> str:
     """
     Custom CEFR calibration report for soft probabilities.
 
     Evaluates how well predicted probabilities match actual outcomes,
@@ -342,39 +349,45 @@
             accuracy_in_bin = np.mean(y_true[in_bin] == y_pred[in_bin])
             avg_confidence_in_bin = np.mean(confidence[in_bin])
 
             ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
 
-            bin_stats.append({
-                'range': f"({bin_lower:.2f}, {bin_upper:.2f}]",
-                'count': int(np.sum(in_bin)),
-                'accuracy': accuracy_in_bin,
-                'confidence': avg_confidence_in_bin,
-                'calibration_gap': avg_confidence_in_bin - accuracy_in_bin
-            })
+            bin_stats.append(
+                {
+                    "range": f"({bin_lower:.2f}, {bin_upper:.2f}]",
+                    "count": int(np.sum(in_bin)),
+                    "accuracy": accuracy_in_bin,
+                    "confidence": avg_confidence_in_bin,
+                    "calibration_gap": avg_confidence_in_bin - accuracy_in_bin,
+                }
+            )
         else:
-            bin_stats.append({
-                'range': f"({bin_lower:.2f}, {bin_upper:.2f}]",
-                'count': 0,
-                'accuracy': 0.0,
-                'confidence': 0.0,
-                'calibration_gap': 0.0
-            })
+            bin_stats.append(
+                {
+                    "range": f"({bin_lower:.2f}, {bin_upper:.2f}]",
+                    "count": 0,
+                    "accuracy": 0.0,
+                    "confidence": 0.0,
+                    "calibration_gap": 0.0,
+                }
+            )
 
     # Build report
     report = "CEFR CALIBRATION REPORT (Soft Probabilities)\n"
     report += "=" * 80 + "\n\n"
 
     report += f"Expected Calibration Error (ECE): {ece:.{digits}f}\n\n"
 
     # Bin statistics
     report += "Confidence Bins:\n"
-    report += f"{'Range':<20} {'Count':>10} {'Accuracy':>12} {'Confidence':>12} {'Gap':>12}\n"
+    report += (
+        f"{'Range':<20} {'Count':>10} {'Accuracy':>12} {'Confidence':>12} {'Gap':>12}\n"
+    )
     report += "-" * 80 + "\n"
 
     for stat in bin_stats:
-        if stat['count'] > 0:
+        if stat["count"] > 0:
             report += f"{stat['range']:<20} {stat['count']:>10} "
             report += f"{stat['accuracy']:>12.{digits}f} "
             report += f"{stat['confidence']:>12.{digits}f} "
             report += f"{stat['calibration_gap']:>12.{digits}f}\n"
 
@@ -402,11 +415,11 @@
 def predict_on_features(
     config: GlobalConfig,
     classifier_model_name: str,
     features_file: str,
     labels_file: Optional[str] = None,
-    labels_csv: Optional[str] = None
+    labels_csv: Optional[str] = None,
 ) -> Tuple[Optional[np.ndarray], np.ndarray]:
     """
     Make predictions on pre-extracted features.
 
     Args:
@@ -433,11 +446,11 @@
     expected_feature_type = None
     expected_config_hash = None
 
     if classifier_config_path.exists():
         try:
-            with open(classifier_config_path, 'r') as f:
+            with open(classifier_config_path, "r") as f:
                 clf_cfg = json.load(f)
                 expected_feature_type = clf_cfg.get("feature_type")
                 expected_config_hash = clf_cfg.get("config_hash")
         except:
             pass
@@ -456,11 +469,11 @@
     X_test, y_test, y_test_series = load_features_and_labels(
         features_file=features_file,
         labels_file=labels_file,
         labels_csv=labels_csv,
         cefr_column=data_config.cefr_column,
-        verbose=verbose
+        verbose=verbose,
     )
 
     # Make predictions
     if verbose:
         print("\nMaking predictions...")
@@ -473,16 +486,18 @@
     else:
         y_pred = y_pred_encoded
 
     # Get probabilities if available
     y_pred_proba = None
-    if hasattr(classifier, 'predict_proba'):
+    if hasattr(classifier, "predict_proba"):
         try:
             y_pred_proba = classifier.predict_proba(X_test)
         except (AttributeError, NotImplementedError):
             if verbose:
-                print("Note: Classifier does not support predict_proba, skipping calibration report")
+                print(
+                    "Note: Classifier does not support predict_proba, skipping calibration report"
+                )
 
     # Get full model class list (needed for calibration and results saving)
     if label_encoder is not None:
         # Use all classes from the encoder
         model_classes = label_encoder.classes_.tolist()
@@ -498,16 +513,18 @@
         labels_list = sorted(y_test_series.unique())
 
         print("\n" + "=" * 80)
         print("CEFR CLASSIFICATION REPORT (Multiclass)")
         print("=" * 80)
-        print(cefr_classification_report(
-            y_test,
-            y_pred,
-            labels=labels_list,
-            target_names=[str(l) for l in labels_list]
-        ))
+        print(
+            cefr_classification_report(
+                y_test,
+                y_pred,
+                labels=labels_list,
+                target_names=[str(l) for l in labels_list],
+            )
+        )
 
         print("\n" + "=" * 80)
         print("STANDARD CLASSIFICATION REPORT")
         print("=" * 80)
         print(classification_report(y_test, y_pred, zero_division=0))
@@ -520,16 +537,18 @@
         print(cm)
 
         # Print calibration report if probabilities available
         if y_pred_proba is not None:
             print("\n" + "=" * 80)
-            print(cefr_calibration_report(
-                y_test,
-                y_pred_proba,
-                labels=model_classes,  # Use full model classes, not just test labels
-                target_names=[str(l) for l in model_classes]
-            ))
+            print(
+                cefr_calibration_report(
+                    y_test,
+                    y_pred_proba,
+                    labels=model_classes,  # Use full model classes, not just test labels
+                    target_names=[str(l) for l in model_classes],
+                )
+            )
             print("=" * 80)
 
     # Save results organized by model, then by dataset
     if config.output_config.save_results:
         # Derive dataset name from features file
@@ -539,11 +558,13 @@
         else:
             dataset_name = features_path.stem
 
         # Organize results by: model/dataset/
         # This prevents overwrites when same dataset is used with different TF-IDF configs or classifiers
-        results_dir = Path(exp_config.results_dir) / classifier_model_name / dataset_name
+        results_dir = (
+            Path(exp_config.results_dir) / classifier_model_name / dataset_name
+        )
         results_dir.mkdir(parents=True, exist_ok=True)
 
         # Determine actual classes from probability array shape
         if y_pred_proba is not None:
             n_proba_classes = y_pred_proba.shape[1]
@@ -556,37 +577,36 @@
         if y_pred_proba is not None and config.output_config.save_json:
             soft_predictions = []
             for i in range(len(y_pred_proba)):
                 pred_dict = {
                     "sample_id": i,
-                    "probabilities": {str(cls): float(y_pred_proba[i][j])
-                                     for j, cls in enumerate(proba_classes)}
+                    "probabilities": {
+                        str(cls): float(y_pred_proba[i][j])
+                        for j, cls in enumerate(proba_classes)
+                    },
                 }
                 if y_test is not None:
                     pred_dict["true_label"] = str(y_test[i])
                 soft_predictions.append(pred_dict)
 
             soft_pred_path = results_dir / "soft_predictions.json"
-            with open(soft_pred_path, 'w') as f:
+            with open(soft_pred_path, "w") as f:
                 json.dump(soft_predictions, f, indent=2)
 
         # Save argmax predictions
         if config.output_config.save_json:
             argmax_predictions = []
             for i in range(len(y_pred)):
-                pred_dict = {
-                    "sample_id": i,
-                    "predicted_label": str(y_pred[i])
-                }
+                pred_dict = {"sample_id": i, "predicted_label": str(y_pred[i])}
                 if y_test is not None:
                     pred_dict["true_label"] = str(y_test[i])
                 if y_pred_proba is not None:
                     pred_dict["confidence"] = float(np.max(y_pred_proba[i]))
                 argmax_predictions.append(pred_dict)
 
             argmax_pred_path = results_dir / "argmax_predictions.json"
-            with open(argmax_pred_path, 'w') as f:
+            with open(argmax_pred_path, "w") as f:
                 json.dump(argmax_predictions, f, indent=2)
 
         # Save rounded average predictions (regression-style)
         if y_pred_proba is not None and config.output_config.save_json:
             rounded_avg_predictions = []
@@ -596,11 +616,13 @@
             idx_to_class = {idx: cls for idx, cls in enumerate(proba_classes)}
 
             y_pred_rounded_avg = []
             for i in range(len(y_pred_proba)):
                 # Calculate expected value (weighted average of class indices)
-                expected_idx = np.sum([j * y_pred_proba[i][j] for j in range(len(proba_classes))])
+                expected_idx = np.sum(
+                    [j * y_pred_proba[i][j] for j in range(len(proba_classes))]
+                )
                 # Round to nearest integer index
                 rounded_idx = int(np.round(expected_idx))
                 # Clip to valid range
                 rounded_idx = np.clip(rounded_idx, 0, len(proba_classes) - 1)
                 # Map back to class label
@@ -609,18 +631,18 @@
 
                 pred_dict = {
                     "sample_id": i,
                     "predicted_label": str(pred_label),
                     "expected_value": float(expected_idx),
-                    "rounded_index": int(rounded_idx)
+                    "rounded_index": int(rounded_idx),
                 }
                 if y_test is not None:
                     pred_dict["true_label"] = str(y_test[i])
                 rounded_avg_predictions.append(pred_dict)
 
             rounded_avg_path = results_dir / "rounded_avg_predictions.json"
-            with open(rounded_avg_path, 'w') as f:
+            with open(rounded_avg_path, "w") as f:
                 json.dump(rounded_avg_predictions, f, indent=2)
 
             # Generate reports for rounded average strategy
             if y_test is not None and verbose:
                 y_pred_rounded_avg = np.array(y_pred_rounded_avg)
@@ -628,38 +650,46 @@
                 print("\n" + "=" * 80)
                 print("ROUNDED AVERAGE STRATEGY RESULTS")
                 print("=" * 80)
 
                 print("\nCEFR CLASSIFICATION REPORT (Rounded Avg):")
-                print(cefr_classification_report(
-                    y_test,
-                    y_pred_rounded_avg,
-                    labels=labels_list,
-                    target_names=[str(l) for l in labels_list]
-                ))
+                print(
+                    cefr_classification_report(
+                        y_test,
+                        y_pred_rounded_avg,
+                        labels=labels_list,
+                        target_names=[str(l) for l in labels_list],
+                    )
+                )
 
         # Save markdown reports
         if y_test is not None:
             report_path = results_dir / "evaluation_report.md"
-            with open(report_path, 'w') as f:
+            with open(report_path, "w") as f:
                 f.write(f"# Evaluation Report: {dataset_name}\n\n")
                 f.write(f"**Classifier**: {classifier_model_name}\n")
                 f.write(f"**Dataset**: {dataset_name}\n")
                 f.write(f"**Samples**: {len(y_test)}\n")
-                f.write(f"**Classes in test set**: {', '.join(map(str, labels_list))}\n\n")
+                f.write(
+                    f"**Classes in test set**: {', '.join(map(str, labels_list))}\n\n"
+                )
 
                 # Argmax strategy
                 f.write("## Strategy 1: Argmax Predictions\n\n")
-                f.write("Standard argmax strategy: predict class with highest probability.\n\n")
+                f.write(
+                    "Standard argmax strategy: predict class with highest probability.\n\n"
+                )
                 f.write("### CEFR Classification Report\n\n")
                 f.write("```\n")
-                f.write(cefr_classification_report(
-                    y_test,
-                    y_pred,
-                    labels=labels_list,
-                    target_names=[str(l) for l in labels_list]
-                ))
+                f.write(
+                    cefr_classification_report(
+                        y_test,
+                        y_pred,
+                        labels=labels_list,
+                        target_names=[str(l) for l in labels_list],
+                    )
+                )
                 f.write("```\n\n")
 
                 f.write("### Standard Classification Report\n\n")
                 f.write("```\n")
                 f.write(classification_report(y_test, y_pred, zero_division=0))
@@ -674,42 +704,54 @@
 
                 # Calibration report
                 if y_pred_proba is not None:
                     f.write("### Calibration Report\n\n")
                     f.write("```\n")
-                    f.write(cefr_calibration_report(
-                        y_test,
-                        y_pred_proba,
-                        labels=model_classes,
-                        target_names=[str(l) for l in model_classes]
-                    ))
+                    f.write(
+                        cefr_calibration_report(
+                            y_test,
+                            y_pred_proba,
+                            labels=model_classes,
+                            target_names=[str(l) for l in model_classes],
+                        )
+                    )
                     f.write("```\n\n")
 
                 # Rounded average strategy
                 if y_pred_proba is not None:
                     f.write("## Strategy 2: Rounded Average Predictions\n\n")
-                    f.write("Regression-style strategy: calculate expected class index from probabilities, ")
+                    f.write(
+                        "Regression-style strategy: calculate expected class index from probabilities, "
+                    )
                     f.write("round to nearest integer, map back to class label.\n\n")
                     f.write("### CEFR Classification Report\n\n")
                     f.write("```\n")
-                    f.write(cefr_classification_report(
-                        y_test,
-                        y_pred_rounded_avg,
-                        labels=labels_list,
-                        target_names=[str(l) for l in labels_list]
-                    ))
+                    f.write(
+                        cefr_classification_report(
+                            y_test,
+                            y_pred_rounded_avg,
+                            labels=labels_list,
+                            target_names=[str(l) for l in labels_list],
+                        )
+                    )
                     f.write("```\n\n")
 
                     f.write("### Standard Classification Report\n\n")
                     f.write("```\n")
-                    f.write(classification_report(y_test, y_pred_rounded_avg, zero_division=0))
+                    f.write(
+                        classification_report(
+                            y_test, y_pred_rounded_avg, zero_division=0
+                        )
+                    )
                     f.write("```\n\n")
 
                     f.write("### Confusion Matrix\n\n")
                     f.write(f"Labels (rows=true, cols=pred): {labels_list}\n\n")
                     f.write("```\n")
-                    cm_rounded = confusion_matrix(y_test, y_pred_rounded_avg, labels=labels_list)
+                    cm_rounded = confusion_matrix(
+                        y_test, y_pred_rounded_avg, labels=labels_list
+                    )
                     f.write(str(cm_rounded))
                     f.write("\n```\n\n")
 
         if verbose:
             print(f"\n✓ Results saved to: {results_dir}/")
@@ -727,11 +769,11 @@
 
 def predict_all_feature_sets(
     config: GlobalConfig,
     classifier_model_name: str,
     features_dir: str,
-    labels_csv_dir: Optional[str] = None
+    labels_csv_dir: Optional[str] = None,
 ) -> List[Tuple]:
     """
     Run predictions for all feature sets in a directory.
 
     Args:
@@ -781,41 +823,46 @@
         # Try test data directory first, then training data directory
         labels_csv = Path(labels_csv_dir) / f"{feature_subdir.name}.csv"
 
         if not labels_csv.exists():
             # Try training data directory
-            training_labels_csv = Path(exp_config.ml_training_dir) / f"{feature_subdir.name}.csv"
+            training_labels_csv = (
+                Path(exp_config.ml_training_dir) / f"{feature_subdir.name}.csv"
+            )
             if training_labels_csv.exists():
                 labels_csv = training_labels_csv
                 if verbose:
-                    print(f"  Using labels from training data: {training_labels_csv.name}")
+                    print(
+                        f"  Using labels from training data: {training_labels_csv.name}"
+                    )
             else:
                 if verbose:
-                    print(f"⚠ Labels CSV not found in test or training data (predictions only, no evaluation)")
+                    print(
+                        f"⚠ Labels CSV not found in test or training data (predictions only, no evaluation)"
+                    )
                 labels_csv = None
 
         try:
             y_test, y_pred = predict_on_features(
                 config,
                 classifier_model_name=classifier_model_name,
                 features_file=str(features_file),
-                labels_csv=str(labels_csv) if labels_csv else None
+                labels_csv=str(labels_csv) if labels_csv else None,
             )
             results.append((y_test, y_pred))
         except Exception as e:
             print(f"✗ Error: {e}")
             if verbose:
                 import traceback
+
                 traceback.print_exc()
 
     return results
 
 
 def predict_with_text_pipeline(
-    config: GlobalConfig,
-    classifier_model_name: str,
-    test_file: str
+    config: GlobalConfig, classifier_model_name: str, test_file: str
 ) -> Tuple[np.ndarray, np.ndarray]:
     """
     Make predictions using TF-IDF pipeline with raw text (legacy mode).
     Requires --preprocess-text flag.
 
@@ -857,16 +904,20 @@
 
     df_test = pd.read_csv(test_file_path)
 
     # Validate columns
     if data_config.text_column not in df_test.columns:
-        raise ValueError(f"Text column '{data_config.text_column}' not found in {test_file_path}")
+        raise ValueError(
+            f"Text column '{data_config.text_column}' not found in {test_file_path}"
+        )
 
     if data_config.label_column not in df_test.columns:
-        raise ValueError(f"Label column '{data_config.label_column}' not found in {test_file_path}")
-
-    X_test = df_test[data_config.text_column].fillna('').astype(str)
+        raise ValueError(
+            f"Label column '{data_config.label_column}' not found in {test_file_path}"
+        )
+
+    X_test = df_test[data_config.text_column].fillna("").astype(str)
     y_test = df_test[data_config.label_column].values
 
     if verbose:
         print(f"Loaded {len(X_test)} samples")
         print(f"True classes: {sorted(pd.Series(y_test).unique())}")
@@ -875,11 +926,11 @@
     if verbose:
         print("\nCreating sklearn pipeline with TF-IDF + classifier...")
 
     pipeline = make_pipeline(
         PretrainedTfidfWrapper(str(tfidf_model_path)),
-        PretrainedClassifierWrapper(classifier_dir)
+        PretrainedClassifierWrapper(classifier_dir),
     )
 
     # Make predictions
     if verbose:
         print("Making predictions...")
@@ -890,32 +941,39 @@
     y_pred_proba = None
     try:
         y_pred_proba = pipeline.predict_proba(X_test)
     except (AttributeError, NotImplementedError):
         if verbose:
-            print("Note: Classifier does not support predict_proba, skipping calibration report")
+            print(
+                "Note: Classifier does not support predict_proba, skipping calibration report"
+            )
 
     # Print evaluation reports
     if verbose:
         labels_list = sorted(pd.Series(y_test).unique())
 
         # Get full model class list from the classifier wrapper (for calibration report)
         classifier_wrapper = pipeline.steps[-1][1]
-        if hasattr(classifier_wrapper, 'label_encoder') and classifier_wrapper.label_encoder is not None:
+        if (
+            hasattr(classifier_wrapper, "label_encoder")
+            and classifier_wrapper.label_encoder is not None
+        ):
             model_classes = classifier_wrapper.label_encoder.classes_.tolist()
         else:
             model_classes = labels_list
 
         print("\n" + "=" * 80)
         print("CEFR CLASSIFICATION REPORT (Multiclass)")
         print("=" * 80)
-        print(cefr_classification_report(
-            y_test,
-            y_pred,
-            labels=labels_list,
-            target_names=[str(l) for l in labels_list]
-        ))
+        print(
+            cefr_classification_report(
+                y_test,
+                y_pred,
+                labels=labels_list,
+                target_names=[str(l) for l in labels_list],
+            )
+        )
 
         print("\n" + "=" * 80)
         print("STANDARD CLASSIFICATION REPORT")
         print("=" * 80)
         print(classification_report(y_test, y_pred, zero_division=0))
@@ -928,16 +986,18 @@
         print(cm)
 
         # Print calibration report if probabilities available
         if y_pred_proba is not None:
             print("\n" + "=" * 80)
-            print(cefr_calibration_report(
-                y_test,
-                y_pred_proba,
-                labels=model_classes,  # Use full model classes, not just test labels
-                target_names=[str(l) for l in model_classes]
-            ))
+            print(
+                cefr_calibration_report(
+                    y_test,
+                    y_pred_proba,
+                    labels=model_classes,  # Use full model classes, not just test labels
+                    target_names=[str(l) for l in model_classes],
+                )
+            )
             print("=" * 80)
 
     # Save results organized by dataset
     if config.output_config.save_results:
         dataset_name = Path(test_file).stem
@@ -956,35 +1016,37 @@
         if y_pred_proba is not None and config.output_config.save_json:
             soft_predictions = []
             for i in range(len(y_pred_proba)):
                 pred_dict = {
                     "sample_id": i,
-                    "probabilities": {str(cls): float(y_pred_proba[i][j])
-                                     for j, cls in enumerate(proba_classes)}
+                    "probabilities": {
+                        str(cls): float(y_pred_proba[i][j])
+                        for j, cls in enumerate(proba_classes)
+                    },
                 }
                 pred_dict["true_label"] = str(y_test[i])
                 soft_predictions.append(pred_dict)
 
             soft_pred_path = results_dir / "soft_predictions.json"
-            with open(soft_pred_path, 'w') as f:
+            with open(soft_pred_path, "w") as f:
                 json.dump(soft_predictions, f, indent=2)
 
         # Save argmax predictions
         if config.output_config.save_json:
             argmax_predictions = []
             for i in range(len(y_pred)):
                 pred_dict = {
                     "sample_id": i,
                     "predicted_label": str(y_pred[i]),
-                    "true_label": str(y_test[i])
+                    "true_label": str(y_test[i]),
                 }
                 if y_pred_proba is not None:
                     pred_dict["confidence"] = float(np.max(y_pred_proba[i]))
                 argmax_predictions.append(pred_dict)
 
             argmax_pred_path = results_dir / "argmax_predictions.json"
-            with open(argmax_pred_path, 'w') as f:
+            with open(argmax_pred_path, "w") as f:
                 json.dump(argmax_predictions, f, indent=2)
 
         # Save rounded average predictions (regression-style)
         if y_pred_proba is not None and config.output_config.save_json:
             rounded_avg_predictions = []
@@ -994,11 +1056,13 @@
             idx_to_class = {idx: cls for idx, cls in enumerate(proba_classes)}
 
             y_pred_rounded_avg = []
             for i in range(len(y_pred_proba)):
                 # Calculate expected value (weighted average of class indices)
-                expected_idx = np.sum([j * y_pred_proba[i][j] for j in range(len(proba_classes))])
+                expected_idx = np.sum(
+                    [j * y_pred_proba[i][j] for j in range(len(proba_classes))]
+                )
                 # Round to nearest integer index
                 rounded_idx = int(np.round(expected_idx))
                 # Clip to valid range
                 rounded_idx = np.clip(rounded_idx, 0, len(proba_classes) - 1)
                 # Map back to class label
@@ -1008,16 +1072,16 @@
                 pred_dict = {
                     "sample_id": i,
                     "predicted_label": str(pred_label),
                     "expected_value": float(expected_idx),
                     "rounded_index": int(rounded_idx),
-                    "true_label": str(y_test[i])
+                    "true_label": str(y_test[i]),
                 }
                 rounded_avg_predictions.append(pred_dict)
 
             rounded_avg_path = results_dir / "rounded_avg_predictions.json"
-            with open(rounded_avg_path, 'w') as f:
+            with open(rounded_avg_path, "w") as f:
                 json.dump(rounded_avg_predictions, f, indent=2)
 
             # Generate reports for rounded average strategy
             if verbose:
                 y_pred_rounded_avg = np.array(y_pred_rounded_avg)
@@ -1025,37 +1089,43 @@
                 print("\n" + "=" * 80)
                 print("ROUNDED AVERAGE STRATEGY RESULTS")
                 print("=" * 80)
 
                 print("\nCEFR CLASSIFICATION REPORT (Rounded Avg):")
-                print(cefr_classification_report(
-                    y_test,
-                    y_pred_rounded_avg,
-                    labels=labels_list,
-                    target_names=[str(l) for l in labels_list]
-                ))
+                print(
+                    cefr_classification_report(
+                        y_test,
+                        y_pred_rounded_avg,
+                        labels=labels_list,
+                        target_names=[str(l) for l in labels_list],
+                    )
+                )
 
         # Save markdown reports
         report_path = results_dir / "evaluation_report.md"
-        with open(report_path, 'w') as f:
+        with open(report_path, "w") as f:
             f.write(f"# Evaluation Report: {dataset_name}\n\n")
             f.write(f"**Classifier**: {classifier_model_name}\n")
             f.write(f"**Dataset**: {dataset_name}\n")
             f.write(f"**Samples**: {len(y_test)}\n")
             f.write(f"**Classes in test set**: {', '.join(map(str, labels_list))}\n\n")
 
             # Argmax strategy
             f.write("## Strategy 1: Argmax Predictions\n\n")
-            f.write("Standard argmax strategy: predict class with highest probability.\n\n")
+            f.write(
+                "Standard argmax strategy: predict class with highest probability.\n\n"
+            )
             f.write("### CEFR Classification Report\n\n")
             f.write("```\n")
-            f.write(cefr_classification_report(
-                y_test,
-                y_pred,
-                labels=labels_list,
-                target_names=[str(l) for l in labels_list]
-            ))
+            f.write(
+                cefr_classification_report(
+                    y_test,
+                    y_pred,
+                    labels=labels_list,
+                    target_names=[str(l) for l in labels_list],
+                )
+            )
             f.write("```\n\n")
 
             f.write("### Standard Classification Report\n\n")
             f.write("```\n")
             f.write(classification_report(y_test, y_pred, zero_division=0))
@@ -1070,42 +1140,52 @@
 
             # Calibration report
             if y_pred_proba is not None:
                 f.write("### Calibration Report\n\n")
                 f.write("```\n")
-                f.write(cefr_calibration_report(
-                    y_test,
-                    y_pred_proba,
-                    labels=model_classes,
-                    target_names=[str(l) for l in model_classes]
-                ))
+                f.write(
+                    cefr_calibration_report(
+                        y_test,
+                        y_pred_proba,
+                        labels=model_classes,
+                        target_names=[str(l) for l in model_classes],
+                    )
+                )
                 f.write("```\n\n")
 
             # Rounded average strategy
             if y_pred_proba is not None:
                 f.write("## Strategy 2: Rounded Average Predictions\n\n")
-                f.write("Regression-style strategy: calculate expected class index from probabilities, ")
+                f.write(
+                    "Regression-style strategy: calculate expected class index from probabilities, "
+                )
                 f.write("round to nearest integer, map back to class label.\n\n")
                 f.write("### CEFR Classification Report\n\n")
                 f.write("```\n")
-                f.write(cefr_classification_report(
-                    y_test,
-                    y_pred_rounded_avg,
-                    labels=labels_list,
-                    target_names=[str(l) for l in labels_list]
-                ))
+                f.write(
+                    cefr_classification_report(
+                        y_test,
+                        y_pred_rounded_avg,
+                        labels=labels_list,
+                        target_names=[str(l) for l in labels_list],
+                    )
+                )
                 f.write("```\n\n")
 
                 f.write("### Standard Classification Report\n\n")
                 f.write("```\n")
-                f.write(classification_report(y_test, y_pred_rounded_avg, zero_division=0))
+                f.write(
+                    classification_report(y_test, y_pred_rounded_avg, zero_division=0)
+                )
                 f.write("```\n\n")
 
                 f.write("### Confusion Matrix\n\n")
                 f.write(f"Labels (rows=true, cols=pred): {labels_list}\n\n")
                 f.write("```\n")
-                cm_rounded = confusion_matrix(y_test, y_pred_rounded_avg, labels=labels_list)
+                cm_rounded = confusion_matrix(
+                    y_test, y_pred_rounded_avg, labels=labels_list
+                )
                 f.write(str(cm_rounded))
                 f.write("\n```\n\n")
 
         if verbose:
             print(f"\n✓ Results saved to: {results_dir}/")
@@ -1122,157 +1202,155 @@
 
 def create_parser() -> argparse.ArgumentParser:
     """Create argument parser for predict."""
     parser = argparse.ArgumentParser(
         description="Make predictions using pretrained classifiers (default: pre-extracted features, optional: --preprocess-text for raw text)",
-        formatter_class=argparse.RawDescriptionHelpFormatter
+        formatter_class=argparse.RawDescriptionHelpFormatter,
     )
 
     # Config loading
     config_group = parser.add_argument_group("Configuration Loading")
     config_method = config_group.add_mutually_exclusive_group()
     config_method.add_argument(
-        "-c", "--config-file",
-        help="Path to JSON or YAML config file"
+        "-c", "--config-file", help="Path to JSON or YAML config file"
     )
     config_method.add_argument(
-        "--config-json",
-        help="JSON string containing full configuration"
+        "--config-json", help="JSON string containing full configuration"
     )
 
     # Experiment configuration
     exp_group = parser.add_argument_group("Experiment Configuration")
     exp_group.add_argument(
-        "-e", "--experiment-dir",
-        help="Path to experiment directory (e.g., data/experiments/zero-shot)"
+        "-e",
+        "--experiment-dir",
+        help="Path to experiment directory (e.g., data/experiments/zero-shot)",
     )
     exp_group.add_argument(
-        "-o", "--output-dir",
-        help="Custom output directory for models (default: <experiment-dir>/feature-models)"
+        "-o",
+        "--output-dir",
+        help="Custom output directory for models (default: <experiment-dir>/feature-models)",
     )
 
     # Classifier selection
     clf_group = parser.add_argument_group("Classifier Selection")
     clf_group.add_argument(
-        "-m", "--classifier-model",
+        "-m",
+        "--classifier-model",
         required=True,
-        help="Classifier model name (e.g., norm-EFCAMDAT-train_xgboost)"
+        help="Classifier model name (e.g., norm-EFCAMDAT-train_xgboost)",
     )
 
     # Processing mode
     mode_group = parser.add_argument_group("Processing Mode")
     mode_group.add_argument(
         "--preprocess-text",
         action="store_true",
         help="Use TF-IDF preprocessing pipeline with raw text (legacy mode). "
-             "Requires -t/--test-file with text CSV. "
-             "Default is to use pre-extracted features."
+        "Requires -t/--test-file with text CSV. "
+        "Default is to use pre-extracted features.",
     )
 
     # Input data selection (features mode - default)
-    input_group = parser.add_argument_group("Input Data Selection (Features Mode - Default)")
+    input_group = parser.add_argument_group(
+        "Input Data Selection (Features Mode - Default)"
+    )
 
     # Simple use case: single feature directory
     input_group.add_argument(
-        "-d", "--feature-dir",
+        "-d",
+        "--feature-dir",
         help="[Features mode] Path to TF-IDF feature directory (e.g., features/norm-CELVA-SP/). "
-             "Must contain features_dense.csv. Script will look for matching labels CSV."
+        "Must contain features_dense.csv. Script will look for matching labels CSV.",
     )
 
     # Advanced: explicit file paths
     input_group.add_argument(
-        "-f", "--features-file",
-        help="[Features mode] Path to pre-extracted features CSV file (flat features, one row per sample)"
+        "-f",
+        "--features-file",
+        help="[Features mode] Path to pre-extracted features CSV file (flat features, one row per sample)",
     )
     input_group.add_argument(
         "--labels-file",
-        help="[Features mode] Path to labels file (one label per line, matching features row order)"
+        help="[Features mode] Path to labels file (one label per line, matching features row order)",
     )
     input_group.add_argument(
         "--labels-csv",
-        help="[Features mode] Path to CSV file containing labels in a column (use with --cefr-column)"
+        help="[Features mode] Path to CSV file containing labels in a column (use with --cefr-column)",
     )
 
     # Batch processing: multiple feature directories
     input_group.add_argument(
         "--batch-features-dir",
-        help="[Features mode] Directory containing multiple feature subdirectories (for batch processing all)"
+        help="[Features mode] Directory containing multiple feature subdirectories (for batch processing all)",
     )
     input_group.add_argument(
         "--labels-csv-dir",
-        help="[Features mode] Directory containing label CSV files (for batch processing, default: ml-test-data)"
+        help="[Features mode] Directory containing label CSV files (for batch processing, default: ml-test-data)",
     )
 
     # Text mode input
     text_group = parser.add_argument_group("Text Mode Input (with --preprocess-text)")
     text_group.add_argument(
-        "-t", "--test-file",
+        "-t",
+        "--test-file",
         help="[Text mode] Test CSV file with raw text (use with --preprocess-text). "
-             "File must contain text and label columns."
+        "File must contain text and label columns.",
     )
 
     # Data configuration
     data_group = parser.add_argument_group("Data Configuration")
     data_group.add_argument(
         "--text-column",
         default="text",
-        help="[Text mode] Column name containing text (default: text)"
+        help="[Text mode] Column name containing text (default: text)",
     )
     data_group.add_argument(
         "--label-column",
         default="label",
-        help="[Text mode] Column name containing labels (default: label)"
+        help="[Text mode] Column name containing labels (default: label)",
     )
     data_group.add_argument(
         "--cefr-column",
         default="cefr_label",
-        help="[Features mode] Column name containing CEFR labels (default: cefr_label)"
+        help="[Features mode] Column name containing CEFR labels (default: cefr_label)",
     )
 
     # Output configuration
     output_group = parser.add_argument_group("Output Configuration")
     output_group.add_argument(
-        "--no-save-results",
-        action="store_true",
-        help="Skip saving prediction results"
+        "--no-save-results", action="store_true", help="Skip saving prediction results"
     )
     output_group.add_argument(
-        "--no-save-csv",
-        action="store_true",
-        help="Skip saving CSV outputs"
+        "--no-save-csv", action="store_true", help="Skip saving CSV outputs"
     )
     output_group.add_argument(
-        "--no-save-json",
-        action="store_true",
-        help="Skip saving JSON outputs"
+        "--no-save-json", action="store_true", help="Skip saving JSON outputs"
     )
     output_group.add_argument(
-        "-q", "--quiet",
-        action="store_true",
-        help="Suppress verbose output"
+        "-q", "--quiet", action="store_true", help="Suppress verbose output"
     )
 
     return parser
 
 
 def args_to_config(args: argparse.Namespace) -> GlobalConfig:
     """Convert argparse namespace to GlobalConfig."""
     # Check if config file or json string provided
     if args.config_file:
         config_path = Path(args.config_file)
-        if config_path.suffix in ['.yaml', '.yml']:
+        if config_path.suffix in [".yaml", ".yml"]:
             config = GlobalConfig.from_yaml_file(str(config_path))
-        elif config_path.suffix == '.json':
+        elif config_path.suffix == ".json":
             config = GlobalConfig.from_json_file(str(config_path))
         else:
             raise ValueError(f"Unsupported config file format: {config_path.suffix}")
 
         # CLI args override config file
         if args.experiment_dir:
             config.experiment_config = ExperimentConfig(
                 experiment_dir=args.experiment_dir,
-                models_dir=args.output_dir if args.output_dir else None
+                models_dir=args.output_dir if args.output_dir else None,
             )
         elif args.output_dir:
             config.experiment_config.models_dir = args.output_dir
 
         if args.cefr_column != "cefr_label":
@@ -1302,30 +1380,28 @@
 
     else:
         # Build config from CLI args
         experiment_config = ExperimentConfig(
             experiment_dir=args.experiment_dir or "data/experiments/zero-shot",
-            models_dir=args.output_dir if args.output_dir else None
+            models_dir=args.output_dir if args.output_dir else None,
         )
 
         data_config = DataConfig(
             text_column=args.text_column,
             label_column=args.label_column,
-            cefr_column=args.cefr_column
+            cefr_column=args.cefr_column,
         )
 
         output_config = OutputConfig(
             save_results=not args.no_save_results,
             save_csv=not args.no_save_csv,
             save_json=not args.no_save_json,
-            verbose=not args.quiet
+            verbose=not args.quiet,
         )
 
         return GlobalConfig(
-            experiment_config,
-            data_config=data_config,
-            output_config=output_config
+            experiment_config, data_config=data_config, output_config=output_config
         )
 
 
 def main():
     """Main entry point for predictions."""
@@ -1357,11 +1433,11 @@
                 print()
 
             predict_with_text_pipeline(
                 config,
                 classifier_model_name=args.classifier_model,
-                test_file=args.test_file
+                test_file=args.test_file,
             )
 
         else:
             # Features mode (default): use pre-extracted features
             if config.output_config.verbose:
@@ -1382,11 +1458,13 @@
 
                 # Derive dataset name from directory name
                 dataset_name = feature_dir_path.name
 
                 # Look for corresponding label CSV in ml-test-data
-                labels_csv_path = Path(config.experiment_config.ml_test_dir) / f"{dataset_name}.csv"
+                labels_csv_path = (
+                    Path(config.experiment_config.ml_test_dir) / f"{dataset_name}.csv"
+                )
 
                 if not labels_csv_path.exists():
                     if config.output_config.verbose:
                         print(f"⚠ Label CSV not found: {labels_csv_path}")
                         print(f"  Making predictions without evaluation")
@@ -1400,34 +1478,36 @@
 
                 predict_on_features(
                     config,
                     classifier_model_name=args.classifier_model,
                     features_file=str(features_file),
-                    labels_csv=str(labels_csv_path) if labels_csv_path else None
+                    labels_csv=str(labels_csv_path) if labels_csv_path else None,
                 )
 
             elif args.features_file:
                 # Advanced: predict on specific features file
                 predict_on_features(
                     config,
                     classifier_model_name=args.classifier_model,
                     features_file=args.features_file,
                     labels_file=args.labels_file,
-                    labels_csv=args.labels_csv
+                    labels_csv=args.labels_csv,
                 )
 
             elif args.batch_features_dir:
                 # Batch: predict on all feature sets in directory
                 predict_all_feature_sets(
                     config,
                     classifier_model_name=args.classifier_model,
                     features_dir=args.batch_features_dir,
-                    labels_csv_dir=args.labels_csv_dir
+                    labels_csv_dir=args.labels_csv_dir,
                 )
 
             else:
-                parser.error("Must provide either -d/--feature-dir, -f/--features-file, or --batch-features-dir (or use --preprocess-text with -t/--test-file)")
+                parser.error(
+                    "Must provide either -d/--feature-dir, -f/--features-file, or --batch-features-dir (or use --preprocess-text with -t/--test-file)"
+                )
 
     except Exception as e:
         print(f"Error during prediction: {e}")
         raise
 
would reformat /home/b/p/cefr-classification/minimal-cefr/src/predict.py
--- /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho_multifeat.py	2025-10-18 10:46:52.712243+00:00
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho_multifeat.py	2025-10-18 11:24:41.952927+00:00
@@ -23,38 +23,44 @@
 from sklearn.model_selection import train_test_split
 from sklearn.utils.class_weight import compute_class_weight
 
 try:
     import xgboost as xgb
+
     XGBOOST_AVAILABLE = True
 except ImportError:
     XGBOOST_AVAILABLE = False
 
 try:
     import optuna
+
     OPTUNA_AVAILABLE = True
 except ImportError:
     OPTUNA_AVAILABLE = False
 
-from src.config import GlobalConfig, ExperimentConfig, ClassifierConfig, DataConfig, OutputConfig
+from src.config import (
+    GlobalConfig,
+    ExperimentConfig,
+    ClassifierConfig,
+    DataConfig,
+    OutputConfig,
+)
 
 
 # Fixed CEFR classes (all 6 levels)
-CEFR_CLASSES = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
+CEFR_CLASSES = ["A1", "A2", "B1", "B2", "C1", "C2"]
 
 
 def get_cefr_label_encoder() -> LabelEncoder:
     """Create a fixed label encoder for all 6 CEFR classes."""
     encoder = LabelEncoder()
     encoder.fit(CEFR_CLASSES)
     return encoder
 
 
 def compute_sample_weights(
-    y: np.ndarray,
-    strategy: str = 'equal',
-    alpha: float = 1.0
+    y: np.ndarray, strategy: str = "equal", alpha: float = 1.0
 ) -> np.ndarray:
     """
     Compute sample weights based on class distribution.
 
     Args:
@@ -69,21 +75,21 @@
     Returns:
         Array of sample weights
     """
     n_samples = len(y)
 
-    if strategy == 'equal':
+    if strategy == "equal":
         return np.ones(n_samples)
 
     # Compute class weights
     classes = np.unique(y)
     n_classes = len(classes)
 
-    if strategy == 'inverse':
+    if strategy == "inverse":
         # weight = n_samples / (n_classes * class_count)
-        class_weights = compute_class_weight('balanced', classes=classes, y=y)
-    elif strategy == 'inverse_sqrt':
+        class_weights = compute_class_weight("balanced", classes=classes, y=y)
+    elif strategy == "inverse_sqrt":
         # Custom: weight = sqrt(n_samples / class_count)
         class_counts = np.bincount(y)
         class_weights = np.sqrt(n_samples / (class_counts[classes] * n_classes))
     else:
         raise ValueError(f"Unknown weighting strategy: {strategy}")
@@ -102,11 +108,11 @@
 
 def compute_ordinal_accuracy(
     y_true: np.ndarray,
     y_pred: np.ndarray,
     sample_weight: Optional[np.ndarray] = None,
-    tolerance: int = 1
+    tolerance: int = 1,
 ) -> float:
     """
     Compute ordinal accuracy allowing for adjacent level errors.
 
     For CEFR classification: A1=0, A2=1, B1=2, B2=3, C1=4, C2=5
@@ -130,11 +136,11 @@
 
 def compute_ordinal_distance_score(
     y_true: np.ndarray,
     y_pred: np.ndarray,
     sample_weight: Optional[np.ndarray] = None,
-    penalty: str = 'quadratic'
+    penalty: str = "quadratic",
 ) -> float:
     """
     Compute ordinal distance-based score (higher is better).
 
     Penalizes predictions based on distance from true label.
@@ -152,29 +158,27 @@
         Mean ordinal distance score (0 to 1, higher is better)
     """
     distances = np.abs(y_true - y_pred)
     max_distance = len(CEFR_CLASSES) - 1  # 5 for A1-C2
 
-    if penalty == 'linear':
+    if penalty == "linear":
         # Linear penalty: 0 distance = 1.0, max distance = 0.0
         scores = 1.0 - (distances / max_distance)
-    elif penalty == 'quadratic':
+    elif penalty == "quadratic":
         # Quadratic penalty: penalizes far predictions more heavily
-        scores = 1.0 - (distances ** 2) / (max_distance ** 2)
+        scores = 1.0 - (distances**2) / (max_distance**2)
     else:
         raise ValueError(f"Unknown penalty: {penalty}")
 
     if sample_weight is not None:
         return np.average(scores, weights=sample_weight)
     else:
         return np.mean(scores)
 
 
 def compute_ordinal_mse_score(
-    y_true: np.ndarray,
-    y_pred: np.ndarray,
-    sample_weight: Optional[np.ndarray] = None
+    y_true: np.ndarray, y_pred: np.ndarray, sample_weight: Optional[np.ndarray] = None
 ) -> float:
     """
     Compute ordinal MSE-based score (higher is better).
 
     Treats labels as numerical indices and computes MSE, then converts to
@@ -220,12 +224,12 @@
 
 
 def compute_metric(
     y_true: np.ndarray,
     y_pred: np.ndarray,
-    metric: str = 'f1_macro',
-    sample_weight: Optional[np.ndarray] = None
+    metric: str = "f1_macro",
+    sample_weight: Optional[np.ndarray] = None,
 ) -> float:
     """
     Compute evaluation metric.
 
     Args:
@@ -245,38 +249,48 @@
         sample_weight: Sample weights (optional)
 
     Returns:
         Metric value
     """
-    if metric == 'accuracy':
+    if metric == "accuracy":
         return accuracy_score(y_true, y_pred, sample_weight=sample_weight)
 
-    elif metric == 'f1_macro':
-        return f1_score(y_true, y_pred, average='macro', sample_weight=sample_weight)
-
-    elif metric == 'f1_weighted':
-        return f1_score(y_true, y_pred, average='weighted', sample_weight=sample_weight)
-
-    elif metric == 'f1_micro':
-        return f1_score(y_true, y_pred, average='micro', sample_weight=sample_weight)
-
-    elif metric == 'precision_macro':
-        return precision_score(y_true, y_pred, average='macro', sample_weight=sample_weight)
-
-    elif metric == 'recall_macro':
-        return recall_score(y_true, y_pred, average='macro', sample_weight=sample_weight)
-
-    elif metric == 'ordinal_accuracy':
-        return compute_ordinal_accuracy(y_true, y_pred, sample_weight=sample_weight, tolerance=1)
-
-    elif metric == 'ordinal_distance_linear':
-        return compute_ordinal_distance_score(y_true, y_pred, sample_weight=sample_weight, penalty='linear')
-
-    elif metric == 'ordinal_distance_quadratic':
-        return compute_ordinal_distance_score(y_true, y_pred, sample_weight=sample_weight, penalty='quadratic')
-
-    elif metric == 'ordinal_mse':
+    elif metric == "f1_macro":
+        return f1_score(y_true, y_pred, average="macro", sample_weight=sample_weight)
+
+    elif metric == "f1_weighted":
+        return f1_score(y_true, y_pred, average="weighted", sample_weight=sample_weight)
+
+    elif metric == "f1_micro":
+        return f1_score(y_true, y_pred, average="micro", sample_weight=sample_weight)
+
+    elif metric == "precision_macro":
+        return precision_score(
+            y_true, y_pred, average="macro", sample_weight=sample_weight
+        )
+
+    elif metric == "recall_macro":
+        return recall_score(
+            y_true, y_pred, average="macro", sample_weight=sample_weight
+        )
+
+    elif metric == "ordinal_accuracy":
+        return compute_ordinal_accuracy(
+            y_true, y_pred, sample_weight=sample_weight, tolerance=1
+        )
+
+    elif metric == "ordinal_distance_linear":
+        return compute_ordinal_distance_score(
+            y_true, y_pred, sample_weight=sample_weight, penalty="linear"
+        )
+
+    elif metric == "ordinal_distance_quadratic":
+        return compute_ordinal_distance_score(
+            y_true, y_pred, sample_weight=sample_weight, penalty="quadratic"
+        )
+
+    elif metric == "ordinal_mse":
         return compute_ordinal_mse_score(y_true, y_pred, sample_weight=sample_weight)
 
     else:
         raise ValueError(f"Unknown metric: {metric}")
 
@@ -284,11 +298,11 @@
 def load_features_and_labels_from_dir(
     feature_dir: Path,
     cefr_column: str = "cefr_label",
     ml_training_dir: Optional[str] = None,
     ml_test_dir: Optional[str] = None,
-    verbose: bool = False
+    verbose: bool = False,
 ) -> Tuple[np.ndarray, np.ndarray, List[str], str]:
     """
     Load features and labels from a feature directory.
 
     Args:
@@ -312,11 +326,11 @@
     features_df = pd.read_csv(features_file)
     X = features_df.values
 
     # Load feature names
     if feature_names_file.exists():
-        with open(feature_names_file, 'r') as f:
+        with open(feature_names_file, "r") as f:
             feature_names = [line.strip() for line in f if line.strip()]
     else:
         feature_names = features_df.columns.tolist()
 
     # Find labels CSV
@@ -348,25 +362,27 @@
     # Encode labels
     label_encoder = get_cefr_label_encoder()
     y_encoded = label_encoder.transform(y)
 
     if verbose:
-        print(f"  Loaded {dataset_name}: {len(X)} samples, {len(feature_names)} features")
+        print(
+            f"  Loaded {dataset_name}: {len(X)} samples, {len(feature_names)} features"
+        )
 
     return X, y_encoded, feature_names, dataset_name
 
 
 def quick_evaluate_xgboost(
     X_train: np.ndarray,
     y_train: np.ndarray,
     X_val: np.ndarray,
     y_val: np.ndarray,
-    metric: str = 'f1_macro',
-    weight_strategy: str = 'equal',
+    metric: str = "f1_macro",
+    weight_strategy: str = "equal",
     weight_alpha: float = 1.0,
     use_gpu: bool = False,
-    random_state: int = 42
+    random_state: int = 42,
 ) -> float:
     """
     Quick evaluation with baseline XGBoost hyperparameters.
 
     Args:
@@ -383,30 +399,30 @@
     """
     # Compute sample weights
     train_weights = compute_sample_weights(y_train, weight_strategy, weight_alpha)
 
     params = {
-        'objective': 'multi:softmax',
-        'num_class': len(CEFR_CLASSES),
-        'eval_metric': 'mlogloss',
-        'tree_method': 'gpu_hist' if use_gpu else 'hist',
-        'device': 'cuda' if use_gpu else 'cpu',
-        'max_depth': 6,
-        'learning_rate': 0.1,
-        'n_estimators': 100,
-        'random_state': random_state
+        "objective": "multi:softmax",
+        "num_class": len(CEFR_CLASSES),
+        "eval_metric": "mlogloss",
+        "tree_method": "gpu_hist" if use_gpu else "hist",
+        "device": "cuda" if use_gpu else "cpu",
+        "max_depth": 6,
+        "learning_rate": 0.1,
+        "n_estimators": 100,
+        "random_state": random_state,
     }
 
     dtrain = xgb.DMatrix(X_train, label=y_train, weight=train_weights)
     dval = xgb.DMatrix(X_val, label=y_val)
 
     booster = xgb.train(
         params,
         dtrain,
-        num_boost_round=params['n_estimators'],
-        evals=[(dval, 'validation')],
-        verbose_eval=False
+        num_boost_round=params["n_estimators"],
+        evals=[(dval, "validation")],
+        verbose_eval=False,
     )
 
     y_pred = booster.predict(dval)
     val_weights = compute_sample_weights(y_val, weight_strategy, weight_alpha)
     return compute_metric(y_val, y_pred, metric, sample_weight=val_weights)
@@ -415,14 +431,14 @@
 def quick_evaluate_logistic(
     X_train: np.ndarray,
     y_train: np.ndarray,
     X_val: np.ndarray,
     y_val: np.ndarray,
-    metric: str = 'f1_macro',
-    weight_strategy: str = 'equal',
+    metric: str = "f1_macro",
+    weight_strategy: str = "equal",
     weight_alpha: float = 1.0,
-    random_state: int = 42
+    random_state: int = 42,
 ) -> float:
     """
     Quick evaluation with baseline Logistic Regression hyperparameters.
 
     Args:
@@ -438,14 +454,11 @@
     """
     # Compute sample weights
     train_weights = compute_sample_weights(y_train, weight_strategy, weight_alpha)
 
     clf = LogisticRegression(
-        C=1.0,
-        max_iter=1000,
-        random_state=random_state,
-        class_weight='balanced'
+        C=1.0, max_iter=1000, random_state=random_state, class_weight="balanced"
     )
     clf.fit(X_train, y_train, sample_weight=train_weights)
     y_pred = clf.predict(X_val)
 
     val_weights = compute_sample_weights(y_val, weight_strategy, weight_alpha)
@@ -455,16 +468,16 @@
 def stage1_screen_features(
     feature_dirs: List[Path],
     classifier_type: str,
     config: GlobalConfig,
     val_split: float = 0.2,
-    metric: str = 'f1_macro',
-    weight_strategy: str = 'equal',
+    metric: str = "f1_macro",
+    weight_strategy: str = "equal",
     weight_alpha: float = 1.0,
     early_discard_threshold: Optional[float] = None,
     early_discard_percentile: Optional[float] = None,
-    verbose: bool = True
+    verbose: bool = True,
 ) -> List[Dict[str, Any]]:
     """
     Stage 1: Screen all feature configurations with baseline hyperparameters.
 
     Args:
@@ -481,13 +494,13 @@
 
     Returns:
         List of dicts with feature config evaluation results, sorted by metric (descending)
     """
     if verbose:
-        print("\n" + "="*80)
+        print("\n" + "=" * 80)
         print("STAGE 1: Feature Configuration Screening")
-        print("="*80)
+        print("=" * 80)
         print(f"Evaluating {len(feature_dirs)} feature configurations...")
         print(f"Classifier: {classifier_type}")
         print(f"Metric: {metric}")
         print(f"Weight strategy: {weight_strategy} (alpha={weight_alpha})")
         print(f"Validation split: {val_split}")
@@ -501,120 +514,145 @@
 
     for i, feature_dir in enumerate(feature_dirs, 1):
         feature_config_name = feature_dir.parent.name  # e.g., "252cd532_tfidf"
 
         if verbose:
-            print(f"[{i}/{len(feature_dirs)}] Evaluating {feature_config_name}/{feature_dir.name}...", end=" ")
+            print(
+                f"[{i}/{len(feature_dirs)}] Evaluating {feature_config_name}/{feature_dir.name}...",
+                end=" ",
+            )
 
         try:
             # Load data
-            X, y_encoded, feature_names, dataset_name = load_features_and_labels_from_dir(
-                feature_dir,
-                cefr_column=config.data_config.cefr_column,
-                ml_training_dir=config.experiment_config.ml_training_dir,
-                ml_test_dir=config.experiment_config.ml_test_dir,
-                verbose=False
+            X, y_encoded, feature_names, dataset_name = (
+                load_features_and_labels_from_dir(
+                    feature_dir,
+                    cefr_column=config.data_config.cefr_column,
+                    ml_training_dir=config.experiment_config.ml_training_dir,
+                    ml_test_dir=config.experiment_config.ml_test_dir,
+                    verbose=False,
+                )
             )
 
             # Split train/val
             X_train, X_val, y_train, y_val = train_test_split(
-                X, y_encoded, test_size=val_split,
+                X,
+                y_encoded,
+                test_size=val_split,
                 random_state=config.classifier_config.random_state,
-                stratify=y_encoded
+                stratify=y_encoded,
             )
 
             # Quick evaluation
-            if classifier_type == 'xgboost':
+            if classifier_type == "xgboost":
                 score = quick_evaluate_xgboost(
-                    X_train, y_train, X_val, y_val,
+                    X_train,
+                    y_train,
+                    X_val,
+                    y_val,
                     metric=metric,
                     weight_strategy=weight_strategy,
                     weight_alpha=weight_alpha,
                     use_gpu=config.classifier_config.xgb_use_gpu,
-                    random_state=config.classifier_config.random_state
+                    random_state=config.classifier_config.random_state,
                 )
-            elif classifier_type == 'logistic':
+            elif classifier_type == "logistic":
                 score = quick_evaluate_logistic(
-                    X_train, y_train, X_val, y_val,
+                    X_train,
+                    y_train,
+                    X_val,
+                    y_val,
                     metric=metric,
                     weight_strategy=weight_strategy,
                     weight_alpha=weight_alpha,
-                    random_state=config.classifier_config.random_state
+                    random_state=config.classifier_config.random_state,
                 )
             else:
                 raise ValueError(f"Unsupported classifier: {classifier_type}")
 
             result = {
-                'feature_config': feature_config_name,
-                'dataset': dataset_name,
-                'feature_dir': str(feature_dir),
-                'n_features': len(feature_names),
-                'n_samples': len(X),
-                'metric': metric,
-                'score': score,
-                'passed_screening': True  # May be updated below
+                "feature_config": feature_config_name,
+                "dataset": dataset_name,
+                "feature_dir": str(feature_dir),
+                "n_features": len(feature_names),
+                "n_samples": len(X),
+                "metric": metric,
+                "score": score,
+                "passed_screening": True,  # May be updated below
             }
             results.append(result)
 
             if verbose:
                 print(f"{metric}: {score:.4f}")
 
         except Exception as e:
             if verbose:
                 print(f"FAILED: {e}")
-            results.append({
-                'feature_config': feature_config_name,
-                'dataset': dataset_name,
-                'feature_dir': str(feature_dir),
-                'metric': metric,
-                'score': 0.0,
-                'error': str(e),
-                'passed_screening': False
-            })
+            results.append(
+                {
+                    "feature_config": feature_config_name,
+                    "dataset": dataset_name,
+                    "feature_dir": str(feature_dir),
+                    "metric": metric,
+                    "score": 0.0,
+                    "error": str(e),
+                    "passed_screening": False,
+                }
+            )
 
     # Sort by score (descending)
-    results.sort(key=lambda x: x['score'], reverse=True)
+    results.sort(key=lambda x: x["score"], reverse=True)
 
     # Apply early discard filters
     if early_discard_threshold is not None:
         for result in results:
-            if result['score'] < early_discard_threshold:
-                result['passed_screening'] = False
-                result['discard_reason'] = f"{metric} {result['score']:.4f} < threshold {early_discard_threshold:.4f}"
+            if result["score"] < early_discard_threshold:
+                result["passed_screening"] = False
+                result["discard_reason"] = (
+                    f"{metric} {result['score']:.4f} < threshold {early_discard_threshold:.4f}"
+                )
 
     if early_discard_percentile is not None:
         n_discard = int(len(results) * early_discard_percentile / 100)
         for i in range(len(results) - n_discard, len(results)):
-            if results[i]['passed_screening']:
-                results[i]['passed_screening'] = False
-                results[i]['discard_reason'] = f"Bottom {early_discard_percentile}% percentile"
+            if results[i]["passed_screening"]:
+                results[i]["passed_screening"] = False
+                results[i][
+                    "discard_reason"
+                ] = f"Bottom {early_discard_percentile}% percentile"
 
     # Summary
-    passed = [r for r in results if r['passed_screening']]
-    failed = [r for r in results if not r['passed_screening']]
+    passed = [r for r in results if r["passed_screening"]]
+    failed = [r for r in results if not r["passed_screening"]]
 
     if verbose:
-        print("\n" + "-"*80)
+        print("\n" + "-" * 80)
         print("Stage 1 Results:")
-        print("-"*80)
+        print("-" * 80)
         print(f"Total evaluated: {len(results)}")
         print(f"Passed screening: {len(passed)}")
         print(f"Discarded: {len(failed)}")
         print()
 
         print("Top 10 feature configurations:")
         for i, result in enumerate(results[:10], 1):
-            status = "✓" if result['passed_screening'] else "✗"
-            print(f"  {i:2d}. [{status}] {result['feature_config']:25s} "
-                  f"{metric}={result['score']:.4f} ({result.get('n_features', 0):5d} features)")
+            status = "✓" if result["passed_screening"] else "✗"
+            print(
+                f"  {i:2d}. [{status}] {result['feature_config']:25s} "
+                f"{metric}={result['score']:.4f} ({result.get('n_features', 0):5d} features)"
+            )
 
         if failed:
             print()
             print(f"Discarded configurations ({len(failed)}):")
             for result in failed:
-                reason = result.get('discard_reason', result.get('error', 'Failed screening'))
-                print(f"  ✗ {result['feature_config']:25s} {metric}={result['score']:.4f} - {reason}")
+                reason = result.get(
+                    "discard_reason", result.get("error", "Failed screening")
+                )
+                print(
+                    f"  ✗ {result['feature_config']:25s} {metric}={result['score']:.4f} - {reason}"
+                )
         print()
 
     return results
 
 
@@ -623,14 +661,14 @@
     top_k: int,
     classifier_type: str,
     config: GlobalConfig,
     n_trials_per_config: int = 50,
     val_split: float = 0.2,
-    metric: str = 'f1_macro',
-    weight_strategy: str = 'equal',
+    metric: str = "f1_macro",
+    weight_strategy: str = "equal",
     weight_alpha: float = 1.0,
-    verbose: bool = True
+    verbose: bool = True,
 ) -> List[Dict[str, Any]]:
     """
     Stage 2: Deep hyperparameter optimization on top-K feature configurations.
 
     Args:
@@ -647,128 +685,148 @@
 
     Returns:
         List of optimization results with best hyperparameters
     """
     if verbose:
-        print("\n" + "="*80)
+        print("\n" + "=" * 80)
         print("STAGE 2: Deep Hyperparameter Optimization")
-        print("="*80)
+        print("=" * 80)
         print(f"Optimizing top {top_k} feature configurations")
         print(f"Trials per configuration: {n_trials_per_config}")
         print(f"Metric: {metric}")
         print(f"Weight strategy: {weight_strategy} (alpha={weight_alpha})")
         print()
 
     # Get top-k that passed screening
-    passed_results = [r for r in stage1_results if r['passed_screening']]
+    passed_results = [r for r in stage1_results if r["passed_screening"]]
     top_configs = passed_results[:top_k]
 
     if len(top_configs) < top_k:
-        print(f"Warning: Only {len(top_configs)} configs passed screening (requested top-{top_k})")
+        print(
+            f"Warning: Only {len(top_configs)} configs passed screening (requested top-{top_k})"
+        )
 
     optimization_results = []
 
     for i, stage1_result in enumerate(top_configs, 1):
-        feature_dir = Path(stage1_result['feature_dir'])
-        feature_config = stage1_result['feature_config']
+        feature_dir = Path(stage1_result["feature_dir"])
+        feature_config = stage1_result["feature_config"]
 
         if verbose:
             print(f"\n[{i}/{len(top_configs)}] Optimizing: {feature_config}")
             print(f"  Stage 1 {metric}: {stage1_result['score']:.4f}")
             print(f"  Features: {stage1_result.get('n_features', 'N/A')}")
             print(f"  Running {n_trials_per_config} trials...")
 
         try:
             # Load data
-            X, y_encoded, feature_names, dataset_name = load_features_and_labels_from_dir(
-                feature_dir,
-                cefr_column=config.data_config.cefr_column,
-                ml_training_dir=config.experiment_config.ml_training_dir,
-                ml_test_dir=config.experiment_config.ml_test_dir,
-                verbose=False
+            X, y_encoded, feature_names, dataset_name = (
+                load_features_and_labels_from_dir(
+                    feature_dir,
+                    cefr_column=config.data_config.cefr_column,
+                    ml_training_dir=config.experiment_config.ml_training_dir,
+                    ml_test_dir=config.experiment_config.ml_test_dir,
+                    verbose=False,
+                )
             )
 
             # Split train/val
             X_train, X_val, y_train, y_val = train_test_split(
-                X, y_encoded, test_size=val_split,
+                X,
+                y_encoded,
+                test_size=val_split,
                 random_state=config.classifier_config.random_state,
-                stratify=y_encoded
+                stratify=y_encoded,
             )
 
             # Run Optuna optimization
-            if classifier_type == 'xgboost':
+            if classifier_type == "xgboost":
                 best_params, tuning_history = optimize_xgboost(
-                    X_train, y_train, X_val, y_val,
+                    X_train,
+                    y_train,
+                    X_val,
+                    y_val,
                     n_trials=n_trials_per_config,
                     metric=metric,
                     weight_strategy=weight_strategy,
                     weight_alpha=weight_alpha,
                     use_gpu=config.classifier_config.xgb_use_gpu,
                     random_state=config.classifier_config.random_state,
-                    verbose=verbose
+                    verbose=verbose,
                 )
-            elif classifier_type == 'logistic':
+            elif classifier_type == "logistic":
                 best_params, tuning_history = optimize_logistic(
-                    X_train, y_train, X_val, y_val,
+                    X_train,
+                    y_train,
+                    X_val,
+                    y_val,
                     n_trials=n_trials_per_config,
                     metric=metric,
                     weight_strategy=weight_strategy,
                     weight_alpha=weight_alpha,
                     random_state=config.classifier_config.random_state,
-                    verbose=verbose
+                    verbose=verbose,
                 )
             else:
                 raise ValueError(f"Unsupported classifier: {classifier_type}")
 
             # Get best score from tuning history
-            best_score = max(trial['score'] for trial in tuning_history)
+            best_score = max(trial["score"] for trial in tuning_history)
 
             result = {
-                'feature_config': feature_config,
-                'dataset': dataset_name,
-                'feature_dir': str(feature_dir),
-                'n_features': len(feature_names),
-                'metric': metric,
-                'stage1_score': stage1_result['score'],
-                'stage2_best_score': best_score,
-                'improvement': best_score - stage1_result['score'],
-                'best_params': best_params,
-                'tuning_history': tuning_history,
-                'n_trials': len(tuning_history)
+                "feature_config": feature_config,
+                "dataset": dataset_name,
+                "feature_dir": str(feature_dir),
+                "n_features": len(feature_names),
+                "metric": metric,
+                "stage1_score": stage1_result["score"],
+                "stage2_best_score": best_score,
+                "improvement": best_score - stage1_result["score"],
+                "best_params": best_params,
+                "tuning_history": tuning_history,
+                "n_trials": len(tuning_history),
             }
             optimization_results.append(result)
 
             if verbose:
-                print(f"  ✓ Best {metric}: {best_score:.4f} "
-                      f"(+{result['improvement']:.4f} improvement)")
+                print(
+                    f"  ✓ Best {metric}: {best_score:.4f} "
+                    f"(+{result['improvement']:.4f} improvement)"
+                )
 
         except Exception as e:
             if verbose:
                 print(f"  ✗ FAILED: {e}")
-            optimization_results.append({
-                'feature_config': feature_config,
-                'dataset': dataset_name,
-                'metric': metric,
-                'error': str(e),
-                'stage1_score': stage1_result['score']
-            })
+            optimization_results.append(
+                {
+                    "feature_config": feature_config,
+                    "dataset": dataset_name,
+                    "metric": metric,
+                    "error": str(e),
+                    "stage1_score": stage1_result["score"],
+                }
+            )
 
     # Sort by stage2 score
-    optimization_results.sort(key=lambda x: x.get('stage2_best_score', 0), reverse=True)
+    optimization_results.sort(key=lambda x: x.get("stage2_best_score", 0), reverse=True)
 
     if verbose:
-        print("\n" + "="*80)
+        print("\n" + "=" * 80)
         print("STAGE 2 COMPLETE - Final Rankings:")
-        print("="*80)
-        print(f"{'Rank':<6} {'Feature Config':<25} {'Stage1':<10} {'Stage2':<10} {'Improve':<10}")
-        print("-"*80)
+        print("=" * 80)
+        print(
+            f"{'Rank':<6} {'Feature Config':<25} {'Stage1':<10} {'Stage2':<10} {'Improve':<10}"
+        )
+        print("-" * 80)
         for rank, result in enumerate(optimization_results, 1):
-            if 'stage2_best_score' in result:
-                print(f"{rank:<6} {result['feature_config']:<25} "
-                      f"{result['stage1_score']:<10.4f} "
-                      f"{result['stage2_best_score']:<10.4f} "
-                      f"+{result['improvement']:<9.4f}")
+            if "stage2_best_score" in result:
+                print(
+                    f"{rank:<6} {result['feature_config']:<25} "
+                    f"{result['stage1_score']:<10.4f} "
+                    f"{result['stage2_best_score']:<10.4f} "
+                    f"+{result['improvement']:<9.4f}"
+                )
         print()
 
     return optimization_results
 
 
@@ -776,16 +834,16 @@
     X_train: np.ndarray,
     y_train: np.ndarray,
     X_val: np.ndarray,
     y_val: np.ndarray,
     n_trials: int = 50,
-    metric: str = 'f1_macro',
-    weight_strategy: str = 'equal',
+    metric: str = "f1_macro",
+    weight_strategy: str = "equal",
     weight_alpha: float = 1.0,
     use_gpu: bool = False,
     random_state: int = 42,
-    verbose: bool = True
+    verbose: bool = True,
 ) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
     """Optimize XGBoost hyperparameters using Optuna."""
     # Compute sample weights
     train_weights = compute_sample_weights(y_train, weight_strategy, weight_alpha)
     val_weights = compute_sample_weights(y_val, weight_strategy, weight_alpha)
@@ -795,193 +853,214 @@
 
     tuning_history = []
 
     def objective(trial):
         params = {
-            'objective': 'multi:softmax',
-            'num_class': len(CEFR_CLASSES),
-            'eval_metric': 'mlogloss',
-            'tree_method': 'gpu_hist' if use_gpu else 'hist',
-            'device': 'cuda' if use_gpu else 'cpu',
-            'random_state': random_state,
-            'max_depth': trial.suggest_int('max_depth', 3, 10),
-            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
-            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
-            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
-            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
-            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
-            'gamma': trial.suggest_float('gamma', 0.0, 5.0),
-            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 5.0),
-            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 5.0),
+            "objective": "multi:softmax",
+            "num_class": len(CEFR_CLASSES),
+            "eval_metric": "mlogloss",
+            "tree_method": "gpu_hist" if use_gpu else "hist",
+            "device": "cuda" if use_gpu else "cpu",
+            "random_state": random_state,
+            "max_depth": trial.suggest_int("max_depth", 3, 10),
+            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
+            "n_estimators": trial.suggest_int("n_estimators", 50, 300),
+            "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
+            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
+            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
+            "gamma": trial.suggest_float("gamma", 0.0, 5.0),
+            "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 5.0),
+            "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 5.0),
         }
 
         booster = xgb.train(
-            params, dtrain,
-            num_boost_round=params['n_estimators'],
-            evals=[(dval, 'validation')],
-            verbose_eval=False
+            params,
+            dtrain,
+            num_boost_round=params["n_estimators"],
+            evals=[(dval, "validation")],
+            verbose_eval=False,
         )
 
         y_pred = booster.predict(dval)
         score = compute_metric(y_val, y_pred, metric, sample_weight=val_weights)
 
-        tuning_history.append({
-            'trial_number': trial.number,
-            'params': params.copy(),
-            'metric': metric,
-            'score': score
-        })
+        tuning_history.append(
+            {
+                "trial_number": trial.number,
+                "params": params.copy(),
+                "metric": metric,
+                "score": score,
+            }
+        )
 
         return score
 
-    study = optuna.create_study(direction='maximize')
-    optuna.logging.set_verbosity(optuna.logging.WARNING if not verbose else optuna.logging.INFO)
+    study = optuna.create_study(direction="maximize")
+    optuna.logging.set_verbosity(
+        optuna.logging.WARNING if not verbose else optuna.logging.INFO
+    )
     study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
 
     best_params = study.best_params
-    best_params.update({
-        'objective': 'multi:softmax',
-        'num_class': len(CEFR_CLASSES),
-        'eval_metric': 'mlogloss',
-        'tree_method': 'gpu_hist' if use_gpu else 'hist',
-        'device': 'cuda' if use_gpu else 'cpu',
-        'random_state': random_state
-    })
+    best_params.update(
+        {
+            "objective": "multi:softmax",
+            "num_class": len(CEFR_CLASSES),
+            "eval_metric": "mlogloss",
+            "tree_method": "gpu_hist" if use_gpu else "hist",
+            "device": "cuda" if use_gpu else "cpu",
+            "random_state": random_state,
+        }
+    )
 
     return best_params, tuning_history
 
 
 def optimize_logistic(
     X_train: np.ndarray,
     y_train: np.ndarray,
     X_val: np.ndarray,
     y_val: np.ndarray,
     n_trials: int = 30,
-    metric: str = 'f1_macro',
-    weight_strategy: str = 'equal',
+    metric: str = "f1_macro",
+    weight_strategy: str = "equal",
     weight_alpha: float = 1.0,
     random_state: int = 42,
-    verbose: bool = True
+    verbose: bool = True,
 ) -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:
     """Optimize Logistic Regression hyperparameters using Optuna."""
     # Compute sample weights
     train_weights = compute_sample_weights(y_train, weight_strategy, weight_alpha)
     val_weights = compute_sample_weights(y_val, weight_strategy, weight_alpha)
 
     tuning_history = []
 
     def objective(trial):
         params = {
-            'C': trial.suggest_float('C', 0.001, 100.0, log=True),
-            'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet']),
-            'solver': 'saga',
-            'max_iter': trial.suggest_int('max_iter', 500, 3000),
-            'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),
-            'random_state': random_state
+            "C": trial.suggest_float("C", 0.001, 100.0, log=True),
+            "penalty": trial.suggest_categorical("penalty", ["l1", "l2", "elasticnet"]),
+            "solver": "saga",
+            "max_iter": trial.suggest_int("max_iter", 500, 3000),
+            "class_weight": trial.suggest_categorical(
+                "class_weight", ["balanced", None]
+            ),
+            "random_state": random_state,
         }
 
-        if params['penalty'] == 'elasticnet':
-            params['l1_ratio'] = trial.suggest_float('l1_ratio', 0.0, 1.0)
+        if params["penalty"] == "elasticnet":
+            params["l1_ratio"] = trial.suggest_float("l1_ratio", 0.0, 1.0)
 
         try:
             clf = LogisticRegression(**params)
             clf.fit(X_train, y_train, sample_weight=train_weights)
             y_pred = clf.predict(X_val)
             score = compute_metric(y_val, y_pred, metric, sample_weight=val_weights)
 
-            tuning_history.append({
-                'trial_number': trial.number,
-                'params': params.copy(),
-                'metric': metric,
-                'score': score
-            })
+            tuning_history.append(
+                {
+                    "trial_number": trial.number,
+                    "params": params.copy(),
+                    "metric": metric,
+                    "score": score,
+                }
+            )
 
             return score
         except:
             return 0.0
 
-    study = optuna.create_study(direction='maximize')
-    optuna.logging.set_verbosity(optuna.logging.WARNING if not verbose else optuna.logging.INFO)
+    study = optuna.create_study(direction="maximize")
+    optuna.logging.set_verbosity(
+        optuna.logging.WARNING if not verbose else optuna.logging.INFO
+    )
     study.optimize(objective, n_trials=n_trials, show_progress_bar=False)
 
     return study.best_params, tuning_history
 
 
 def save_results(
     stage1_results: List[Dict[str, Any]],
     stage2_results: List[Dict[str, Any]],
     output_dir: Path,
     classifier_type: str,
-    verbose: bool = True
+    verbose: bool = True,
 ):
     """Save two-stage optimization results."""
     output_dir.mkdir(parents=True, exist_ok=True)
 
     # Save stage 1 results
     stage1_file = output_dir / "stage1_screening_results.json"
-    with open(stage1_file, 'w') as f:
+    with open(stage1_file, "w") as f:
         json.dump(stage1_results, f, indent=2)
 
     # Save stage 2 results (without tuning_history to keep file smaller)
     stage2_summary = []
     for result in stage2_results:
-        summary = {k: v for k, v in result.items() if k != 'tuning_history'}
+        summary = {k: v for k, v in result.items() if k != "tuning_history"}
         stage2_summary.append(summary)
 
     stage2_file = output_dir / "stage2_optimization_results.json"
-    with open(stage2_file, 'w') as f:
+    with open(stage2_file, "w") as f:
         json.dump(stage2_summary, f, indent=2)
 
     # Save detailed results with tuning history
     stage2_detailed_file = output_dir / "stage2_detailed_results.json"
-    with open(stage2_detailed_file, 'w') as f:
+    with open(stage2_detailed_file, "w") as f:
         json.dump(stage2_results, f, indent=2)
 
     # Create summary report
     report_file = output_dir / "optimization_summary.md"
-    with open(report_file, 'w') as f:
+    with open(report_file, "w") as f:
         f.write("# Two-Stage Hyperparameter Optimization Summary\n\n")
         f.write(f"**Classifier:** {classifier_type}\n\n")
 
         f.write("## Stage 1: Feature Configuration Screening\n\n")
-        passed = [r for r in stage1_results if r['passed_screening']]
+        passed = [r for r in stage1_results if r["passed_screening"]]
         f.write(f"- Total configurations evaluated: {len(stage1_results)}\n")
         f.write(f"- Passed screening: {len(passed)}\n")
         f.write(f"- Discarded: {len(stage1_results) - len(passed)}\n\n")
 
         f.write("### Top 10 Configurations (Stage 1)\n\n")
         f.write("| Rank | Feature Config | Score | Features | Status |\n")
         f.write("|------|---------------|-------|----------|--------|\n")
         for i, result in enumerate(stage1_results[:10], 1):
-            status = "✓ Passed" if result['passed_screening'] else "✗ Discarded"
-            metric_name = result.get('metric', 'score')
-            f.write(f"| {i} | {result['feature_config']} | {result['score']:.4f} | "
-                   f"{result.get('n_features', 'N/A')} | {status} |\n")
+            status = "✓ Passed" if result["passed_screening"] else "✗ Discarded"
+            metric_name = result.get("metric", "score")
+            f.write(
+                f"| {i} | {result['feature_config']} | {result['score']:.4f} | "
+                f"{result.get('n_features', 'N/A')} | {status} |\n"
+            )
 
         f.write("\n## Stage 2: Deep Hyperparameter Optimization\n\n")
         f.write(f"- Configurations optimized: {len(stage2_results)}\n\n")
 
         f.write("### Final Rankings\n\n")
-        metric_label = stage2_results[0].get('metric', 'Score') if stage2_results else 'Score'
-        f.write(f"| Rank | Feature Config | Stage 1 {metric_label} | Stage 2 {metric_label} | Improvement |\n")
+        metric_label = (
+            stage2_results[0].get("metric", "Score") if stage2_results else "Score"
+        )
+        f.write(
+            f"| Rank | Feature Config | Stage 1 {metric_label} | Stage 2 {metric_label} | Improvement |\n"
+        )
         f.write("|------|---------------|-------------|-------------|-------------|\n")
         for i, result in enumerate(stage2_results, 1):
-            if 'stage2_best_score' in result:
-                f.write(f"| {i} | {result['feature_config']} | "
-                       f"{result['stage1_score']:.4f} | "
-                       f"{result['stage2_best_score']:.4f} | "
-                       f"+{result['improvement']:.4f} |\n")
-
-        if stage2_results and 'best_params' in stage2_results[0]:
+            if "stage2_best_score" in result:
+                f.write(
+                    f"| {i} | {result['feature_config']} | "
+                    f"{result['stage1_score']:.4f} | "
+                    f"{result['stage2_best_score']:.4f} | "
+                    f"+{result['improvement']:.4f} |\n"
+                )
+
+        if stage2_results and "best_params" in stage2_results[0]:
             f.write("\n### Best Configuration\n\n")
             best = stage2_results[0]
             f.write(f"**Feature Configuration:** {best['feature_config']}\n\n")
-            metric_label = best.get('metric', 'Score')
+            metric_label = best.get("metric", "Score")
             f.write(f"**Best {metric_label}:** {best['stage2_best_score']:.4f}\n\n")
             f.write("**Best Hyperparameters:**\n\n")
             f.write("```json\n")
-            f.write(json.dumps(best['best_params'], indent=2))
+            f.write(json.dumps(best["best_params"], indent=2))
             f.write("\n```\n")
 
     if verbose:
         print(f"\n✓ Results saved to: {output_dir}")
         print(f"  - {stage1_file.name}")
@@ -1014,88 +1093,150 @@
   python -m src.train_classifiers_with_ho_multifeat \\
       -e data/experiments/zero-shot \\
       --features-base-dir data/experiments/zero-shot/features \\
       --early-discard-percentile 30 \\
       --top-k 5
-        """
+        """,
     )
 
     # Configuration
-    parser.add_argument('-e', '--experiment-dir', type=str, required=True,
-                       help='Experiment directory path')
+    parser.add_argument(
+        "-e",
+        "--experiment-dir",
+        type=str,
+        required=True,
+        help="Experiment directory path",
+    )
 
     # Feature configurations
-    parser.add_argument('--features-base-dir', type=str, required=True,
-                       help='Base directory containing feature configuration folders')
-    parser.add_argument('--feature-pattern', type=str, default='*_tfidf*',
-                       help='Glob pattern to match feature directories (default: *_tfidf*)')
+    parser.add_argument(
+        "--features-base-dir",
+        type=str,
+        required=True,
+        help="Base directory containing feature configuration folders",
+    )
+    parser.add_argument(
+        "--feature-pattern",
+        type=str,
+        default="*_tfidf*",
+        help="Glob pattern to match feature directories (default: *_tfidf*)",
+    )
 
     # Stage 1 settings
-    parser.add_argument('--early-discard-threshold', type=float,
-                       help='Discard configs with accuracy below this threshold (e.g., 0.40 = 40%%)')
-    parser.add_argument('--early-discard-percentile', type=float,
-                       help='Discard bottom N%% of configs (e.g., 25 = bottom 25%%)')
+    parser.add_argument(
+        "--early-discard-threshold",
+        type=float,
+        help="Discard configs with accuracy below this threshold (e.g., 0.40 = 40%%)",
+    )
+    parser.add_argument(
+        "--early-discard-percentile",
+        type=float,
+        help="Discard bottom N%% of configs (e.g., 25 = bottom 25%%)",
+    )
 
     # Stage 2 settings
-    parser.add_argument('--top-k', type=int, default=5,
-                       help='Number of top configs to deeply optimize in stage 2 (default: 5)')
-    parser.add_argument('--stage2-trials', type=int, default=50,
-                       help='Number of Optuna trials per config in stage 2 (default: 50)')
+    parser.add_argument(
+        "--top-k",
+        type=int,
+        default=5,
+        help="Number of top configs to deeply optimize in stage 2 (default: 5)",
+    )
+    parser.add_argument(
+        "--stage2-trials",
+        type=int,
+        default=50,
+        help="Number of Optuna trials per config in stage 2 (default: 50)",
+    )
 
     # Optimization goal
-    parser.add_argument('--metric', type=str,
-                       choices=['accuracy', 'f1_macro', 'f1_weighted', 'f1_micro',
-                               'precision_macro', 'recall_macro',
-                               'ordinal_accuracy', 'ordinal_distance_linear',
-                               'ordinal_distance_quadratic', 'ordinal_mse'],
-                       default='f1_macro',
-                       help='Optimization metric (default: f1_macro). '
-                            'Ordinal metrics account for CEFR level ordering: '
-                            'ordinal_accuracy allows ±1 level error, '
-                            'ordinal_distance_linear uses linear penalty, '
-                            'ordinal_distance_quadratic penalizes far predictions more heavily, '
-                            'ordinal_mse treats labels as numerical and uses MSE')
+    parser.add_argument(
+        "--metric",
+        type=str,
+        choices=[
+            "accuracy",
+            "f1_macro",
+            "f1_weighted",
+            "f1_micro",
+            "precision_macro",
+            "recall_macro",
+            "ordinal_accuracy",
+            "ordinal_distance_linear",
+            "ordinal_distance_quadratic",
+            "ordinal_mse",
+        ],
+        default="f1_macro",
+        help="Optimization metric (default: f1_macro). "
+        "Ordinal metrics account for CEFR level ordering: "
+        "ordinal_accuracy allows ±1 level error, "
+        "ordinal_distance_linear uses linear penalty, "
+        "ordinal_distance_quadratic penalizes far predictions more heavily, "
+        "ordinal_mse treats labels as numerical and uses MSE",
+    )
 
     # Sample weighting strategy
-    parser.add_argument('--weight-strategy', type=str,
-                       choices=['equal', 'inverse', 'inverse_sqrt'],
-                       default='equal',
-                       help='Sample weighting strategy (default: equal). '
-                            'inverse = weight inversely to class size, '
-                            'inverse_sqrt = weight inversely to sqrt of class size')
-    parser.add_argument('--weight-alpha', type=float, default=1.0,
-                       help='Weight calibration parameter (0=equal weights, 1=full inverse weighting). '
-                            'Default: 1.0')
+    parser.add_argument(
+        "--weight-strategy",
+        type=str,
+        choices=["equal", "inverse", "inverse_sqrt"],
+        default="equal",
+        help="Sample weighting strategy (default: equal). "
+        "inverse = weight inversely to class size, "
+        "inverse_sqrt = weight inversely to sqrt of class size",
+    )
+    parser.add_argument(
+        "--weight-alpha",
+        type=float,
+        default=1.0,
+        help="Weight calibration parameter (0=equal weights, 1=full inverse weighting). "
+        "Default: 1.0",
+    )
 
     # Common settings
-    parser.add_argument('--classifier', type=str, choices=['xgboost', 'logistic'],
-                       default='xgboost', help='Classifier type (default: xgboost)')
-    parser.add_argument('--val-split', type=float, default=0.2,
-                       help='Validation split ratio (default: 0.2)')
-    parser.add_argument('--random-state', type=int, default=42,
-                       help='Random state for reproducibility')
-    parser.add_argument('--xgb-use-gpu', action='store_true',
-                       help='Use GPU for XGBoost')
+    parser.add_argument(
+        "--classifier",
+        type=str,
+        choices=["xgboost", "logistic"],
+        default="xgboost",
+        help="Classifier type (default: xgboost)",
+    )
+    parser.add_argument(
+        "--val-split",
+        type=float,
+        default=0.2,
+        help="Validation split ratio (default: 0.2)",
+    )
+    parser.add_argument(
+        "--random-state", type=int, default=42, help="Random state for reproducibility"
+    )
+    parser.add_argument(
+        "--xgb-use-gpu", action="store_true", help="Use GPU for XGBoost"
+    )
 
     # Data config
-    parser.add_argument('--cefr-column', type=str, default='cefr_level',
-                       help='CEFR column name')
+    parser.add_argument(
+        "--cefr-column", type=str, default="cefr_level", help="CEFR column name"
+    )
 
     # Output
-    parser.add_argument('-o', '--output-dir', type=str,
-                       help='Output directory for results (default: experiment_dir/ho_multifeat_results)')
-    parser.add_argument('-q', '--quiet', action='store_true',
-                       help='Suppress verbose output')
+    parser.add_argument(
+        "-o",
+        "--output-dir",
+        type=str,
+        help="Output directory for results (default: experiment_dir/ho_multifeat_results)",
+    )
+    parser.add_argument(
+        "-q", "--quiet", action="store_true", help="Suppress verbose output"
+    )
 
     args = parser.parse_args()
 
     # Check dependencies
     if not OPTUNA_AVAILABLE:
         print("ERROR: Optuna is not installed. Install it with: pip install optuna")
         return 1
 
-    if args.classifier == 'xgboost' and not XGBOOST_AVAILABLE:
+    if args.classifier == "xgboost" and not XGBOOST_AVAILABLE:
         print("ERROR: XGBoost is not installed. Install it with: pip install xgboost")
         return 1
 
     # Create config
     config = GlobalConfig()
@@ -1116,11 +1257,13 @@
         return 1
 
     # Find all feature config directories
     feature_config_dirs = sorted(features_base.glob(args.feature_pattern))
     if not feature_config_dirs:
-        print(f"ERROR: No feature directories found matching pattern: {args.feature_pattern}")
+        print(
+            f"ERROR: No feature directories found matching pattern: {args.feature_pattern}"
+        )
         return 1
 
     # For each feature config, find dataset subdirectories
     all_feature_dirs = []
     for feature_config_dir in feature_config_dirs:
@@ -1130,12 +1273,14 @@
 
     if not all_feature_dirs:
         print("ERROR: No dataset directories with features_dense.csv found")
         return 1
 
-    print(f"Found {len(all_feature_dirs)} feature directories across "
-          f"{len(feature_config_dirs)} feature configurations")
+    print(
+        f"Found {len(all_feature_dirs)} feature directories across "
+        f"{len(feature_config_dirs)} feature configurations"
+    )
 
     # Stage 1: Screen all features
     stage1_results = stage1_screen_features(
         feature_dirs=all_feature_dirs,
         classifier_type=args.classifier,
@@ -1144,11 +1289,11 @@
         metric=args.metric,
         weight_strategy=args.weight_strategy,
         weight_alpha=args.weight_alpha,
         early_discard_threshold=args.early_discard_threshold,
         early_discard_percentile=args.early_discard_percentile,
-        verbose=not args.quiet
+        verbose=not args.quiet,
     )
 
     # Stage 2: Optimize top-K
     stage2_results = stage2_optimize_top_features(
         stage1_results=stage1_results,
@@ -1158,27 +1303,29 @@
         n_trials_per_config=args.stage2_trials,
         val_split=args.val_split,
         metric=args.metric,
         weight_strategy=args.weight_strategy,
         weight_alpha=args.weight_alpha,
-        verbose=not args.quiet
+        verbose=not args.quiet,
     )
 
     # Save results
     if args.output_dir:
         output_dir = Path(args.output_dir)
     else:
-        output_dir = Path(config.experiment_config.experiment_dir) / "ho_multifeat_results"
+        output_dir = (
+            Path(config.experiment_config.experiment_dir) / "ho_multifeat_results"
+        )
 
     save_results(
         stage1_results=stage1_results,
         stage2_results=stage2_results,
         output_dir=output_dir,
         classifier_type=args.classifier,
-        verbose=not args.quiet
+        verbose=not args.quiet,
     )
 
     return 0
 
 
-if __name__ == '__main__':
+if __name__ == "__main__":
     exit(main())
would reformat /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho_multifeat.py

Oh no! 💥 💔 💥
11 files would be reformatted, 1 file would be left unchanged.
[1;33m[WARNING][0m Black check failed - run with 'fix' mode to auto-format

==========================================
[0;34m2. ISORT - Import Sorting[0m
==========================================
[0;34m[INFO][0m Running isort in CHECK mode...
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/extract_perplexity_features.py Imports are incorrectly sorted and/or formatted.
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/config.py Imports are incorrectly sorted and/or formatted.
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho_multifeat.py Imports are incorrectly sorted and/or formatted.
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho.py Imports are incorrectly sorted and/or formatted.
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers.py Imports are incorrectly sorted and/or formatted.
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/predict.py Imports are incorrectly sorted and/or formatted.
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf_groupby.py Imports are incorrectly sorted and/or formatted.
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/report.py Imports are incorrectly sorted and/or formatted.
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf.py Imports are incorrectly sorted and/or formatted.
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/extract_features.py Imports are incorrectly sorted and/or formatted.
--- /home/b/p/cefr-classification/minimal-cefr/src/extract_perplexity_features.py:before	2025-10-18 12:23:10.571010
+++ /home/b/p/cefr-classification/minimal-cefr/src/extract_perplexity_features.py:after	2025-10-18 12:24:42.700024
@@ -136,10 +136,11 @@
 import argparse
 import json
 import pickle
+from abc import ABC, abstractmethod
+from dataclasses import asdict, dataclass
 from pathlib import Path
-from typing import List, Dict, Any, Optional, Union, Tuple
-from abc import ABC, abstractmethod
-from dataclasses import dataclass, asdict
+from typing import Any, Dict, List, Optional, Tuple, Union
+
 import numpy as np
 import pandas as pd
 from tqdm import tqdm
--- /home/b/p/cefr-classification/minimal-cefr/src/config.py:before	2025-10-18 03:49:32.641082
+++ /home/b/p/cefr-classification/minimal-cefr/src/config.py:after	2025-10-18 12:24:42.747191
@@ -6,12 +6,13 @@
 text classification pipeline.
 """
 
+import hashlib
 import json
+from dataclasses import dataclass, field
+from pathlib import Path
+from typing import Any, Dict, List, Optional
+
 import yaml
-import hashlib
-from dataclasses import dataclass, field
-from typing import List, Optional, Dict, Any
-from pathlib import Path
 
 
 @dataclass
--- /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho_multifeat.py:before	2025-10-18 11:46:52.712243
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho_multifeat.py:after	2025-10-18 12:24:42.849651
@@ -14,13 +14,14 @@
 import json
 import pickle
 from pathlib import Path
-from typing import List, Optional, Tuple, Dict, Any
+from typing import Any, Dict, List, Optional, Tuple
+
 import numpy as np
 import pandas as pd
 from sklearn.linear_model import LogisticRegression
 from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
+from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import LabelEncoder
-from sklearn.model_selection import train_test_split
 from sklearn.utils.class_weight import compute_class_weight
 
 try:
@@ -35,8 +36,13 @@
 except ImportError:
     OPTUNA_AVAILABLE = False
 
-from src.config import GlobalConfig, ExperimentConfig, ClassifierConfig, DataConfig, OutputConfig
-
+from src.config import (
+    ClassifierConfig,
+    DataConfig,
+    ExperimentConfig,
+    GlobalConfig,
+    OutputConfig,
+)
 
 # Fixed CEFR classes (all 6 levels)
 CEFR_CLASSES = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
--- /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho.py:before	2025-10-18 10:00:08.917583
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers_with_ho.py:after	2025-10-18 12:24:42.916858
@@ -9,13 +9,14 @@
 import json
 import pickle
 from pathlib import Path
-from typing import List, Optional, Tuple, Dict, Any
+from typing import Any, Dict, List, Optional, Tuple
+
 import numpy as np
 import pandas as pd
 from sklearn.linear_model import LogisticRegression
 from sklearn.metrics import accuracy_score, log_loss
+from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import LabelEncoder
-from sklearn.model_selection import train_test_split
 
 try:
     import xgboost as xgb
@@ -29,8 +30,13 @@
 except ImportError:
     OPTUNA_AVAILABLE = False
 
-from src.config import GlobalConfig, ExperimentConfig, ClassifierConfig, DataConfig, OutputConfig
-
+from src.config import (
+    ClassifierConfig,
+    DataConfig,
+    ExperimentConfig,
+    GlobalConfig,
+    OutputConfig,
+)
 
 # Fixed CEFR classes (all 6 levels)
 CEFR_CLASSES = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
--- /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers.py:before	2025-10-18 04:17:20.183916
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_classifiers.py:after	2025-10-18 12:24:42.967927
@@ -12,14 +12,15 @@
 import pickle
 from pathlib import Path
 from typing import List, Optional, Tuple
+
 import numpy as np
 import pandas as pd
+from sklearn.ensemble import RandomForestClassifier
+from sklearn.linear_model import LogisticRegression
+from sklearn.metrics import classification_report
 from sklearn.naive_bayes import MultinomialNB
-from sklearn.linear_model import LogisticRegression
-from sklearn.ensemble import RandomForestClassifier
+from sklearn.preprocessing import LabelEncoder
 from sklearn.svm import LinearSVC
-from sklearn.metrics import classification_report
-from sklearn.preprocessing import LabelEncoder
 
 try:
     import xgboost as xgb
@@ -27,8 +28,13 @@
 except ImportError:
     XGBOOST_AVAILABLE = False
 
-from src.config import GlobalConfig, ExperimentConfig, ClassifierConfig, DataConfig, OutputConfig
-
+from src.config import (
+    ClassifierConfig,
+    DataConfig,
+    ExperimentConfig,
+    GlobalConfig,
+    OutputConfig,
+)
 
 # Fixed CEFR classes (all 6 levels)
 CEFR_CLASSES = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
--- /home/b/p/cefr-classification/minimal-cefr/src/predict.py:before	2025-10-18 04:33:13.267817
+++ /home/b/p/cefr-classification/minimal-cefr/src/predict.py:after	2025-10-18 12:24:43.024271
@@ -12,14 +12,20 @@
 import json
 import pickle
 from pathlib import Path
-from typing import Dict, List, Tuple, Optional
+from typing import Dict, List, Optional, Tuple
+
 import numpy as np
 import pandas as pd
+from sklearn.metrics import (
+    accuracy_score,
+    classification_report,
+    confusion_matrix,
+    precision_recall_fscore_support,
+)
 from sklearn.pipeline import make_pipeline
-from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
-from sklearn.metrics import precision_recall_fscore_support
-
-from src.config import GlobalConfig, ExperimentConfig, DataConfig, OutputConfig
+
+from src.config import DataConfig, ExperimentConfig, GlobalConfig, OutputConfig
+
 # Import GroupedTfidfVectorizer so pickle can load it
 from src.train_tfidf_groupby import GroupedTfidfVectorizer
 
--- /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf_groupby.py:before	2025-10-18 03:50:29.093729
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf_groupby.py:after	2025-10-18 12:24:43.068566
@@ -11,13 +11,20 @@
 import json
 import pickle
 from pathlib import Path
-from typing import Optional, Dict, List
+from typing import Dict, List, Optional
+
+import numpy as np
 import pandas as pd
-import numpy as np
+from sklearn.base import BaseEstimator, TransformerMixin
 from sklearn.feature_extraction.text import TfidfVectorizer
-from sklearn.base import BaseEstimator, TransformerMixin
-
-from src.config import GlobalConfig, ExperimentConfig, TfidfConfig, DataConfig, OutputConfig
+
+from src.config import (
+    DataConfig,
+    ExperimentConfig,
+    GlobalConfig,
+    OutputConfig,
+    TfidfConfig,
+)
 
 
 class GroupedTfidfVectorizer(BaseEstimator, TransformerMixin):
--- /home/b/p/cefr-classification/minimal-cefr/src/report.py:before	2025-10-18 02:37:00.199200
+++ /home/b/p/cefr-classification/minimal-cefr/src/report.py:after	2025-10-18 12:24:43.110873
@@ -8,10 +8,10 @@
 import argparse
 import json
 import re
+import sys
+from dataclasses import dataclass, field
 from pathlib import Path
-from typing import Dict, List, Tuple, Optional
-from dataclasses import dataclass, field
-import sys
+from typing import Dict, List, Optional, Tuple
 
 
 @dataclass
--- /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf.py:before	2025-10-18 03:49:56.192116
+++ /home/b/p/cefr-classification/minimal-cefr/src/train_tfidf.py:after	2025-10-18 12:24:43.133327
@@ -10,10 +10,17 @@
 import pickle
 from pathlib import Path
 from typing import Optional
+
 import pandas as pd
 from sklearn.feature_extraction.text import TfidfVectorizer
 
-from src.config import GlobalConfig, ExperimentConfig, TfidfConfig, DataConfig, OutputConfig
+from src.config import (
+    DataConfig,
+    ExperimentConfig,
+    GlobalConfig,
+    OutputConfig,
+    TfidfConfig,
+)
 
 
 def train_tfidf(config: GlobalConfig) -> Path:
--- /home/b/p/cefr-classification/minimal-cefr/src/extract_features.py:before	2025-10-18 04:04:40.976764
+++ /home/b/p/cefr-classification/minimal-cefr/src/extract_features.py:after	2025-10-18 12:24:43.158572
@@ -319,10 +319,12 @@
 import pickle
ERROR: /home/b/p/cefr-classification/minimal-cefr/src/pipeline.py Imports are incorrectly sorted and/or formatted.
 from pathlib import Path
 from typing import List, Optional
+
+import numpy as np
 import pandas as pd
-import numpy as np
-
-from src.config import GlobalConfig, ExperimentConfig, DataConfig, OutputConfig
+
+from src.config import DataConfig, ExperimentConfig, GlobalConfig, OutputConfig
+
 # Import GroupedTfidfVectorizer so pickle can load it
 from src.train_tfidf_groupby import GroupedTfidfVectorizer
 
--- /home/b/p/cefr-classification/minimal-cefr/src/pipeline.py:before	2025-10-18 02:27:09.624897
+++ /home/b/p/cefr-classification/minimal-cefr/src/pipeline.py:after	2025-10-18 12:24:43.222758
@@ -12,13 +12,20 @@
 import json
 import sys
 from pathlib import Path
-from typing import Optional, List
-
-from src.config import GlobalConfig, ExperimentConfig, TfidfConfig, ClassifierConfig, DataConfig, OutputConfig
+from typing import List, Optional
+
+from src.config import (
+    ClassifierConfig,
+    DataConfig,
+    ExperimentConfig,
+    GlobalConfig,
+    OutputConfig,
+    TfidfConfig,
+)
+from src.extract_features import extract_all_from_source
+from src.predict import predict_all_feature_sets
+from src.train_classifiers import train_all_classifiers
 from src.train_tfidf import train_tfidf
-from src.extract_features import extract_all_from_source
-from src.train_classifiers import train_all_classifiers
-from src.predict import predict_all_feature_sets
 
 
 def run_pipeline(
[1;33m[WARNING][0m Isort check failed - run with 'fix' mode to auto-sort

==========================================
[0;34m3. FLAKE8 - Linting (PEP 8 Compliance)[0m
==========================================
[0;34m[INFO][0m Running flake8...
../src/config.py:13:1: F401 'typing.List' imported but unused
../src/config.py:30:30: F541 f-string is missing placeholders
../src/extract_features.py:321:1: F401 'typing.Optional' imported but unused
../src/extract_features.py:323:1: F401 'numpy as np' imported but unused
../src/extract_features.py:327:1: F401 'src.train_tfidf_groupby.GroupedTfidfVectorizer' imported but unused
../src/extract_features.py:330:1: C901 'extract_features_for_file' is too complex (20)
../src/extract_features.py:466:23: F541 f-string is missing placeholders
../src/extract_features.py:471:1: C901 'extract_all_from_source' is too complex (12)
../src/extract_features.py:628:1: C901 'args_to_config' is too complex (15)
../src/extract_features.py:699:1: C901 'main' is too complex (15)
../src/extract_perplexity_features.py:140:1: F401 'typing.Tuple' imported but unused
../src/extract_perplexity_features.py:638:1: C901 'main' is too complex (14)
../src/pipeline.py:24:1: C901 'run_pipeline' is too complex (48)
../src/pipeline.py:57:15: F541 f-string is missing placeholders
../src/pipeline.py:201:23: F541 f-string is missing placeholders
../src/pipeline.py:229:35: F541 f-string is missing placeholders
../src/pipeline.py:234:31: F541 f-string is missing placeholders
../src/pipeline.py:1455:1: C901 'args_to_config' is too complex (22)
../src/predict.py:15:1: F401 'typing.Dict' imported but unused
../src/predict.py:24:1: F401 'src.train_tfidf_groupby.GroupedTfidfVectorizer' imported but unused
../src/predict.py:319:5: F841 local variable 'label_to_idx' is assigned to but never used
../src/predict.py:402:1: C901 'predict_on_features' is too complex (40)
../src/predict.py:442:9: E722 do not use bare 'except'
../src/predict.py:507:38: E741 ambiguous variable name 'l'
../src/predict.py:529:42: E741 ambiguous variable name 'l'
../src/predict.py:562:38: E128 continuation line under-indented for visual indent
../src/predict.py:595:13: F841 local variable 'class_to_idx' is assigned to but never used
../src/predict.py:637:46: E741 ambiguous variable name 'l'
../src/predict.py:659:46: E741 ambiguous variable name 'l'
../src/predict.py:683:50: E741 ambiguous variable name 'l'
../src/predict.py:698:50: E741 ambiguous variable name 'l'
../src/predict.py:718:27: F541 f-string is missing placeholders
../src/predict.py:719:23: F541 f-string is missing placeholders
../src/predict.py:721:27: F541 f-string is missing placeholders
../src/predict.py:723:23: F541 f-string is missing placeholders
../src/predict.py:728:1: C901 'predict_all_feature_sets' is too complex (15)
../src/predict.py:793:27: F541 f-string is missing placeholders
../src/predict.py:813:1: C901 'predict_with_text_pipeline' is too complex (33)
../src/predict.py:915:38: E741 ambiguous variable name 'l'
../src/predict.py:937:42: E741 ambiguous variable name 'l'
../src/predict.py:962:38: E128 continuation line under-indented for visual indent
../src/predict.py:993:13: F841 local variable 'class_to_idx' is assigned to but never used
../src/predict.py:1034:46: E741 ambiguous variable name 'l'
../src/predict.py:1055:42: E741 ambiguous variable name 'l'
../src/predict.py:1079:46: E741 ambiguous variable name 'l'
../src/predict.py:1094:46: E741 ambiguous variable name 'l'
../src/predict.py:1114:27: F541 f-string is missing placeholders
../src/predict.py:1115:23: F541 f-string is missing placeholders
../src/predict.py:1117:27: F541 f-string is missing placeholders
../src/predict.py:1118:19: F541 f-string is missing placeholders
../src/predict.py:1257:1: C901 'args_to_config' is too complex (15)
../src/predict.py:1330:1: C901 'main' is too complex (19)
../src/predict.py:1392:31: F541 f-string is missing placeholders
../src/report.py:12:1: F401 'typing.Tuple' imported but unused
../src/report.py:41:1: C901 'parse_evaluation_report' is too complex (12)
../src/report.py:353:1: C901 'generate_summary_report' is too complex (16)
../src/train_classifiers.py:118:1: C901 'load_features_and_labels' is too complex (12)
../src/train_classifiers.py:201:1: C901 'train_classifier' is too complex (15)
../src/train_classifiers.py:245:15: F541 f-string is missing placeholders
../src/train_classifiers.py:280:13: E722 do not use bare 'except'
../src/train_classifiers.py:353:1: C901 'train_all_classifiers' is too complex (15)
../src/train_classifiers.py:616:1: C901 'args_to_config' is too complex (23)
../src/train_classifiers.py:718:1: C901 'main' is too complex (14)
../src/train_classifiers_with_ho.py:16:1: F401 'sklearn.metrics.log_loss' imported but unused
../src/train_classifiers_with_ho.py:32:1: F401 'src.config.ExperimentConfig' imported but unused
../src/train_classifiers_with_ho.py:32:1: F401 'src.config.ClassifierConfig' imported but unused
../src/train_classifiers_with_ho.py:32:1: F401 'src.config.DataConfig' imported but unused
../src/train_classifiers_with_ho.py:32:1: F401 'src.config.OutputConfig' imported but unused
../src/train_classifiers_with_ho.py:51:1: C901 'load_features_and_labels' is too complex (12)
../src/train_classifiers_with_ho.py:304:9: F841 local variable 'e' is assigned to but never used
../src/train_classifiers_with_ho.py:325:1: C901 'train_classifier_with_ho' is too complex (17)
../src/train_classifiers_with_ho.py:378:15: F541 f-string is missing placeholders
../src/train_classifiers_with_ho.py:389:15: F541 f-string is missing placeholders
../src/train_classifiers_with_ho.py:405:19: F541 f-string is missing placeholders
../src/train_classifiers_with_ho.py:426:19: F541 f-string is missing placeholders
../src/train_classifiers_with_ho.py:522:15: F541 f-string is missing placeholders
../src/train_classifiers_with_ho.py:529:1: C901 'batch_train_classifiers_with_ho' is too complex (14)
../src/train_classifiers_with_ho.py:618:1: C901 'main' is too complex (13)
../src/train_classifiers_with_ho.py:738:23: F541 f-string is missing placeholders
../src/train_classifiers_with_ho_multifeat.py:15:1: F401 'pickle' imported but unused
../src/train_classifiers_with_ho_multifeat.py:38:1: F401 'src.config.ExperimentConfig' imported but unused
../src/train_classifiers_with_ho_multifeat.py:38:1: F401 'src.config.ClassifierConfig' imported but unused
../src/train_classifiers_with_ho_multifeat.py:38:1: F401 'src.config.DataConfig' imported but unused
../src/train_classifiers_with_ho_multifeat.py:38:1: F401 'src.config.OutputConfig' imported but unused
../src/train_classifiers_with_ho_multifeat.py:222:1: C901 'compute_metric' is too complex (11)
../src/train_classifiers_with_ho_multifeat.py:284:1: C901 'load_features_and_labels_from_dir' is too complex (11)
../src/train_classifiers_with_ho_multifeat.py:455:1: C901 'stage1_screen_features' is too complex (22)
../src/train_classifiers_with_ho_multifeat.py:621:1: C901 'stage2_optimize_top_features' is too complex (14)
../src/train_classifiers_with_ho_multifeat.py:899:9: E722 do not use bare 'except'
../src/train_classifiers_with_ho_multifeat.py:956:13: F841 local variable 'metric_name' is assigned to but never used
../src/train_classifiers_with_ho_multifeat.py:958:20: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:970:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:971:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:972:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1024:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1028:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1030:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1034:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1036:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1040:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1042:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1046:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1047:32: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1048:32: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1049:32: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1050:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1051:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1060:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1061:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1062:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1066:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1071:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1073:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1075:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1077:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1081:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1085:24: E128 continuation line under-indented for visual indent
../src/train_classifiers_with_ho_multifeat.py:1087:24: E128 continuation line under-indented for visual indent
../src/train_tfidf.py:12:1: F401 'typing.Optional' imported but unused
../src/train_tfidf.py:19:1: C901 'train_tfidf' is too complex (12)
../src/train_tfidf.py:209:1: C901 'args_to_config' is too complex (17)
../src/train_tfidf_groupby.py:14:1: F401 'typing.Optional' imported but unused
../src/train_tfidf_groupby.py:161:1: C901 'train_grouped_tfidf' is too complex (16)
../src/train_tfidf_groupby.py:244:19: F541 f-string is missing placeholders
../src/train_tfidf_groupby.py:413:1: C901 'args_to_config' is too complex (17)
[0;31m[ERROR][0m Flake8 check failed - please fix linting errors

==========================================
[0;34m4. PYDOCSTYLE - Docstring Convention[0m
==========================================
[0;34m[INFO][0m Running pydocstyle...
../src/extract_perplexity_features.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/extract_perplexity_features.py:1 at module level:
        D301: Use r""" if any backslashes in a docstring
../src/extract_perplexity_features.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 's')
../src/extract_perplexity_features.py:1 at module level:
        D405: Section name should be properly capitalized ('Examples', not 'EXAMPLES')
../src/extract_perplexity_features.py:215 in public method `compute_token_perplexities`:
        D212: Multi-line docstring summary should start at the first line
../src/extract_perplexity_features.py:243 in public method `__init__`:
        D212: Multi-line docstring summary should start at the first line
../src/extract_perplexity_features.py:282 in public method `compute_token_perplexities`:
        D202: No blank lines allowed after function docstring (found 1)
../src/extract_perplexity_features.py:393 in public method `__init__`:
        D212: Multi-line docstring summary should start at the first line
../src/extract_perplexity_features.py:448 in public function `load_model`:
        D212: Multi-line docstring summary should start at the first line
../src/extract_perplexity_features.py:572 in public function `save_results`:
        D212: Multi-line docstring summary should start at the first line
../src/__init__.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/__init__.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 'e')
../src/config.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/config.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 'e')
../src/config.py:26 in public method `__post_init__`:
        D105: Missing docstring in magic method
../src/config.py:37 in public method `get_hash`:
        D212: Multi-line docstring summary should start at the first line
../src/config.py:60 in public method `get_readable_name`:
        D212: Multi-line docstring summary should start at the first line
../src/config.py:94 in public method `__post_init__`:
        D105: Missing docstring in magic method
../src/config.py:116 in public method `__post_init__`:
        D105: Missing docstring in magic method
../src/config.py:131 in public method `get_feature_model_dir`:
        D212: Multi-line docstring summary should start at the first line
../src/config.py:131 in public method `get_feature_model_dir`:
        D411: Missing blank line before section ('Examples')
../src/config.py:152 in public method `get_tfidf_model_dir`:
        D212: Multi-line docstring summary should start at the first line
../src/config.py:166 in public method `get_features_dir`:
        D212: Multi-line docstring summary should start at the first line
../src/config.py:190 in public method `__post_init__`:
        D105: Missing docstring in magic method
../src/config.py:221 in public method `__post_init__`:
        D105: Missing docstring in magic method
../src/train_classifiers_with_ho_multifeat.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 'n')
../src/train_classifiers_with_ho_multifeat.py:57 in public function `compute_sample_weights`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:109 in public function `compute_ordinal_accuracy`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:137 in public function `compute_ordinal_distance_score`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:177 in public function `compute_ordinal_mse_score`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:228 in public function `compute_metric`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:291 in public function `load_features_and_labels_from_dir`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:369 in public function `quick_evaluate_xgboost`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:425 in public function `quick_evaluate_logistic`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:467 in public function `stage1_screen_features`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho_multifeat.py:633 in public function `stage2_optimize_top_features`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not ')')
../src/train_classifiers_with_ho.py:40 in public function `get_cefr_label_encoder`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho.py:59 in public function `load_features_and_labels`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho.py:144 in public function `optimize_xgboost`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho.py:251 in public function `optimize_logistic`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho.py:335 in public function `train_classifier_with_ho`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers_with_ho.py:536 in public function `batch_train_classifiers_with_ho`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 'n')
../src/train_classifiers.py:38 in public function `get_cefr_label_encoder`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers.py:53 in public function `get_classifier`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers.py:126 in public function `load_features_and_labels`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers.py:209 in public function `train_classifier`:
        D212: Multi-line docstring summary should start at the first line
../src/train_classifiers.py:358 in public function `train_all_classifiers`:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 's')
../src/predict.py:28 in public class `PretrainedTfidfWrapper`:
        D205: 1 blank line required between summary line and description (found 0)
../src/predict.py:28 in public class `PretrainedTfidfWrapper`:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:32 in public method `__init__`:
        D107: Missing docstring in __init__
../src/predict.py:36 in public method `fit`:
        D102: Missing docstring in public method
../src/predict.py:40 in public method `transform`:
        D102: Missing docstring in public method
../src/predict.py:43 in public method `fit_transform`:
        D102: Missing docstring in public method
../src/predict.py:48 in public class `PretrainedClassifierWrapper`:
        D205: 1 blank line required between summary line and description (found 0)
../src/predict.py:48 in public class `PretrainedClassifierWrapper`:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:53 in public method `__init__`:
        D107: Missing docstring in __init__
../src/predict.py:67 in public method `fit`:
        D102: Missing docstring in public method
../src/predict.py:71 in public method `predict`:
        D102: Missing docstring in public method
../src/predict.py:80 in public method `predict_proba`:
        D102: Missing docstring in public method
../src/predict.py:87 in public function `load_classifier_and_encoder`:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:120 in public function `load_features_and_labels`:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:184 in public function `cefr_classification_report`:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:295 in public function `cefr_calibration_report`:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:409 in public function `predict_on_features`:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:734 in public function `predict_all_feature_sets`:
        D212: Multi-line docstring summary should start at the first line
../src/predict.py:818 in public function `predict_with_text_pipeline`:
        D205: 1 blank line required between summary line and description (found 0)
../src/predict.py:818 in public function `predict_with_text_pipeline`:
        D212: Multi-line docstring summary should start at the first line
../src/train_tfidf_groupby.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/train_tfidf_groupby.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 'e')
../src/train_tfidf_groupby.py:24 in public class `GroupedTfidfVectorizer`:
        D205: 1 blank line required between summary line and description (found 0)
../src/train_tfidf_groupby.py:24 in public class `GroupedTfidfVectorizer`:
        D212: Multi-line docstring summary should start at the first line
../src/train_tfidf_groupby.py:24 in public class `GroupedTfidfVectorizer`:
        D415: First line should end with a period, question mark, or exclamation point (not 'p')
../src/train_tfidf_groupby.py:29 in public method `__init__`:
        D107: Missing docstring in __init__
../src/train_tfidf_groupby.py:51 in public method `fit`:
        D212: Multi-line docstring summary should start at the first line
../src/train_tfidf_groupby.py:102 in public method `transform`:
        D212: Multi-line docstring summary should start at the first line
../src/train_tfidf_groupby.py:162 in public function `train_grouped_tfidf`:
        D212: Multi-line docstring summary should start at the first line
../src/report.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/report.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 'l')
../src/report.py:42 in public function `parse_evaluation_report`:
        D212: Multi-line docstring summary should start at the first line
../src/report.py:124 in public function `collect_all_metrics`:
        D212: Multi-line docstring summary should start at the first line
../src/report.py:204 in public function `rank_models`:
        D212: Multi-line docstring summary should start at the first line
../src/report.py:478 in public function `main`:
        D103: Missing docstring in public function
../src/train_tfidf.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/train_tfidf.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 'e')
../src/train_tfidf.py:20 in public function `train_tfidf`:
        D212: Multi-line docstring summary should start at the first line
../src/extract_features.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/extract_features.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 'l')
../src/extract_features.py:335 in public function `extract_features_for_file`:
        D212: Multi-line docstring summary should start at the first line
../src/extract_features.py:475 in public function `extract_all_from_source`:
        D212: Multi-line docstring summary should start at the first line
../src/extract_features.py:538 in public function `extract_all_testsets`:
        D212: Multi-line docstring summary should start at the first line
../src/pipeline.py:1 at module level:
        D212: Multi-line docstring summary should start at the first line
../src/pipeline.py:1 at module level:
        D415: First line should end with a period, question mark, or exclamation point (not 'r')
../src/pipeline.py:31 in public function `run_pipeline`:
        D212: Multi-line docstring summary should start at the first line
[1;33m[WARNING][0m Pydocstyle check failed - please improve docstrings

==========================================
[0;34m5. MYPY - Static Type Checking[0m
==========================================
[0;34m[INFO][0m Running mypy...
../src/report.py: note: In function "parse_evaluation_report":
../src/report.py:107: error: Incompatible types in assignment (expression has
type "list[str | Any]", target has type "float")  [assignment]
                strategy["classes_in_test"] = classes
                                              ^~~~~~~
../src/report.py: note: In function "load_model_config":
../src/report.py:120: error: Returning Any from function declared to return
"dict[Any, Any]"  [no-any-return]
            return json.load(f)
            ^~~~~~~~~~~~~~~~~~~
../src/report.py: note: In function "collect_all_metrics":
../src/report.py:182: error: Argument "n_samples" to "ModelMetrics" has
incompatible type "float | None"; expected "int | None"  [arg-type]
                        n_samples=metrics.get("n_samples"),
                                  ^~~~~~~~~~~~~~~~~~~~~~~~
../src/report.py:183: error: Argument "classes_in_test" to "ModelMetrics" has
incompatible type "float | list[str] | None"; expected "list[str] | None" 
[arg-type]
                        classes_in_test=metrics.get("classes_in_test", [])...
                                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/report.py: note: In function "print_ranking_grouped_by_dataset":
../src/report.py:293: error: Need type annotation for "datasets" (hint:
"datasets: dict[<type>, <type>] = ...")  [var-annotated]
        datasets = {}
        ^~~~~~~~
../src/report.py: note: In function "generate_summary_report":
../src/report.py:437: error: Need type annotation for "tfidf_configs" (hint:
"tfidf_configs: dict[<type>, <type>] = ...")  [var-annotated]
        tfidf_configs = {}
        ^~~~~~~~~~~~~
../src/config.py:10: error: Library stubs not installed for "yaml" 
[import-untyped]
    import yaml
    ^
../src/config.py:10: note: Hint: "python3 -m pip install types-PyYAML"
../src/config.py:10: note: (or run "mypy --install-types" to install all missing stub packages)
../src/config.py:10: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#missing-imports
../src/config.py: note: In member "get_feature_model_dir" of class "ExperimentConfig":
../src/config.py:149: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
            return str(Path(self.models_dir) / model_name)
                            ^~~~~~~~~~~~~~~
../src/config.py: note: In member "get_features_dir" of class "ExperimentConfig":
../src/config.py:177: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
            return str(Path(self.features_output_dir) / dir_name)
                            ^~~~~~~~~~~~~~~~~~~~~~~~
../src/train_tfidf.py:13: error: Library stubs not installed for "pandas" 
[import-untyped]
    import pandas as pd
    ^
../src/train_tfidf.py:14: error: Skipping analyzing
"sklearn.feature_extraction.text": module is installed, but missing library
stubs or py.typed marker  [import-untyped]
    from sklearn.feature_extraction.text import TfidfVectorizer
    ^
../src/train_tfidf.py: note: In function "train_tfidf":
../src/train_tfidf.py:34: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
        features_training_dir = Path(exp_config.features_training_dir)
                                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/train_tfidf_groupby.py:15: error: Library stubs not installed for
"pandas"  [import-untyped]
    import pandas as pd
    ^
../src/train_tfidf_groupby.py:17: error: Skipping analyzing
"sklearn.feature_extraction.text": module is installed, but missing library
stubs or py.typed marker  [import-untyped]
    from sklearn.feature_extraction.text import TfidfVectorizer
    ^
../src/train_tfidf_groupby.py:18: error: Skipping analyzing "sklearn.base":
module is installed, but missing library stubs or py.typed marker 
[import-untyped]
    from sklearn.base import BaseEstimator, TransformerMixin
    ^
../src/train_tfidf_groupby.py: note: In function "train_grouped_tfidf":
../src/train_tfidf_groupby.py:177: error: Argument 1 to "Path" has incompatible
type "str | None"; expected "str | PathLike[str]"  [arg-type]
        features_training_dir = Path(exp_config.features_training_dir)
                                     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/predict.py:17: error: Library stubs not installed for "pandas" 
[import-untyped]
    import pandas as pd
    ^
../src/predict.py:18: error: Skipping analyzing "sklearn.pipeline": module is
installed, but missing library stubs or py.typed marker  [import-untyped]
    from sklearn.pipeline import make_pipeline
    ^
../src/predict.py:19: error: Skipping analyzing "sklearn.metrics": module is
installed, but missing library stubs or py.typed marker  [import-untyped]
    from sklearn.metrics import classification_report, confusion_matrix, a...
    ^
../src/predict.py: note: In function "load_features_and_labels":
../src/predict.py:172: error: Item "None" of "Any | None" has no attribute
"unique"  [union-attr]
                print(f"True classes: {sorted(y_test_series.unique())}")
                                              ^~~~~~~~~~~~~~~~~~~~
../src/predict.py: note: In function "predict_on_features":
../src/predict.py:427: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
        classifier_dir = Path(exp_config.models_dir) / "classifiers" / cla...
                              ^~~~~~~~~~~~~~~~~~~~~
../src/predict.py:491: error: Item "None" of "Any | None" has no attribute
"unique"  [union-attr]
            model_classes = sorted(y_test_series.unique())
                                   ^~~~~~~~~~~~~~~~~~~~
../src/predict.py:498: error: Item "None" of "Any | None" has no attribute
"unique"  [union-attr]
            labels_list = sorted(y_test_series.unique())
                                 ^~~~~~~~~~~~~~~~~~~~
../src/predict.py:544: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
            results_dir = Path(exp_config.results_dir) / classifier_model_...
                               ^~~~~~~~~~~~~~~~~~~~~~
../src/predict.py:626: error: Incompatible types in assignment (expression has
type "ndarray[Any, dtype[Any]]", variable has type "list[Any]")  [assignment]
                    y_pred_rounded_avg = np.array(y_pred_rounded_avg)
                                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/predict.py:635: error: Argument 2 to "cefr_classification_report" has
incompatible type "list[Any]"; expected "ndarray[Any, Any]"  [arg-type]
                        y_pred_rounded_avg,
                        ^~~~~~~~~~~~~~~~~~
../src/predict.py:696: error: Argument 2 to "cefr_classification_report" has
incompatible type "list[Any]"; expected "ndarray[Any, Any]"  [arg-type]
                            y_pred_rounded_avg,
                            ^~~~~~~~~~~~~~~~~~
../src/predict.py: note: In function "predict_all_feature_sets":
../src/predict.py:782: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
            labels_csv = Path(labels_csv_dir) / f"{feature_subdir.name}.cs...
                              ^~~~~~~~~~~~~~
../src/predict.py:786: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
                training_labels_csv = Path(exp_config.ml_training_dir) / f...
                                           ^~~~~~~~~~~~~~~~~~~~~~~~~~
../src/predict.py:794: error: Incompatible types in assignment (expression has
type "None", variable has type "Path")  [assignment]
                    labels_csv = None
                                 ^~~~
../src/predict.py: note: In function "predict_with_text_pipeline":
../src/predict.py:835: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
        tfidf_model_path = Path(exp_config.models_dir) / "tfidf" / "tfidf_...
                                ^~~~~~~~~~~~~~~~~~~~~
../src/predict.py:840: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
        classifier_dir = Path(exp_config.models_dir) / "classifiers" / cla...
                              ^~~~~~~~~~~~~~~~~~~~~
../src/predict.py:848: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
            test_file_path = Path(exp_config.ml_test_dir) / test_file
                                  ^~~~~~~~~~~~~~~~~~~~~~
../src/predict.py:944: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
            results_dir = Path(exp_config.results_dir) / dataset_name
                               ^~~~~~~~~~~~~~~~~~~~~~
../src/predict.py:1023: error: Incompatible types in assignment (expression has
type "ndarray[Any, dtype[Any]]", variable has type "list[Any]")  [assignment]
                    y_pred_rounded_avg = np.array(y_pred_rounded_avg)
                                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/predict.py:1032: error: Argument 2 to "cefr_classification_report" has
incompatible type "list[Any]"; expected "ndarray[Any, Any]"  [arg-type]
                        y_pred_rounded_avg,
                        ^~~~~~~~~~~~~~~~~~
../src/predict.py:1092: error: Argument 2 to "cefr_classification_report" has
incompatible type "list[Any]"; expected "ndarray[Any, Any]"  [arg-type]
                        y_pred_rounded_avg,
                        ^~~~~~~~~~~~~~~~~~
../src/predict.py: note: In function "main":
../src/predict.py:1387: error: Argument 1 to "Path" has incompatible type
"str | None"; expected "str | PathLike[str]"  [arg-type]
                    labels_csv_path = Path(config.experiment_config.ml_tes...
                                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...
../src/predict.py:1393: error: Incompatible types in assignment (expression has
type "None", variable has type "Path")  [assignment]
                        labels_csv_path = None
                                          ^~~~
../src/extract_features.py:322: error: Library stubs not installed for "pandas"
 [import-untyped]
    import pandas as pd
    ^
../src/extract_features.py:322: note: Hint: "python3 -m pip install pandas-stubs"
../src/extract_features.py: note: In function "extract_features_for_file":
../src/extract_features.py:398: error: Argument 1 to "Path" has incompatible
type "str | None"; expected "str | PathLike[str]"  [arg-type]
        data_file_path = Path(data_dir) / file_name
                              ^~~~~~~~
../src/extract_features.py: note: In function "extract_all_from_source":
../src/extract_features.py:499: error: Argument 1 to "Path" has incompatible
type "str | None"; expected "str | PathLike[str]"  [arg-type]
                data_dir = Path(exp_config.ml_test_dir)
                                ^~~~~~~~~~~~~~~~~~~~~~
../src/extract_features.py:502: error: Argument 1 to "Path" has incompatible
type "str | None"; expected "str | PathLike[str]"  [arg-type]
                data_dir = Path(exp_config.ml_training_dir)
                                ^~~~~~~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho_multifeat.py:19: error: Library stubs not
installed for "pandas"  [import-untyped]
    import pandas as pd
    ^
../src/train_classifiers_with_ho_multifeat.py:20: error: Skipping analyzing
"sklearn.linear_model": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.linear_model import LogisticRegression
    ^
../src/train_classifiers_with_ho_multifeat.py:21: error: Skipping analyzing
"sklearn.metrics": module is installed, but missing library stubs or py.typed
marker  [import-untyped]
    from sklearn.metrics import accuracy_score, f1_score, precision_score,...
    ^
../src/train_classifiers_with_ho_multifeat.py:22: error: Skipping analyzing
"sklearn.preprocessing": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.preprocessing import LabelEncoder
    ^
../src/train_classifiers_with_ho_multifeat.py:23: error: Skipping analyzing
"sklearn.model_selection": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.model_selection import train_test_split
    ^
../src/train_classifiers_with_ho_multifeat.py:24: error: Skipping analyzing
"sklearn.utils.class_weight": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.utils.class_weight import compute_class_weight
    ^
../src/train_classifiers_with_ho_multifeat.py:33: error: Cannot find
implementation or library stub for module named "optuna"  [import-not-found]
        import optuna
    ^
../src/train_classifiers_with_ho_multifeat.py: note: In function "compute_sample_weights":
../src/train_classifiers_with_ho_multifeat.py:100: error: Returning Any from
function declared to return "ndarray[Any, Any]"  [no-any-return]
        return sample_weights
        ^~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho_multifeat.py: note: In function "compute_ordinal_accuracy":
../src/train_classifiers_with_ho_multifeat.py:126: error: Returning Any from
function declared to return "float"  [no-any-return]
            return np.average(correct, weights=sample_weight)
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho_multifeat.py:128: error: Returning Any from
function declared to return "float"  [no-any-return]
            return np.mean(correct)
            ^~~~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho_multifeat.py: note: In function "compute_ordinal_distance_score":
../src/train_classifiers_with_ho_multifeat.py:167: error: Returning Any from
function declared to return "float"  [no-any-return]
            return np.average(scores, weights=sample_weight)
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho_multifeat.py:169: error: Returning Any from
function declared to return "float"  [no-any-return]
            return np.mean(scores)
            ^~~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho_multifeat.py: note: In function "compute_ordinal_mse_score":
../src/train_classifiers_with_ho_multifeat.py:219: error: Returning Any from
function declared to return "float"  [no-any-return]
        return score
        ^~~~~~~~~~~~
../src/train_classifiers_with_ho_multifeat.py: note: In function "compute_metric":
../src/train_classifiers_with_ho_multifeat.py:251: error: Returning Any from
function declared to return "float"  [no-any-return]
            return accuracy_score(y_true, y_pred, sample_weight=sample_wei...
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...
../src/train_classifiers_with_ho_multifeat.py:254: error: Returning Any from
function declared to return "float"  [no-any-return]
            return f1_score(y_true, y_pred, average='macro', sample_weight...
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...
../src/train_classifiers_with_ho_multifeat.py:257: error: Returning Any from
function declared to return "float"  [no-any-return]
            return f1_score(y_true, y_pred, average='weighted', sample_wei...
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...
../src/train_classifiers_with_ho_multifeat.py:260: error: Returning Any from
function declared to return "float"  [no-any-return]
            return f1_score(y_true, y_pred, average='micro', sample_weight...
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...
../src/train_classifiers_with_ho_multifeat.py:263: error: Returning Any from
function declared to return "float"  [no-any-return]
            return precision_score(y_true, y_pred, average='macro', sample...
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...
../src/train_classifiers_with_ho_multifeat.py:266: error: Returning Any from
function declared to return "float"  [no-any-return]
            return recall_score(y_true, y_pred, average='macro', sample_we...
            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...
../src/train_classifiers_with_ho_multifeat.py: note: In function "stage1_screen_features":
../src/train_classifiers_with_ho_multifeat.py:575: error: Argument "key" to
"sort" of "list" has incompatible type "Callable[[dict[str, object]], object]";
expected
"Callable[[dict[str, object]], SupportsDunderLT[Any] | SupportsDunderGT[Any]]" 
[arg-type]
        results.sort(key=lambda x: x['score'], reverse=True)
                         ^
../src/train_classifiers_with_ho_multifeat.py:575: error: Incompatible return
value type (got "object", expected
"SupportsDunderLT[Any] | SupportsDunderGT[Any]")  [return-value]
        results.sort(key=lambda x: x['score'], reverse=True)
                                   ^~~~~~~~~~
../src/train_classifiers_with_ho_multifeat.py:580: error: Unsupported operand
types for > ("float" and "object")  [operator]
                if result['score'] < early_discard_threshold:
                   ^
../src/train_classifiers_with_ho.py:14: error: Library stubs not installed for
"pandas"  [import-untyped]
    import pandas as pd
    ^
../src/train_classifiers_with_ho.py:15: error: Skipping analyzing
"sklearn.linear_model": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.linear_model import LogisticRegression
    ^
../src/train_classifiers_with_ho.py:16: error: Skipping analyzing
"sklearn.metrics": module is installed, but missing library stubs or py.typed
marker  [import-untyped]
    from sklearn.metrics import accuracy_score, log_loss
    ^
../src/train_classifiers_with_ho.py:17: error: Skipping analyzing
"sklearn.preprocessing": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.preprocessing import LabelEncoder
    ^
../src/train_classifiers_with_ho.py:18: error: Skipping analyzing
"sklearn.model_selection": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.model_selection import train_test_split
    ^
../src/train_classifiers_with_ho.py:27: error: Cannot find implementation or
library stub for module named "optuna"  [import-not-found]
        import optuna
    ^
../src/train_classifiers_with_ho.py: note: In function "train_classifier_with_ho":
../src/train_classifiers_with_ho.py:467: error: Argument 1 to "Path" has
incompatible type "str | None"; expected "str | PathLike[str]"  [arg-type]
        classifiers_dir = Path(exp_config.models_dir) / "classifiers"
                               ^~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho.py: note: In function "batch_train_classifiers_with_ho":
../src/train_classifiers_with_ho.py:585: error: Argument 1 to "Path" has
incompatible type "str | None"; expected "str | PathLike[str]"  [arg-type]
                labels_csv = Path(config.experiment_config.ml_training_dir...
                                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho.py:588: error: Argument 1 to "Path" has
incompatible type "str | None"; expected "str | PathLike[str]"  [arg-type]
                    labels_csv = Path(config.experiment_config.ml_test_dir...
                                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho.py: note: In function "main":
../src/train_classifiers_with_ho.py:673: error: "type[GlobalConfig]" has no
attribute "from_file"  [attr-defined]
            config = GlobalConfig.from_file(args.config_file)
                     ^~~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers_with_ho.py:675: error: "type[GlobalConfig]" has no
attribute "from_json"  [attr-defined]
            config = GlobalConfig.from_json(args.config_json)
                     ^~~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers.py:16: error: Library stubs not installed for "pandas"
 [import-untyped]
    import pandas as pd
    ^
../src/train_classifiers.py:17: error: Skipping analyzing
"sklearn.naive_bayes": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.naive_bayes import MultinomialNB
    ^
../src/train_classifiers.py:18: error: Skipping analyzing
"sklearn.linear_model": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.linear_model import LogisticRegression
    ^
../src/train_classifiers.py:19: error: Skipping analyzing "sklearn.ensemble":
module is installed, but missing library stubs or py.typed marker 
[import-untyped]
    from sklearn.ensemble import RandomForestClassifier
    ^
../src/train_classifiers.py:20: error: Skipping analyzing "sklearn.svm": module
is installed, but missing library stubs or py.typed marker  [import-untyped]
    from sklearn.svm import LinearSVC
    ^
../src/train_classifiers.py:21: error: Skipping analyzing "sklearn.metrics":
module is installed, but missing library stubs or py.typed marker 
[import-untyped]
    from sklearn.metrics import classification_report
    ^
../src/train_classifiers.py:22: error: Skipping analyzing
"sklearn.preprocessing": module is installed, but missing library stubs or
py.typed marker  [import-untyped]
    from sklearn.preprocessing import LabelEncoder
    ^
../src/train_classifiers.py: note: In function "train_classifier":
../src/train_classifiers.py:295: error: Argument 1 to "Path" has incompatible
type "str | None"; expected "str | PathLike[str]"  [arg-type]
        model_dir = Path(exp_config.models_dir) / "classifiers" / model_na...
                         ^~~~~~~~~~~~~~~~~~~~~
../src/train_classifiers.py: note: In function "train_all_classifiers":
../src/train_classifiers.py:410: error: Argument 1 to "Path" has incompatible
type "str | None"; expected "str | PathLike[str]"  [arg-type]
            labels_csv = Path(labels_csv_dir) / f"{feature_subdir.name}.cs...
                              ^~~~~~~~~~~~~~
../src/train_classifiers.py: note: In function "main":
../src/train_classifiers.py:753: error: Argument 1 to "Path" has incompatible
type "str | None"; expected "str | PathLike[str]"  [arg-type]
                labels_csv_path = Path(config.experiment_config.ml_trainin...
                                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...
